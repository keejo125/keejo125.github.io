<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TypeError: can only concatenate str (not bytes) to str]]></title>
    <url>%2FTypeError-can-only-concatenate-str-not-bytes-to-str.html</url>
    <content type="text"><![CDATA[背景最近原来运行的好好的代码突然异常了，检查下日志，发现有如下的报错： 1234Traceback (most recent call last): File "run.py", line 69, in send_mail_attachment logging.info(str(response.status_code) + ':' + response.body)TypeError: can only concatenate str (not "bytes") to str 思路从报错上看，是由于类型不匹配导致的。 在用python发请求时，response.status_code返回的是int类型的，通过用来判断返回是否正常： 123456if response.status_code == 200: # 返回正常 passelse: # 返回异常 pass 所以在输出字符串的时候，前面加了str()用来前强制转换成str类型。 那么问题应该就出在response.body上了，正常情况response.body返回应该是字符串，但根据提示，实际上是一个bytes类型数据。 对实际程序进行调试看下输出： 果然body输出变成了bytes类型。 结论定位了问题，那么该如何转换呢？ 通过str()转换 上面返回是个空值，那么我们也强制转换看下： 123body = b''str(body)"b''" 强制转换之后，虽然变成了str类型，但会有前缀b&#39;&#39;，虽然不会报错，但明显不是我们想要的。 通过decode()转换 那么编码问题，就尝试用编码解码来解决了。我们用decode()试下： 12body.decode()'' 这次就对了。 至于请求的返回结果以前是str类型，现在变成了bytes类型，这就不纠结了，毕竟网络瞬息万变，还是要多用多总结。]]></content>
      <categories>
        <category>python</category>
        <category>异常处理</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[猫眼电影评论爬取异常问题解析]]></title>
    <url>%2F%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA%E7%88%AC%E5%8F%96%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[背景前期以《流浪地球》为例，介绍了抓取猫眼电影评论和分析的爬虫，参考：获取猫眼电影的评论，通过猫眼的接口获取了全量的电影评论。 最近有人在github上提了两个issue，说无法获取评论信息了。 分析根据最开始的思路，猫眼获取电影评论的接口url如下： 1http://m.maoyan.com/review/v2/comments.json?movieId=248172&amp;userId=-1&amp;offset=45&amp;limit=15&amp;ts=1557915814000&amp;type=3 其中主要参数为： movieId：电影的id offset：获取的起始评论为第几条 limit：每次获取评论的条数 ts：获取评论的时间，该数值为以ms为单位的unix时间戳 之前获取的思路是， 初始ts为0，及当前时间 获取评论之后，将最后一条评论的时间转换为时间戳ts，并构造请求url 再次获取以上次ts为当前时间的评论 这样通过不断的将时间往前推，就可以获取所有的评论了。 现在用相同的接口测试我们会发现： 当ts=0时，可以获取当前最新的评论，并返回当前的时间戳。 当我们将ts修改为前一天的时间则会返回异常，并且返回的时间即为我们输入的时间。 1&#123;&quot;data&quot;:&#123;&quot;t2total&quot;:219089,&quot;total&quot;:330321&#125;,&quot;paging&quot;:&#123;&quot;hasMore&quot;:false,&quot;limit&quot;:0,&quot;offset&quot;:0,&quot;total&quot;:0&#125;,&quot;ts&quot;:1557834316000&#125; 注：1557920573965转为时间为：2019-05-15 19:42:53； 前一天 2019-05-14 19:42:53 转为时间戳：1557834173000 我们将时间修改为当前时间之前1小时（1557916973000）就会发现又可以获取数据了。 前期我们可以将limit指定为30，来每次获取30条数据。当我们将ts设置为0是，limit保持为30，同样的返回异常： 1&#123;&quot;data&quot;:&#123;&quot;t2total&quot;:219106,&quot;total&quot;:330348&#125;,&quot;paging&quot;:&#123;&quot;hasMore&quot;:false,&quot;limit&quot;:0,&quot;offset&quot;:0,&quot;total&quot;:0&#125;,&quot;ts&quot;:1557921388556&#125; 我们将limit设置为15时，返回正常。 结论根据上述分析，基本上可以得出如下结论： 我们可以通过猫眼的接口获取电影评论，但无法通过修改ts来获取全量评论。 猫眼通过ts来判断是否为人为浏览，当ts与当前时间差距小于一小时时，可以获取数据，而大于一小时则不行。 猫眼显示每次最多获取15条评论，超过15则也返回异常。 那么获取评论的思路就变为： 通过接口，初始设置ts=0来获取数据 使用返回的ts值，以及offset和limit来构成url 获取更多的数据 需要注意的是，该接口仅可获取1000条评论，超过1000之后返回hasMore:false，无法继续获取。 具体时间参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/maoyan issue参考：https://github.com/keejo125/web_scraping_and_data_analysis/issues/1]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取爱奇艺用户评论分析我是唱作人到底谁最火]]></title>
    <url>%2F%E8%8E%B7%E5%8F%96%E7%88%B1%E5%A5%87%E8%89%BA%E7%94%A8%E6%88%B7%E8%AF%84%E8%AE%BA%E5%88%86%E6%9E%90%E6%88%91%E6%98%AF%E5%94%B1%E4%BD%9C%E4%BA%BA%E5%88%B0%E5%BA%95%E8%B0%81%E6%9C%80%E7%81%AB.html</url>
    <content type="text"><![CDATA[背景《我是唱作人》是爱奇艺自制的原创音乐节目，当前十分火爆，本周更是上半季的最后一集。我们可通过抓取人们观看节目并发表的评论，看看这节目以及最后决赛究竟如何。 评论抓取我们先访问爱奇艺的官网，找到对应的节目：https://www.iqiyi.com/a_19rrhuo9bd.html 会发现仅有节目观看，但是并没有用户的评论，而我们手机APP应用上却是可以看到很多评论的. 那么我们在chrome中打开开发者界面，勾选手机版，并将网址换成手机版地址：https://m.iqiyi.com/v_19rsh3hwuo.html 接下来就是要找到对应评论的接口地址了。 通过不断的下拉会发现，评论是会自动加载的，打开chrome开发者工具中的Network信息，可以很容易的发现，在下拉过程中会有新的请求产生，且都是以get_comments.action开头的，查看这些请求信息： 这就是我们需要找的评论接口了。我们比较几个地址，来查找规律： 123https://sns-comment.iqiyi.com/v3/comment/get_comments.action?content_id=2400411900&amp;types=hot%2Ctime&amp;business_type=17&amp;agent_type=119&amp;agent_version=9.9.0&amp;authcookie=https://sns-comment.iqiyi.com/v3/comment/get_comments.action?content_id=2400411900&amp;types=time&amp;last_id=200771562121&amp;business_type=17&amp;agent_type=119&amp;agent_version=9.9.0&amp;authcookie=https://sns-comment.iqiyi.com/v3/comment/get_comments.action?content_id=2400411900&amp;types=time&amp;last_id=200763395821&amp;business_type=17&amp;agent_type=119&amp;agent_version=9.9.0&amp;authcookie= 可以发现主要有这么几个参数： content_id：对应某个视频 types：直接访问页面时会自动加载一部分评论这时候types值为hot，在不断下拉加载新的评论是，types的值变为time last_id：这个参数在后续下拉加载中会不断变化，是记录目前已加载到哪条评论的参数 其他参数：&amp;business_type=17&amp;agent_type=119&amp;agent_version=9.9.0&amp;authcookie=保持不变 我们再观看第一个请求的具体内容 可以发现最后一个评论的id，正好是下一次请求中last_id参数的值。 另外，我们发现有一个remaining的返回值，并且是1，所以我们猜想这个是用于判断是否已加载所有评论。 最后我们看下我们可以获取哪些数据： 根据上图，可以看到，我们可以获取到评论的时间（addTime）、评论的内容(content)、用户信息(userInfo)等，其中用户信息中可以获取用户昵称(uname)，性别(gender)等。 我们这次分析观看该节目的用户男女比例和评论内容中的词频分析，那么只要获取评论内容，用户昵称、性别即可。 根据上述思路，整个抓取过程就出来了： 先访问节目首页获取初始评论 解析返回值并获取数据 通过初始评论中的最后一条评论的id构造新的评论获取url 通过remaining返回值进行循环，直至获取所有评论数据 实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding: utf-8 -*-# author：zhengkimport requestsimport jsonimport csvimport timedef update_url(last_comment_id): global content_id url = 'https://sns-comment.iqiyi.com/v3/comment/get_comments.action?' \ 'content_id=' + content_id + \ '&amp;types=time&amp;' \ 'last_id=' + last_comment_id + \ '&amp;business_type=17&amp;agent_type=119&amp;agent_version=9.9.0&amp;authcookie=' return urlif __name__ == '__main__': content_id = '2400411900' init_url = 'https://sns-comment.iqiyi.com/v3/comment/get_comments.action?' \ 'content_id=' + content_id + \ '&amp;types=hot%2Ctime&amp;business_type=17&amp;agent_type=119&amp;agent_version=9.9.0&amp;authcookie=' headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'&#125; res = requests.get(url=init_url, headers=headers) # 初始化csvwriter f = open('comments.csv', 'w+', newline='') writer = csv.writer(f, delimiter=';') count = 0 while res.status_code == 200: data = json.loads(res.content) last_comment_id = '' for comment in data['data']['comments']: c_content = comment['content'].strip().replace('\n',';') c_time = time.strftime("%Y-%m-%d %H:%M:%S",time.localtime(comment['addTime'])) c_nickname = comment['userInfo']['uname'] c_gender = comment['userInfo']['gender'] writer.writerow([c_time, c_nickname, c_gender, c_content]) last_comment_id = comment['id'] count += 1 print('获取第' + str(count) + '条评论：' + c_content) url = update_url(last_comment_id) if data['data']['remaining'] == 1: res = requests.get(url, headers=headers) else: break print('抓取结束') f.close() 统计分析观看性别比例分析上面我们将需要的信息都写入了comments.csv文件中，我们通过pandas来读取。在性别比例分析中，我们仅需要nickname和gender信息。 分析思路如下： 筛选dateframe，仅保留所需要的nickname和gender信息 由于存在有人重复评论，需要先使用drop_duplicates函数去重 通过groupby函数，按照性别统计总数 画图 实现代码如下： 12345678910111213def gender_analysis(gender_df): # 通过设置中文字体方式解决中文展示问题 font = FontProperties(fname='../common/font/PingFang.ttc') gender_df.drop_duplicates() gender_df['gender'].replace(&#123;0: 'man', 1: 'female'&#125;, inplace=True) g_df = gender_df.groupby(['gender']).count() g_df.plot(kind='bar', legend=False) plt.title("我是唱作人观众性别分析", fontproperties=font) plt.xlabel("性别", fontproperties=font) plt.ylabel("人数", fontproperties=font) plt.xticks(rotation=360) plt.show() 绘制结果： 可见观看节目的男性要远多于女性。 评论分析及词云绘制评论都保存在comment字段中，那么先获每条评论的内容，通过使用结巴分词进行逐行分词。为了提高分词处理效率，可以先对评论中的空格等进行剔除。主要思路如下： 获取一条评论，并替换空格等符号 通过结巴分词进行分词 提取分词中的名词部分并保存 计算保存的所有名词的频率 使用wordcloud第三方库绘制词云 实现如下： 1234567891011121314151617181920212223242526272829303132333435363738def extract_words(comment_df): stop_words = set(line.strip() for line in open('../common/stopwords.txt', encoding='utf-8')) news_list = [] for item in comment_df.itertuples(index=False): comment = item.comment.replace(' ','') if comment.isspace(): continue p = re.compile("n[a-z0-9]&#123;0,2&#125;") word_list = pseg.cut(comment) for word, flag in word_list: if not word in stop_words and p.search(flag) != None: news_list.append(word) content = &#123;&#125; for item in news_list: content[item] = content.get(item, 0) + 1 return contentdef draw_word_cloud(content): d = os.path.dirname(__file__) img = Image.open(os.path.join(d, "changzuoren.jpg")) width = img.width / 80 height = img.height / 80 alice_coloring = np.array(img) my_wordcloud = WordCloud(background_color="white", max_words=500, mask=alice_coloring, max_font_size=200, random_state=42, font_path=(os.path.join(d, "../common/font/PingFang.ttc"))) my_wordcloud = my_wordcloud.generate_from_frequencies(content) image_colors = ImageColorGenerator(alice_coloring) plt.figure(figsize=(width, height)) plt.imshow(my_wordcloud.recolor(color_func=image_colors)) plt.imshow(my_wordcloud) plt.axis("off") # 通过设置subplots_adjust来控制画面外边框 plt.subplots_adjust(bottom=.01, top=.99, left=.01, right=.99) plt.savefig("changzuoren_wordcloud.png") plt.show() 绘制结果： 可见还是王源的呼声最高，其次汪苏泷、曾轶可、梁博。 拓展完整代码和信息参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/iqiyi]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下csv文件在excel中打开乱码问题]]></title>
    <url>%2FMac%E4%B8%8Bcsv%E6%96%87%E4%BB%B6%E5%9C%A8excel%E4%B8%AD%E6%89%93%E5%BC%80%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景今天在爬虫抓取网页信息并写如为csv文件时，使用excel打开后，竟然中文部分都是乱码。 一般出现乱码问题都是文本编码不对的问题，修改成对应的编码即可。 操作系统：MAC 解决办法 方法一： 在windows环境下，可以只用使用notepad++或者系统自带的记事本打开，之后另外保存一份为ANSI编码格式，再使用excel打开即可。 在MAC下也可以安装对应的软件修改编码。 方法二： 上面的方法需要另外保存一份文档，而且mac上还得另外安装软件才行。研究了一下，其实excel是有自带的转换方法的。 操作如下： 首先打开excel并新建一个文档 在 数据 - 从文本 导入数据 在弹出来的导入向导中，我们就可以发现文本的原始格式有误了 将文本的原始格式选择为”Unicode(UTF-8)”之后，我们发现乱码变中文了 继续”下一步”，按照默认的”分号”分割或者自定义的其他分隔符分割 再”下一步”，按需选择后完成即可。 扩展这次的csv文件是在进行网络爬虫抓取数据中遇到的，在浏览器中，现在基本上都是UTF-8格式的编码，所以这点还是要注意一下。]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[git ssh之Could not resolve hostname github.com问题]]></title>
    <url>%2Fgit-ssh%E4%B9%8BCould-not-resolve-hostname-github-com%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景今天在使用hexo提交github代码的时候，突然有了如下的报错： 123456ssh: Could not resolve hostname github.com: nodename nor servname provided, or not knownfatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists.FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html 解决分析按照报错的提示，是无法解析访问github.com了。 于是立马访问了下http://github.com网站，没有问题。 检查了下\etc\hosts文件，也没有写死github的地址，那么问题出在哪里呢？ 我是通过ssh来提交的，突然想来在添加ssh秘钥后第一次访问的时候，会有一个警告，说明是否要在本地添加一次秘钥并记录hosts地址。 于是找了下ssh记录hosts的地址：~/.ssh/known_hosts 果然有一段记录如下： 1github.com,13.250.177.223 ssh-rsa .... 那么问题就出在这儿了。 将这段记录删除，再次提交git，就又出现了首次访问的警告： 再次yes就可以了。 结论ssh提交的时候，为了方便会通过写known_hosts文件来记录提交地址，而外网地址是dns控制，实际ip可能会变化导致。 是否要记住写入，就看大家实际情况了。 该问题很多网上给的建议是手动写etc\hosts文件，如果是因为之前有让ssh记录，那么改hosts文件也没有用。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[chromedriver之版本选择]]></title>
    <url>%2Fchromedriver%E4%B9%8B%E7%89%88%E6%9C%AC%E9%80%89%E6%8B%A9.html</url>
    <content type="text"><![CDATA[背景在跑一个以前的使用chromedriver的selenium程序的时候突然异常，报了如下的错： 12345678910111213141516171819Traceback (most recent call last): File "run.py", line 139, in &lt;module&gt; do_loop() File "run.py", line 90, in do_loop cookies = get_cookies() File "run.py", line 30, in get_cookies driver = webdriver.Chrome(executable_path='./chromedriver', options=option) File "/Users/zhengk/PycharmProjects/zhelitou/venv/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__ desired_capabilities=desired_capabilities) File "/Users/zhengk/PycharmProjects/zhelitou/venv/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__ self.start_session(capabilities, browser_profile) File "/Users/zhengk/PycharmProjects/zhelitou/venv/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session response = self.execute(Command.NEW_SESSION, parameters) File "/Users/zhengk/PycharmProjects/zhelitou/venv/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute self.error_handler.check_response(response) File "/Users/zhengk/PycharmProjects/zhelitou/venv/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response raise exception_class(message, screen, stacktrace)selenium.common.exceptions.SessionNotCreatedException: Message: session not created: Chrome version must be between 70 and 73 (Driver info: chromedriver=2.45.615355 (d5698f682d8b2742017df6c81e0bd8e6a3063189),platform=Mac OS X 10.14.4 x86_64) 这个报错是说明当前的chromedriver版本不匹配，之前也是一样的代码，怎么突然就不匹配了呢？ 检查了下自己的chrome版本，原来是不知道什么时候chrome升级了，到了74版本。 然后去下载了最新版本的chromedriver，问题解决。 chromedriver的版本选择在下载chromedriver的时候，会发现chromedriver的版本也相当的多，那么到底应该选择呢？ 其实在chromedriver的官网有详细的介绍： chromedriver使用的版本号跟chrome是一样的。 每个版本的chromedriver都有大版本号、小版本号、和构建版本。 在大版本前会有Bete版，同步的也会有一个chromedriver 在新版本发布的时候，也会发布chromedriver 那么如何正确的选择chromedriver，方法如下： 先看你的Chrome版本：比如我的是74.0.3729.131 然后取版本号的前三位，加到url： https://chromedriver.storage.googleapis.com/LATEST_RELEASE_ 后面，比如我的就是： https://chromedriver.storage.googleapis.com/LATEST_RELEASE_74.0.3729 访问这个网址，就会获得我们chrome对应的chromedriver的版本号： 访问https://chromedriver.storage.googleapis.com/index.html网址找到对应的版本的chromedriver即可。也可以直接通过获取的版本号来构建url： https://chromedriver.storage.googleapis.com/index.html?path=74.0.3729.6/ 修改上面url的最后版本号数字即可。 更多信息可参考官网说明： http://chromedriver.chromium.org/downloads/version-selection 扩展有时候直接chromedriver的网址会被墙导致无法访问，这是可以访问国内的淘宝镜像： https://npm.taobao.org/mirrors/chromedriver/]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实践——动态新闻标题热点挖掘]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%8A%A8%E6%80%81%E6%96%B0%E9%97%BB%E6%A0%87%E9%A2%98%E7%83%AD%E7%82%B9%E6%8C%96%E6%8E%98.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 8.2 动态新闻标题热点挖掘课件是通过正则获取新浪新闻热点标题并绘制词云，现通过抓取今日头条热点新闻进行挖掘 获取热点新闻 通过chrome访问https://www.toutiao.com/ch/news_hot/，在开发者工具中可以看到，是有api接口可以直接获取热点新闻的json数据的。 通过分析这几个请求的网址可以发现： 首次访问：https://www.toutiao.com/api/pc/feed/?category=news_hot&amp;utm_source=toutiao&amp;widen=1&amp;max_behot_time=0&amp;max_behot_time_tmp=0 即可获取数据，并会有一个next_behot_time值，将该值替换掉上述url中就可获取后续的数据。 注意：直接从chrome工具中看到的网址比较长https://www.toutiao.com/api/pc/feed/?category=news_hot&amp;utm_source=toutiao&amp;widen=1&amp;max_behot_time=0&amp;max_behot_time_tmp=0&amp;tadrequire=true&amp;as=A1258C7D00422B9&amp;cp=5CD032F26B29AE1&amp;_signature=z.t.3AAAkzMhvSCXMjx438.7f8，后面的一些被删除的参数代表的是浏览器等信息，如果不删除会导致python请求异常。 分词 分词用到了jieba分词，可以直接调用jieba第三方包进行分词，要注意停用词的使用，词频需要自行统计。 词云绘制 词云绘制用到了wordcloud第三方库，课件中采用默认词云生成，一般默认的词云比较难看，可以通过numpy计算原图的颜色，并对生成的词云进行重新上色的方式改善效果。 需要注意的是图片不能太小，可能会导致异常。 实现: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# -*- coding: utf-8 -*-# author：zhengkimport requestsimport jsonimport reimport jieba.posseg as psegfrom wordcloud import WordCloud, ImageColorGeneratorimport matplotlib.pyplot as pltimport numpy as npimport PIL.Image as Imageimport osdef get_news_hot(loop=5): max_behot_time = 0 headers = &#123;'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'&#125; with open('hot_news.txt', 'w', encoding='utf-8') as f: for x in range(loop): url = "https://www.toutiao.com/api/pc/feed/?category=news_hot&amp;utm_source=toutiao" \ "&amp;widen=1&amp;max_behot_time=" + str(max_behot_time) + "&amp;max_behot_time_tmp=" + str(max_behot_time) res = requests.get(url, headers=headers) if res.status_code == 200: data = json.loads(res.text) for news in data['data']: f.write(news['title']) max_behot_time = data['next']['max_behot_time']def extract_words(): with open('hot_news.txt', 'r', encoding='utf-8') as f: news_subjects = f.readlines() stop_words = set(line.strip() for line in open('stopwords.txt', encoding='utf-8')) news_list = [] for subject in news_subjects: if subject.isspace(): continue p = re.compile("n[a-z0-9]&#123;0,2&#125;") word_list = pseg.cut(subject) for word, flag in word_list: if not word in stop_words and p.search(flag) != None: news_list.append(word) content = &#123;&#125; for item in news_list: content[item] = content.get(item, 0) + 1 d = os.path.dirname(__file__) img = Image.open(os.path.join(d, "toutiao.jpg")) width = img.width / 80 height = img.height / 80 alice_coloring = np.array(img) my_wordcloud = WordCloud(background_color="white", max_words=500, mask=alice_coloring, max_font_size=200, random_state=42, font_path=(os.path.join(d, "PingFang.ttc"))) my_wordcloud = my_wordcloud.generate_from_frequencies(content) image_colors = ImageColorGenerator(alice_coloring) plt.figure(figsize=(width, height)) plt.imshow(my_wordcloud.recolor(color_func=image_colors)) plt.imshow(my_wordcloud) plt.axis("off") # 通过设置subplots_adjust来控制画面外边框 plt.subplots_adjust(bottom=.01, top=.99, left=.01, right=.99) plt.savefig("jupiter_wordcloud_1.png") plt.show()if __name__ == '__main__': get_news_hot(5) extract_words() 原图： 最终效果：]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实践——线性回归分析入门之波士顿房价预测]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E5%85%A5%E9%97%A8%E4%B9%8B%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 8.1 线性回归分析入门之波士顿房价预测获取数据 加载数据 123456from sklearn import datasetsimport pandas as pdboston = datasets.load_boston()x = pd.DataFrame(boston.data, columns=boston.feature_names)y = pd.DataFrame(boston.target, columns=['MEDV']) 查看数据含义 1234567891011121314151617181920212223242526272829303132333435363738print(boston.DESCR).. _boston_dataset:Boston house prices dataset---------------------------**Data Set Characteristics:** :Number of Instances: 506 :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. :Attribute Information (in order): - CRIM per capita crime rate by town - ZN proportion of residential land zoned for lots over 25,000 sq.ft. - INDUS proportion of non-retail business acres per town - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) - NOX nitric oxides concentration (parts per 10 million) - RM average number of rooms per dwelling - AGE proportion of owner-occupied units built prior to 1940 - DIS weighted distances to five Boston employment centres - RAD index of accessibility to radial highways - TAX full-value property-tax rate per $10,000 - PTRATIO pupil-teacher ratio by town - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town - LSTAT % lower status of the population - MEDV Median value of owner-occupied homes in $1000's :Missing Attribute Values: None :Creator: Harrison, D. and Rubinfeld, D.L.This is a copy of UCI ML housing dataset.https://archive.ics.uci.edu/ml/machine-learning-databases/housing/This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonicprices and the demand for clean air', J. Environ. Economics &amp; Management,vol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, 'Regression diagnostics', Wiley, 1980. N.B. Various transformations are used in the table onpages 244-261 of the latter.The Boston house-price data has been used in many machine learning papers that address regressionproblems. .. topic:: References - Belsley, Kuh &amp; Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261. - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. 预判 绘制房间数[‘RM’]与房价关系的散点图： 12plt.scatter(x['RM'], y, color='red')plt.show() 绘制人口中低收入阶层比例与房价关系的散点图： 12plt.scatter(x['LSTAT'], y, color='blue')plt.show() 发现基本上存在一定的线性关系，采用线性回归模型尝试。 建模 利用statsmodel.api中的普通最小二乘回归模型拟合 12345import statsmodels.api as smx_add1 = sm.add_constant(x)model = sm.OLS(y, x_add1).fit()print(model.summary()) 拟合结果： 1234567891011121314151617181920212223242526272829303132333435363738 OLS Regression Results ==============================================================================Dep. Variable: MEDV R-squared: 0.741Model: OLS Adj. R-squared: 0.734Method: Least Squares F-statistic: 108.1Date: Sun, 05 May 2019 Prob (F-statistic): 6.72e-135Time: 22:59:08 Log-Likelihood: -1498.8No. Observations: 506 AIC: 3026.Df Residuals: 492 BIC: 3085.Df Model: 13 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975]------------------------------------------------------------------------------const 36.4595 5.103 7.144 0.000 26.432 46.487CRIM -0.1080 0.033 -3.287 0.001 -0.173 -0.043ZN 0.0464 0.014 3.382 0.001 0.019 0.073INDUS 0.0206 0.061 0.334 0.738 -0.100 0.141CHAS 2.6867 0.862 3.118 0.002 0.994 4.380NOX -17.7666 3.820 -4.651 0.000 -25.272 -10.262RM 3.8099 0.418 9.116 0.000 2.989 4.631AGE 0.0007 0.013 0.052 0.958 -0.025 0.027DIS -1.4756 0.199 -7.398 0.000 -1.867 -1.084RAD 0.3060 0.066 4.613 0.000 0.176 0.436TAX -0.0123 0.004 -3.280 0.001 -0.020 -0.005PTRATIO -0.9527 0.131 -7.283 0.000 -1.210 -0.696B 0.0093 0.003 3.467 0.001 0.004 0.015LSTAT -0.5248 0.051 -10.347 0.000 -0.624 -0.425==============================================================================Omnibus: 178.041 Durbin-Watson: 1.078Prob(Omnibus): 0.000 Jarque-Bera (JB): 783.126Skew: 1.521 Prob(JB): 8.84e-171Kurtosis: 8.281 Cond. No. 1.51e+04==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.51e+04. This might indicate that there arestrong multicollinearity or other numerical problems. 训练数据优化 根据上面拟合结果报告，P&gt;|t|这列中，大于0.005的即为异常值，可以视为不相关，可以剔除该属性。 1x.drop(['AGE', 'INDUS'], axis=1, inplace=True) 重新拟合 123x_add1 = sm.add_constant(x)model = sm.OLS(y, x_add1).fit()print(model.summary()) 拟合结果 123456789101112131415161718192021222324252627282930313233343536 OLS Regression Results ==============================================================================Dep. Variable: MEDV R-squared: 0.741Model: OLS Adj. R-squared: 0.735Method: Least Squares F-statistic: 128.2Date: Sun, 05 May 2019 Prob (F-statistic): 5.54e-137Time: 23:07:23 Log-Likelihood: -1498.9No. Observations: 506 AIC: 3022.Df Residuals: 494 BIC: 3072.Df Model: 11 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975]------------------------------------------------------------------------------const 36.3411 5.067 7.171 0.000 26.385 46.298CRIM -0.1084 0.033 -3.307 0.001 -0.173 -0.044ZN 0.0458 0.014 3.390 0.001 0.019 0.072CHAS 2.7187 0.854 3.183 0.002 1.040 4.397NOX -17.3760 3.535 -4.915 0.000 -24.322 -10.430RM 3.8016 0.406 9.356 0.000 3.003 4.600DIS -1.4927 0.186 -8.037 0.000 -1.858 -1.128RAD 0.2996 0.063 4.726 0.000 0.175 0.424TAX -0.0118 0.003 -3.493 0.001 -0.018 -0.005PTRATIO -0.9465 0.129 -7.334 0.000 -1.200 -0.693B 0.0093 0.003 3.475 0.001 0.004 0.015LSTAT -0.5226 0.047 -11.019 0.000 -0.616 -0.429==============================================================================Omnibus: 178.430 Durbin-Watson: 1.078Prob(Omnibus): 0.000 Jarque-Bera (JB): 787.785Skew: 1.523 Prob(JB): 8.60e-172Kurtosis: 8.300 Cond. No. 1.47e+04==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.47e+04. This might indicate that there arestrong multicollinearity or other numerical problems. 预测 输出建模结果 coef 列即为计算出的回归系数 1234567891011121314print(model.params)const 36.341145CRIM -0.108413ZN 0.045845CHAS 2.718716NOX -17.376023RM 3.801579DIS -1.492711RAD 0.299608TAX -0.011778PTRATIO -0.946525B 0.009291LSTAT -0.522553dtype: float64 获取测试数据： 12import numpy as npx_test = np.array([[1, 0.006, 18.0, 0.0, 0.52, 6.6, 4.87, 1.0, 290.0, 15.2, 396.2, 5]]) 预测 12print(model.predict(x_test))[29.51617469]]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GUI与面向对象3]]></title>
    <url>%2FGUI%E4%B8%8E%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A13.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 7.8 综合应用 财经数据GUI项目my_finance.py 1234567891011121314151617181920212223242526272829303132333435363738# -*- coding: utf-8 -*-# author：zhengkimport jsonimport reimport requestsdef retrieve_dji_list(): try: url = 'http://money.cnn.com/data/dow30/' res = requests.get(url) except ConnectionError as e: print(e) pattern = re.compile( 'class="wsod_symbol"&gt;(.*?)&lt;\/a&gt;.*?&lt;span.*?&gt;(.*?)&lt;\/span&gt;.*?\n.*?class="wsod_stream"&gt;(.*?)&lt;\/span&gt;') dji_list_raw = re.findall(pattern, res.text) dji_list = [] for item in dji_list_raw: dji_list.append(&#123; 'code': item[0], 'name': item[1], 'price': float(item[2]) &#125;) return dji_listdef retrieve_quotes_historical(stock_code, start = '', end = ''): quotes = [] url = 'https://finance.yahoo.com/quote/%s/history?p=%s' % (stock_code, stock_code) try: r = requests.get(url) except ConnectionError as err: print(err) m = re.findall('"HistoricalPriceStore":&#123;"prices":(.*?),"isPending"', r.text) if m: quotes = json.loads(m[0]) quotes = quotes[::-1] return [item for item in quotes if not 'type' in item] dji_wxPython.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161# -*- coding: utf-8 -*-# author：zhengkimport datetime as dtimport my_finance as financeimport matplotlib.pyplot as pltimport pandas as pdimport _thread as threadimport wxID_EVENT_REFRESH = 9999class StockFrame(wx.Frame): option_list = &#123; 'open': True, 'close': True, 'high': False, 'low': False, 'volume': False &#125; def __init__(self, title): wx.Frame.__init__(self, None, title=title, size=(430,600)) self.CreateStatusBar() # 菜单栏 menuBar = wx.MenuBar() filemenu = wx.Menu() menuBar.Append(filemenu, '文件') # 刷新按钮 menuRefresh = filemenu.Append(ID_EVENT_REFRESH, '刷新', '刷新价格') self.Bind(wx.EVT_MENU, self.on_refresh, menuRefresh) # 退出按钮 menuQuit = filemenu.Append(wx.ID_EXIT, '退出', '退出程序') self.Bind(wx.EVT_MENU, self.on_quit, menuQuit) self.SetMenuBar(menuBar) panel = wx.Panel(self) # 选项栏 codeSizer = wx.BoxSizer(wx.HORIZONTAL) # 标签 labelText = wx.StaticText(panel, label='股票代码') codeSizer.Add(labelText, 0, wx.ALIGN_BOTTOM) # 输入框 codeText = wx.TextCtrl(panel, value='BA', style=wx.TE_PROCESS_ENTER) self.Bind(wx.EVT_TEXT_ENTER, self.on_text_submit, codeText) codeSizer.Add(codeText) # 多选栏 optionSizer = wx.BoxSizer(wx.HORIZONTAL) for key, value in self.option_list.items(): checkBox = wx.CheckBox(panel, label=key) checkBox.SetValue(value) self.Bind(wx.EVT_CHECKBOX, self.on_checked) optionSizer.Add(checkBox) # 列表栏 self.list = wx.ListCtrl(panel, wx.NewId(), style=wx.LC_REPORT) # 插入列表标题 self.create_handler() pos = self.list.InsertItem(0, '--') self.list.SetItem(pos, 1, 'loading...') self.list.SetItem(pos, 2, '--') self.Bind(wx.EVT_LIST_ITEM_ACTIVATED, self.on_double_click, self.list) # 底部按钮栏 ctrlSizer = wx.BoxSizer(wx.HORIZONTAL) # 空隙 ctrlSizer.Add((10, 10)) # 退出按钮 buttonQuit = wx.Button(panel, -1, '退出') self.Bind(wx.EVT_BUTTON, self.on_quit, buttonQuit) ctrlSizer.Add(buttonQuit, 1) # 刷新按钮 buttonRefresh = wx.Button(panel, -1, '刷新') self.Bind(wx.EVT_BUTTON, self.on_refresh, buttonRefresh) ctrlSizer.Add(buttonRefresh, 1, wx.LEFT|wx.BOTTOM) # 全局sizer sizer = wx.BoxSizer(wx.VERTICAL) sizer.Add(codeSizer, 0, wx.ALL, 5) sizer.Add(optionSizer, 0, wx.ALL, 5) sizer.Add(self.list, -1, wx.ALL | wx.EXPAND, 5) sizer.Add(ctrlSizer, 0, wx.ALIGN_BOTTOM) panel.SetSizerAndFit(sizer) self.Center() self.on_refresh(None) def create_handler(self): self.list.InsertColumn(0, '代码') self.list.InsertColumn(1, '名称') self.list.InsertColumn(2, '成交量') def set_data(self, data): self.list.ClearAll() self.create_handler() pos = 0 for row in data: pos = self.list.InsertItem(pos+1, row['code']) self.list.SetItem(pos, 1, row['name']) self.list.SetColumnWidth(1, -1) self.list.SetItem(pos, 2, str(row['price'])) if pos % 2 == 0: self.list.SetItemBackgroundColour(pos, (134, 255, 249)) def plot_data(self, code): quotes = finance.retrieve_quotes_historical(code) fields = ['date', 'open', 'close', 'high', 'low', 'volume'] dates = [] for i in range(0, len(quotes)): x = dt.datetime.utcfromtimestamp(int(quotes[i]['date'])) y = dt.datetime.strftime(x, '%Y-%m-%d') dates.append(y) quotesdf = pd.DataFrame(quotes, index=dates, columns=fields) fields_to_drop = ['date'] for key, value in self.option_list.items(): if not value: fields_to_drop.append(key) quotesdf = quotesdf.drop(fields_to_drop, axis=1) quotesdf.plot() plt.show() def on_double_click(self, event): self.plot_data(event.GetText()) def on_text_submit(self, event): self.plot_data(event.GetString()) def on_checked(self, event): checkBox = event.GetEventObject() text = checkBox.GetLabel().lower() self.option_list[text] = checkBox.GetValue() def on_quit(self, event): self.Close() self.Destroy() def on_refresh(self, event): thread.start_new_thread(self.retrieve_quotes, ()) def retrieve_quotes(self): data = finance.retrieve_dji_list() if data: self.set_data(data) else: wx.MessageBox('下载失败！', '报错信息', wx.OK|wx.ICON_INFORMATION)if __name__ == '__main__': app = wx.App(False) top = StockFrame('Dow Jones Industrial Average (^DJI)') top.Show(True) app.MainLoop()]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GUI与面向对象2]]></title>
    <url>%2FGUI%E4%B8%8E%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A12.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 7.4 GUI的基本框架 7.5 GUI的常用组件 7.6 布局管理 7.7 其他GUI库]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GUI与面向对象1]]></title>
    <url>%2FGUI%E4%B8%8E%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A11.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 7.1 GUI与面向对象 7.2 抽象 7.3 继承]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python高级数据处理和可视化3]]></title>
    <url>%2FPython%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%963.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 数据分析项目—男女电影评分差异比较一、程序功能基于 MovieLens 100k 数据集中男性女性对电影的评分来判断男性还是女性电影评分的差异性更大。 二、数据来源数据集下载: http://files.grouplens.org/datasets/movielens/ml-100k.zip 数据含义: u.data 表示 100k 条评分记录，每一列的数值含义是:user id | item id | rating | timestamp u.user 表示用户的信息，每一列的数值含义是:user id | age | gender | occupation | zip code u.item 文件表示电影的相关信息，每一列的数值含义是:movie/item id | movie title | release date | video release date |IMDb URL | unknown | Action | Adventure | Animation | Children’s | Comedy | Crime | Documentary | Drama | Fantasy |Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |Thriller | War | Western | API 文档请参考 http://pandas.pydata.org/pandas-docs/stable/ 三、分析和参考代码要判断男性还是女性电影评分的差异性大小则可以利用标准差，标准差越大表示评分离散程度大，即差异性大，反之表示数据越聚集，差异性小。 实现代码： 1234567891011121314151617# -*- coding: utf-8 -*-# author：zhengkimport pandas as pdusers_names = ['user id', 'age', 'gender', 'occupation', 'zip code']users = pd.read_csv('ml-100k/u.user', sep='|', names=users_names)data_names = ['user id', 'item id', 'rating', 'timestamp']data = pd.read_csv('ml-100k/u.data', sep='\t', names=data_names)users_df = users[['user id', 'gender']]data_df = data[['user id', 'rating']]rating_df = pd.merge(users_df, data_df)rating_df_mean = rating_df.groupby(['gender', 'user id']).mean()print(rating_df_mean.groupby(['gender']).std()) 输出结果： 1234 ratinggender F 0.481241M 0.430076 结论：女生的电影评分差异更大。]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python高级数据处理和可视化2]]></title>
    <url>%2FPython%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%962.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 6.5 数据存取 扩展：箱型图分析成绩数据把以下数据复制到 excel 工作表中(文件名为 score.xlsx): Maths English Python Music Physics Chemistry PE Wang 88 64 96 85 90 81 95 Ma 92 99 95 94 92 94 90 Liu 91 87 99 95 95 92 70 Qian 78 99 75 81 83 88 92 Meng 88 78 98 84 70 95 98 Song 100 95 100 92 98 95 65 利用 pandas 的 read_excel()方法读出数据保存到一个 DataFrame 中，并绘制相应的箱形图。 实现： 123456789# -*- coding: utf-8 -*-# author：zhengkimport pandas as pdimport matplotlib.pyplot as pltscores = pd.read_excel('scores.xlsx')scores.boxplot()plt.show() 图像显示： (1)哪些课程的成绩分布比较集中，哪些比较分散?你是通过什么来观察到的? Maths、Python 和 Chemistry 分布比较集中，而 English 和 PE 的分布则比较分散，从箱子的长度可以获得这些信息; (2)哪些课程的成绩分布比较平均(对称)?你是通过什么来观察到的? 在 7 门课程中，Python 的成绩分布最为均匀，PE 的成绩分布最不均匀，从中位数到上四分位和下四分位的距离比较可以获得这些信息; (3)哪些课程的成绩总体较好?哪些总体较差?你是通过什么来观察到的? Python 的总体情况最好，Maths、English 和 Music 的总体情况不太理想，从中位数的位置可以获得这些信息，分析数据可以发现这 3 门课程中各有 3 个同学没有达到 90 分，PE 虽然从图中看平均值应该不高，但其总体情况较好，分析数据可以发现有 2 个同学考了低分但 90 以上的有 4 个同学; (4)哪些课程的成绩存在与总体情况不相符的值?你是通过什么来观察到的? Maths、Python 和 Physics 中分别有 2 个、1 个、1 个异常值(离群点)，箱形图中会将这些异常值单独列出，一是为了一目了然地表明数据中的异常，二是为了不因为这些少数的异常数据导致整体特征的偏移。 6.6 Python的理工类应用 扩展：WAV音频处理入门研究1. 网络数据读取与保存主要使用 urllib.request 模块此处选择位于 http://www.nch.com.au/acm/11k16bitpcm.wav 的音频作为数据源，使用 urllib.request.urlopen()函数取得该文件，并以‘english.wav’的文件名保存在本地的程序所在路径。 实现： 123456# 获取wav文件response = urllib.request.urlopen('http://www.nch.com.au/acm/11k16bitpcm.wav')wav_file = 'english.wav'file = open(wav_file, 'wb+')file.write(response.read())file.close() 2. 简单幅度处理使用 wavfile 模块可以比较简单地从 wav 格式的文件中读取出相应的采样率、数据等信息，而不必关心文件的格式细节。python 的 wave 模块也有相应的操作，示例详见代码。读取后使用 pyplot 模块，可以绘制出音频文件的波形图。 将信号幅值按比例减小，表现出来的效果就是声音强度减小。使用 numpy 模块，将原始数据与常数相乘，得到一个值减小的新数组。这里利用了 numpy的广播机制，如果两个相乘的对象长度不等，numpy 会根据一定的规则将对象扩展为相同的类型，再做运算。 计算完成后，使用 wavfile.write()函数将修改后的音频信号写入一个新的文件中，命名为’silent.wav’，采样率未修改，与原音频相同。 最后使用 matplotlib.pyplot 绘制波形图对比。 12345678910111213141516171819202122232425# 读取wav文件wavefile = wave.open(wav_file, 'r')params = wavefile.getparams()nchannels, sample_width, framerate, numframes = params[:4]#nchannels:声道数#sample_width:采样宽度,每个采样的字节数#framerate:采样率#numframes:总采样数sample_rate, data = scipy.io.wavfile.read(wav_file)matplotlib.pyplot.subplot(2,1,1)matplotlib.pyplot.title('Original')matplotlib.pyplot.plot(data)newdata = data * 0.2newdata = newdata.astype(numpy.int16)scipy.io.wavfile.write('silent.wav', sample_rate, newdata)matplotlib.pyplot.subplot(2,1,2)matplotlib.pyplot.title('Quiet')matplotlib.pyplot.plot(newdata)matplotlib.pyplot.show() 图像显示： 3. 简单时频分析音频信号是一种常见的非平稳信号，频域特性随时间变化。时频图是分析音频信号的常用工具，在一幅图中表示出信号的频率、幅度随时间的变化。pylab 模块提供了 specgram 函数，可以简单地通过配置相应参数进行短时傅里叶变换，并输出时频图。 实现： 12345result = matplotlib.pylab.specgram(newdata, NFFT=1024, Fs=sample_rate, noverlap=900)#y:音频信号#NFFT:每个进行快速傅里叶变换的数据块大小，一般取 2 的幂次#Fs:采样率 #noverlap:数据块之间重叠数据点的个数matplotlib.pylab.show() 图像显示： 6.7 Python的人文社科类应用]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python高级数据处理和可视化1]]></title>
    <url>%2FPython%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%961.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 6.1 聚类分析 6.1 扩展 scikit-learn 机器学习经典入门项目scikit-learn 是基于 NumPy、SciPy 和 Matplotlib 的著名的 Python 机器学习包，里面包含了大量经典机器学习的数据集和算法实现，请基于经典的鸢尾花数据集 iris 实现简单的分类和聚类功能。 实现： 1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-# author：zhengkfrom sklearn import datasets, neighbors, cluster, svmiris = datasets.load_iris()# 利用KNN分类算法进行分析knn = neighbors.KNeighborsClassifier()knn.fit(iris.data, iris.target)pred = knn.predict([[5.0, 3.0, 5.0, 2.0]])print('result of KNN is ', pred)# 利用支持向量机SVM分析svc = svm.LinearSVC()svc.fit(iris.data, iris.target)pred = svc.predict([[5.0, 3.0, 5.0, 2.0]])print('result of svm is ', pred)# 利用k-means聚类算法进行聚类kmeans = cluster.KMeans(n_clusters=3).fit(iris.data)pred = kmeans.predict(iris.data)for label in pred: print(label, end='')print('\n')for label in iris.target: print(label, end='') 6.2 Matplotlib绘图基础 6.3 Matplotlib图像属性控制 6.3 练习1、数据比较图绘制请将 Intel 和 IBM 公司近一年来每个月开票价的平均值绘制在一张图中(用 subplot()或subplots()函数)。 实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# -*- coding: utf-8 -*-# author：zhengkimport requestsimport reimport jsonimport pandas as pdfrom datetime import dateimport timeimport matplotlib.pyplot as pltdef retrieve_quotes_historical(stock_code): quotes = [] url = 'https://finance.yahoo.com/quote/%s/history?p=%s' % (stock_code, stock_code) try: r = requests.get(url) except ConnectionError as err: print(err) m = re.findall('"HistoricalPriceStore":&#123;"prices":(.*?),"isPending"', r.text) if m: quotes = json.loads(m[0]) quotes = quotes[::-1] return [item for item in quotes if not 'type' in item]def create_aveg_open(stock_code): quotes = retrieve_quotes_historical(stock_code) list1 = [] for i in range(len(quotes)): x = date.fromtimestamp(quotes[i]['date']) y = date.strftime(x, '%Y-%m-%d') list1.append(y) quotesdf_ori = pd.DataFrame(quotes, index=list1) listtemp = [] for i in range(len(quotesdf_ori)): temp = time.strptime(quotesdf_ori.index[i], '%Y-%m-%d') listtemp.append(temp.tm_mon) tempdf = quotesdf_ori.copy() tempdf['month'] = listtemp meanopen = tempdf.groupby('month').open.mean() return meanopenopen1 = create_aveg_open('INTC')open2 = create_aveg_open('IBM')plt.subplot(211)plt.title('Mean Open of INTC')plt.xlabel('Month')plt.ylabel('$')plt.plot(open1.index, open1.values, color='r', marker='o')plt.subplot(212)plt.title('Mean Open of IBM')plt.xlabel('Month')plt.ylabel('$')plt.plot(open2.index, open2.values, color='green', marker='o')plt.show() 结果： 2、iris数据集绘图利用“6.1 扩展:Scikit-learn 经典机器学习经典入门小项目开发”中介绍的鸢尾花 iris数据集中的某两个特征(例如萼片长度和花瓣长度)绘制散点图。 实现： 123456789101112131415161718# -*- coding: utf-8 -*-# author：zhengkfrom sklearn import datasetsimport matplotlib.pyplot as pltiris = datasets.load_iris()x = [item[0] for item in iris.data]y = [item[2] for item in iris.data]# sample of setosaplt.scatter(x[:50], y[:50], color='red', marker='o', label='setosa')# sample of versicolorplt.scatter(x[50:100], y[50:100], color='green', marker='o', label='versicolor')# sample of virginicaplt.scatter(x[100:150], y[100:150], color='blue', marker='o', label='virginica')plt.legend(loc='best')plt.xlabel('sepal length in cm')plt.ylabel('petal length in cm')plt.show() 结果： 6.4 Pandas作图]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派开机自动登录]]></title>
    <url>%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E5%BC%80%E6%9C%BA%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%95.html</url>
    <content type="text"><![CDATA[背景树莓派一般作为瘦终端或者小服务器使用，通过配置开机自动登录在后续的使用中会方便很多。而且在默认系统配置中，也是自动登录pi用户。那么如何配置自动登录其他用户呢？ 自动登录配置首先我们需要新建一个自己的账户，可以参考树莓派用户配置。假设我们配置了一个raspi的用户，现在需要配置每次开机自动登录raspi。 su到root用户执行raspi-config或者用允许sudo的用户执行sudo raspi-config进入配置页面： 选择”3 Boot Options” 选择”B1 Desktop / CLI” 选择”B3 Desktop”，并”确定”后退出，在提示的重启界面选择”否” 在终端输入”sudo nano /etc/lightdm/lightdm.conf”，编辑该文件： 找到[Seat：*]配置块下的#autologin-user=，将其替换成autologin-user=raspi，raspi为需要默认登陆的账户名。 重启即可。 注意 初始系统中，默认使用pi用户自动登录，如果直接修改/etc/lightdm/lightdm.conf文件，也可以实现指定的用户自动登录，但是系统的后台仍会有一个pi用户的登陆进程。且该进程无法kill， 从而导致pi用户无法删除等问题。]]></content>
      <categories>
        <category>操作系统</category>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派开机启动与桌面快捷方式]]></title>
    <url>%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8%E4%B8%8E%E6%A1%8C%E9%9D%A2%E5%BF%AB%E6%8D%B7%E6%96%B9%E5%BC%8F.html</url>
    <content type="text"><![CDATA[背景虽然树莓派系统精简，性能一般，但是在当做桌面终端使用时，桌面快捷方式和一些开机自启动配置还是很有用的。 在树莓派中，双击运行以及程序自动运行都通过.desktop的桌面配置文件实现。 开机自启动在树莓派中，可以通过在当前用户目录下新建.config/autostart/目录，并在目录中创建对应的.desktop文件来实现开机自启动。 比如我们要实现树莓派开机就自动全屏打开chromium浏览器，并跳转到百度页面。 先进入.config目录，我们发现是没有autostart目录的 创建auotstart目录，并新建一个.desktop文件 123mkdir autostartcd autostartnano my.desktop 输入如下语句： 123[Desktop Entry] Type=Application Exec=chromium-browser --disable-popup-blocking --no-first-run -disable-desktop-notifications --kiosk &quot;https://www.baidu.com&quot; 保存之后重启系统即可 桌面快捷方式参考上面开机自启动的脚本，我们将脚本拷贝到桌面之后，双击打开即可实现相同的功能。 同时，可以通过右键该文件 - 属性，来进一步配置。其中，可以点击如下图标部分，来更换图标。 类似的，如果我们想做一个关机快捷方式，那么也新建一个后缀为.desktop的空白文件，输入如下代码即可。 1234[Desktop Entry]Name=ShutdownType=ApplicationExec=shutdown -h now 拓展特别要注意的是，PC桌面版的Raspi Debian 9 Stretch上的命令与树莓派版的并不一样。 比如上述功能在PC桌面版的系统中，代码应该为： 12345[Desktop Entry]name=baiduExec=chromium %U www.baidu.comType=ApplicationIcon=screensaver 系统默认也会安装很多类似的桌面配置文件，可以打开/usr/share/applications目录查看： 通过参考这些，可以配置更多内容。]]></content>
      <categories>
        <category>操作系统</category>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派用户配置]]></title>
    <url>%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E7%94%A8%E6%88%B7%E9%85%8D%E7%BD%AE.html</url>
    <content type="text"><![CDATA[背景树莓派是基于Debian系统的，总体用户配置与Debian Liunx类似，不同版本之间稍有不同，本文是基于树莓派 Debian 9 (stretch)。 默认用户 pi：这是树莓派的默认登陆用户。在初始化的系统的时候会提示配置密码，但即使设置了密码，在开机时，系统也会自动免密登陆。 root：这是树莓派的超级用户。默认没有设置密码，使用如下命令设置密码 1234sudo passwd root输入新的UNIX密码：重新输入新的UNIX密码：passwd：已成功更新密码 设置密码之后，可以在登陆页面选择使用root用户登陆，也可以通过su命令切换到root用户。 新建用户在管理员权限下，使用如下命令新建用户： 1sudo adduser raspi 按提示输入两次密码以及一些信息(可以为空)即可创建。同步会在/home路径下创建对应的文件夹。 新建的用户默认是普通用户，没有特殊权限。 sudo一般涉及系统配置等操作需要root权限，普通用户可以通过增加sudo前缀来使用管理员权限执行相关命令。 但不是所有的用户都可以使用sudo来执行命令的。在树莓派中，默认账号pi是有sudo权限的，而我们前面自己新建的用户则没有。 比如我们用raspi用户执行sudo ifconfig命令，虽然会提示我们输入密码，但是最终会提示报错，并记录日志。 那么如何将用户加入到可以使用sudo的分组呢？ 增加sudoer权限根据官网推荐，在管理员权限下运行如下命令 1sudo visudo 找到如下语句： 12# User privilege specificationroot ALL=(ALL:ALL) ALL 参考root增加： 1root ALL=(ALL:ALL) ALL 即可，如果希望可以不输入密码直接执行sudo命令，那么可以将该语句修改为： 1raspi ALL = NOPASSWD: ALL 再次使用raspi用户执行sudo命令： 注意： 也可用直接修改配置文件的方式增加sudoer： 1sudo nano /etc/sudoers sudoer的配置文件在修改sudoers配置文件时，我们可以看到这样一句话： 1#includedir /etc/sudoers.d 到对应的目录我们可以看到如下文件： 并且pi用户的配置了可以免密使用sudo权限。 这里是用了另外一种配置方法，我们可以参考同目录下README的说明。 删除sudoer权限对应的，删除某用户的sudo权限，只需要在配置文件中删除对应的条目即可。 但需要注意的是，pi用户是系统默认建立的用户，即使删除010_pi-nopasswd文件，依然无法删除pi用户的sudo权限。可以通过在文件010_pi-nopasswd文件的末尾加上~符号，使文件异常的方式来曲线实现。但该方法会导致整个sudo命令无法使用，即使在其他用户下使用sudo也会报错： 注：这方法是由于在sudo配置文件中不允许存在~符号，否则会导致解析出错。 比较建议的方法是，自己按需建立用户，并赋予权限之后，删除pi用户。 删除用户在管理员权限下，使用如下命令删除用户： 1sudo userdel -r raspi 这里的-r是用于在删除用户的同时删除他们的文件。 注意： 用户pi是树莓派的默认账户，并且在树莓派9 Debian Stretch中，系统开机默认会登录pi用户。即使在root用户下也无法直接删除，会有如下报错： 使用w命令可以发现pi用户默认登陆了，这时候就需要先取消pi用户的自动登录，确认没有pi用户的进程之后才可以删除。 扩展查看树莓派中所有的用户组： 1cat /etc/group 查看树莓派中所有的用户： 1cat /etc/passwd 其他更多的关于树莓派用户配置的信息可参考：https://www.raspberrypi.org/documentation/linux/usage/users.md]]></content>
      <categories>
        <category>操作系统</category>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派设置固定IP]]></title>
    <url>%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E8%AE%BE%E7%BD%AE%E5%9B%BA%E5%AE%9AIP.html</url>
    <content type="text"><![CDATA[背景树莓派默认是DHCP自动获取IP地址的，但在实际使用中，经常会有设置固定IP的情况。网上也有很多设置固定IP的方法，但要注意的是，不同版本的树莓派系统还是有区别的。 之前用的是树莓派8 （Jessie）版本，后来在树莓派9（stretch）中用相同的方法修改配置文件设置固定IP之后，导致整个网卡起不了了，无线和有线网卡都无法使用。这需要特别注意。 树莓派 Debian 8 (Jessie)在树莓派 Debian 8 (Jessie)版本的系统中，设置固定IP需要修改两个文件，详细操作如下： 备份原文件 12sudo cp /etc/network/interfaces /etc/network/interfaces.baksudo cp /etc/dhcpcd.conf /etc/dhcpcd.conf.bak 修改interfaces文件 1sudo nano /etc/network/interfaces 注释掉如下语句： 1iface eth0 inet dhcp 增加如下语句： 1234iface eth0 inet staticaddress xx.xx.xx.xxnetmask 255.255.255.0gateway xx.xx.xx.254 修改dhcpcd.conf文件 1sudo nano /etc/dhcpcd.conf 增加如下语句： 1234interface eth0static ip_address=xx.xx.xx.xx/24static routers=xx.xx.xx.254static domain_name_servers=xx.xx.xx.xx xx.xx.xx.xx 重启树莓派 1reboot 树莓派 Debian 9 (stretch)在树莓派 Debian 9 中，配置方法有了变化，我们看下/etc/network/interfaces文件： 根据提示，我们再看下dhcpcd.conf文件中的例子： 可见，直接修改这个文件即可： 备份原文件 1sudo cp /etc/dhcpcd.conf /etc/dhcpcd.conf.bak 修改dhcpcd.conf文件 增加如下语句： 1234interface eth0static ip_address=xx.xx.xx.xx/24static routers=xx.xx.xx.254static domain_name_servers=xx.xx.xx.xx xx.xx.xx.xx 重启树莓派 1reboot 需要注意的是，如果直接按照之前的方法，也修改了/etc/network/interfaces文件的话，会导致树莓派整个网卡挂了，无法连接网络。]]></content>
      <categories>
        <category>操作系统</category>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派系统瘦身]]></title>
    <url>%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E7%B3%BB%E7%BB%9F%E7%98%A6%E8%BA%AB.html</url>
    <content type="text"><![CDATA[背景树莓派系统一般烧录在sd卡中，sd卡的容量有限，当安装软件过多时就会占用很多不必要的空间。尤其在批量烧录树莓派镜像时，会增加镜像大小，导致烧录的时间变长。所以需要提前卸载一些不必要的软件，达到系统瘦身的效果。 系统瘦身在实现系统瘦身之前我们需要知道目前系统中安装了哪些软件包，都占用多少空间。 目前查看已安装的软件包有两种方法： 通过查看树莓派系统默认的 首选项 - Add/Remove Softeware查看。 通过运行dpkg -l查看安装的软件包 通过运行apt list —installed查看已安装的软件包 但以上三种方式都不能很直接的找出哪些是占用空间很大的软件，反而会列出过多的软件包。 这是强大的开源软件就派上了用场——Wajig。 这是一个基于Python的Debian包管理软件，可以很方便的对包进行管理并列出大容量的软件包。 操作如下： 安装wajia 1sudo apt-get install wajig 查看大软件包清单 1sudo wajig large 可以看出oracle-java8-jdk就占用了近350M的空间，如果不需要的话，那么可以直接卸载 卸载软件包 1sudo apt-get --purge autoremove oracle-java8-jdk 依次卸载对应的软件包即可，也可一次性列出多个软件包一并卸载： 1sudo apt-get --purge autoremove oracle-java8-jdk scratch2 拓展 常见可卸载软件列表： | 软件名 | 说明 || —————– | ————————- || wolfram-engine | wolfram编程语言引擎 || oracle-java | java jdk || scratch | scratch编程语言相关组件 || minecraft-pi | minecraft沙盒游戏开发软件 || x2goclient | x2go客户端，远程连接工具 || sonic-pi | 编曲软件 || claws-mail | 邮件客户端 || geany | 开源集成开发ide || idle | python ide || java-common | java软件包 || libreoffice | 办公软件 || nuscratch | scratch编程语言相关组件 || penguinspuzzle | 游戏 || smartsim | 仿真软件 || realvnc | 远程控制软件 || nodered | 物联网开发工具 || epiphany-browser | epiphany浏览器 || dillo | 浏览器 || xarchiver | 解压软件 || python-pygame | python游戏开发包 || python3-pygame | python3游戏开发包 || leafpad | 轻量级编辑器 || freepats | 音频处理 || lxtask | 轻量级任务管理器 || omxplayer | 视频播放软件 || vlc | 多媒体软件 | wajig： 更多关于Wajig的信息可参考：https://en.wikipedia.org/wiki/Wajig]]></content>
      <categories>
        <category>操作系统</category>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派更换软件源]]></title>
    <url>%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E6%9B%B4%E6%8D%A2%E8%BD%AF%E4%BB%B6%E6%BA%90.html</url>
    <content type="text"><![CDATA[背景树莓派默认更新源是官网地址，相当的慢，所以需要更新为国内的源镜像。 我们选择清华的树莓派源 https://mirrors.tuna.tsinghua.edu.cn/help/raspbian/ 更换源我们可以直接访问 清华大学开源软件镜像站查看，写的还是很详细的。主要操作如下： 备份源文件： 12sudo cp /etc/apt/sources.list /etc/apt/sources.list.baksudo cp /etc/apt/sources.list.d/raspi.list /etc/apt/sources.list.d/raspi.list.bak 修改源镜像 这里需要知道自己所用的树莓派是什么版本，主要版本有如下几个： Debian 9 (stretch) 123456# 编辑 `/etc/apt/sources.list` 文件，删除原文件所有内容，用以下内容取代：deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main non-free contribdeb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main non-free contrib# 编辑 `/etc/apt/sources.list.d/raspi.list` 文件，删除原文件所有内容，用以下内容取代：deb http://mirrors.tuna.tsinghua.edu.cn/raspberrypi/ stretch main Debian 8 (jessie) 123456# 编辑 `/etc/apt/sources.list` 文件，删除原文件所有内容，用以下内容取代：deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ jessie main non-free contribdeb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ jessie main non-free contrib# 编辑 `/etc/apt/sources.list.d/raspi.list` 文件，删除原文件所有内容，用以下内容取代：deb http://mirrors.tuna.tsinghua.edu.cn/raspberrypi/ jessie main Debian 7 (wheezy) 123456# 编辑 `/etc/apt/sources.list` 文件，删除原文件所有内容，用以下内容取代：deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ wheezy main non-free contribdeb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ wheezy main non-free contrib# 编辑 `/etc/apt/sources.list.d/raspi.list` 文件，删除原文件所有内容，用以下内容取代：deb http://mirrors.tuna.tsinghua.edu.cn/raspberrypi/ wheezy main 目前最新版本的树莓派系统为Debian 9 (stretch)，后续如果有更新，可以参考官网说明。 更新系统软件列表 1sudo apt-get update 桌面版树莓派桌面版的树莓派系统其实是debian系统，仅需修改/etc/apt/source.list. 备份源文件： 1sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak 根据自己的版本修改源文件 stretch 123456789# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch main contrib non-free# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch-updates main contrib non-free# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch-updates main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch-backports main contrib non-free# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ stretch-backports main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian-security stretch/updates main contrib non-free# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security stretch/updates main contrib non-free jessie 123456789# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie main contrib non-free# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie-updates main contrib non-free# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie-updates main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie-backports main contrib non-free# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ jessie-backports main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian-security jessie/updates main contrib non-free# deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security jessie/updates main contrib non-free 目前最新版本为stretch，更多版本可参考：https://mirrors.tuna.tsinghua.edu.cn/help/debian/ 更新系统软件列表 1sudo apt-get update 其他树莓派官方认可的源有很多，可参考如下网页：http://www.raspbian.org/RaspbianMirrors 选择一个离自己最近的源即可。]]></content>
      <categories>
        <category>操作系统</category>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python数据统计和可视化3]]></title>
    <url>%2FPython%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%963.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 利用免费财经数据接口 TuShare获取和分析数据 安装 1pip install tushare 初始化程序 12345import pandas as pdimport numpy as npimport tushare as tsdf = ts.get_hist_data('600036',start='2018-07-01',end='2018-12-31') 利用 Tushare 包中的接口函数获取招商银行(股票代码 600036)2018 年下半年的股票数据并完成如下数据处理和分析任务: 1、数据只保留 date、open、high、close、low 和 volume 这几个属性，并按时间先后顺序对数据进行排序; 12df = df[['open', 'high', 'close', 'low', 'volume']]df.sort_index(inplace=True) 2、输出这半年内成交量最低和最高那两天的日期和分别的成交量; 123456min_day = df.sort_values('volume').iloc[0]min_volume = min_day.volumemin_volume_day = min_day.namemax_day = df.sort_values('volume', ascending=False).iloc[0]max_volume = max_day.volumemax_volume_day = max_day.name 3、列出成交量在 1000000 以上的记录; 1df[df.volume&gt;1000000] 4、计算这半年中收盘价(close)高于开盘价(open)的天数; 1len(df[df.close&gt;df.open]) 5、计算前后两天开盘价的涨跌情况，用两种方式表示，第一种输出每两天之间的差值(后一天减去前一天)，第二种输出一个开盘价涨跌列表，涨用 1 表示，跌用-1 表示; 1234# 第一种输出每两天之间的差值(后一天减去前一天)，df.open.diff()# 第二种输出一个开盘价涨跌列表，涨用 1 表示，跌用-1 表示;np.sign(np.diff(df.open)) 6、计算每月收盘价的平均值。 12df.index = pd.DatetimeIndex(df.index)df.open.resample('M').mean()]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python数据统计和可视化2]]></title>
    <url>%2FPython%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%962.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 5.5 简单统计与处理 5.6 Grouping 5.7 Merge]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派系统镜像基本操作]]></title>
    <url>%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E7%B3%BB%E7%BB%9F%E9%95%9C%E5%83%8F%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C.html</url>
    <content type="text"><![CDATA[背景以下操作均在Liunx环境下进行使用命令进行，Windows可以采用一些可视化工具操作。 其中备份文件压缩部分，在Mac环境下无法执行。 树莓派的系统放在TF存储卡上，在进行系统导入，备份时，需通过读卡器连接电脑。 系统镜像准备树莓派的操作系统镜像文件有两种方式获取： 通过访问树莓派官方网站下载最新版系统镜像获取。 通过直接把已装好系统的树莓派存储卡直接导出为img镜像文件获取。也就是我们后续说到的导出系统备份文件。 系统镜像导入TF卡 将树莓派的TF卡通过读卡器连接电脑 查看TF卡所在的盘符： 1234567891011zhengk@ubuntu:/tmp$ sudo fdisk -lDisk /dev/sdc：14.5 GiB，15523119104 字节，30318592 个扇区单元：扇区 / 1 * 512 = 512 字节扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节磁盘标签类型：dos磁盘标识符：0xd602174c设备 启动 起点 末尾 扇区 大小 Id 类型/dev/sdc1 8192 137215 129024 63M c W95 FAT32 (LBA)/dev/sdc2 137216 11130552 10993337 5.2G 83 Linux 可见，TF卡被挂载在/dev/sdc上。 弹出TF卡所在的盘符： 这里unmount操作，是为了防止挂载之后存在其他进程访问而导致后续操作失败，所以必须先进行unmount操作。 12zhengk@ubuntu:/tmp$ umount /dev/sdc1zhengk@ubuntu:/tmp$ umount /dev/sdc2 重建分区： 步骤入下： 用p选项查看分区 用d选项删除分区 用n选项新建分区 用t选项修改分区类型 用w选项写入使生效 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788zhengk@ubuntu:/tmp$ sudo fdisk /dev/sdc欢迎使用 fdisk (util-linux 2.31.1)。更改将停留在内存中，直到您决定将更改写入磁盘。使用写入命令前请三思。命令(输入 m 获取帮助)： pDisk /dev/sdc：14.5 GiB，15523119104 字节，30318592 个扇区单元：扇区 / 1 * 512 = 512 字节扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节磁盘标签类型：dos磁盘标识符：0xcd48578f设备 启动 起点 末尾 扇区 大小 Id 类型/dev/sdc1 8192 96042 87851 42.9M c W95 FAT32 (LBA)/dev/sdc2 98304 6799359 6701056 3.2G 83 Linux命令(输入 m 获取帮助)： d分区号 (1,2, 默认 2): 分区 2 已删除。命令(输入 m 获取帮助)： d已选择分区 1分区 1 已删除。命令(输入 m 获取帮助)： n分区类型 p 主分区 (0个主分区，0个扩展分区，4空闲) e 扩展分区 (逻辑分区容器)选择 (默认 p)： 将使用默认回应 p。分区号 (1-4, 默认 1): 第一个扇区 (2048-30318591, 默认 2048): 上个扇区，+sectors 或 +size&#123;K,M,G,T,P&#125; (2048-30318591, 默认 30318591): 创建了一个新分区 1，类型为“Linux”，大小为 14.5 GiB。命令(输入 m 获取帮助)： t已选择分区 1Hex 代码(输入 L 列出所有代码)： L 0 空 24 NEC DOS 81 Minix / 旧 Linu bf Solaris 1 FAT12 27 隐藏的 NTFS Win 82 Linux swap / So c1 DRDOS/sec (FAT- 2 XENIX root 39 Plan 9 83 Linux c4 DRDOS/sec (FAT- 3 XENIX usr 3c PartitionMagic 84 OS/2 隐藏 或 In c6 DRDOS/sec (FAT- 4 FAT16 &lt;32M 40 Venix 80286 85 Linux 扩展 c7 Syrinx 5 扩展 41 PPC PReP Boot 86 NTFS 卷集 da 非文件系统数据 6 FAT16 42 SFS 87 NTFS 卷集 db CP/M / CTOS / . 7 HPFS/NTFS/exFAT 4d QNX4.x 88 Linux 纯文本 de Dell 工具 8 AIX 4e QNX4.x 第2部分 8e Linux LVM df BootIt 9 AIX 可启动 4f QNX4.x 第3部分 93 Amoeba e1 DOS 访问 a OS/2 启动管理器 50 OnTrack DM 94 Amoeba BBT e3 DOS R/O b W95 FAT32 51 OnTrack DM6 Aux 9f BSD/OS e4 SpeedStor c W95 FAT32 (LBA) 52 CP/M a0 IBM Thinkpad 休 ea Rufus 对齐 e W95 FAT16 (LBA) 53 OnTrack DM6 Aux a5 FreeBSD eb BeOS fs f W95 扩展 (LBA) 54 OnTrackDM6 a6 OpenBSD ee GPT 10 OPUS 55 EZ-Drive a7 NeXTSTEP ef EFI (FAT-12/16/11 隐藏的 FAT12 56 Golden Bow a8 Darwin UFS f0 Linux/PA-RISC 12 Compaq 诊断 5c Priam Edisk a9 NetBSD f1 SpeedStor 14 隐藏的 FAT16 &lt;3 61 SpeedStor ab Darwin 启动 f4 SpeedStor 16 隐藏的 FAT16 63 GNU HURD 或 Sys af HFS / HFS+ f2 DOS 次要 17 隐藏的 HPFS/NTF 64 Novell Netware b7 BSDI fs fb VMware VMFS 18 AST 智能睡眠 65 Novell Netware b8 BSDI swap fc VMware VMKCORE 1b 隐藏的 W95 FAT3 70 DiskSecure 多启 bb Boot Wizard 隐 fd Linux raid 自动1c 隐藏的 W95 FAT3 75 PC/IX bc Acronis FAT32 L fe LANstep 1e 隐藏的 W95 FAT1 80 旧 Minix be Solaris 启动 ff BBT Hex 代码(输入 L 列出所有代码)： b已将分区“Linux”的类型更改为“W95 FAT32”。命令(输入 m 获取帮助)： pDisk /dev/sdc：14.5 GiB，15523119104 字节，30318592 个扇区单元：扇区 / 1 * 512 = 512 字节扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节磁盘标签类型：dos磁盘标识符：0xcd48578f设备 启动 起点 末尾 扇区 大小 Id 类型/dev/sdc1 2048 30318591 30316544 14.5G b W95 FAT32命令(输入 m 获取帮助)： w分区表已调整。将调用 ioctl() 来重新读分区表。正在同步磁盘。 格式化TF卡： 12zhengk@ubuntu:/tmp$ sudo mkfs.vfat /dev/sdc1mkfs.fat 4.1 (2017-01-24) 格式化为msdos格式。 导入镜像文件： 1234zhengk@ubuntu:/tmp$ sudo dd bs=1M if=2019-04-08-raspbian-stretch.img of=/dev/sdc 记录了3320+0 的读入记录了3320+0 的写出3481272320 bytes (3.5 GB, 3.2 GiB) copied, 331.952 s, 10.5 MB/s 注意：可通过另外开一个shell窗口，输入如下命令打印导入进度： 1sudo watch -n 5 pkill -USR1 ^dd$ 此时，原导入shell窗口会每隔5秒打印出如下进度： 12345记录了1640+0 的读入记录了1640+0 的写出1719664640 bytes (1.7 GB, 1.6 GiB) copied, 79.5407 s, 21.6 MB/s记录了1741+0 的读入记录了1741+0 的写出 将TF卡插回至树莓派即可。 导出系统备份文件 将树莓派的TF卡通过读卡器连接电脑 查看TF卡所在的盘符 1234567891011zhengk@ubuntu:/tmp$ sudo fdisk -lDisk /dev/sdc：14.5 GiB，15523119104 字节，30318592 个扇区单元：扇区 / 1 * 512 = 512 字节扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节磁盘标签类型：dos磁盘标识符：0xcd48578f设备 启动 起点 末尾 扇区 大小 Id 类型/dev/sdc1 8192 96042 87851 42.9M c W95 FAT32 (LBA)/dev/sdc2 98304 6799359 6701056 3.2G 83 Linux 弹出TF卡所在的盘符： 这里unmount操作，是为了防止挂载之后存在其他进程访问而导致后续操作失败，所以必须先进行unmount操作。 12zhengk@ubuntu:/tmp$ umount /dev/sdc1zhengk@ubuntu:/tmp$ umount /dev/sdc2 导出镜像文件到当前目录即可 1234zhengk@ubuntu:/tmp$ sudo dd if=/dev/sdc of=raspberrypi.img记录了30318592+0 的读入记录了30318592+0 的写出15523119104 bytes (16 GB, 14 GiB) copied, 212.231 s, 73.1 MB/s 注意：过程中可使用如下命令通过查看文件大小变化来感知进度： 12345watch -d -n 5 ls -lh ~/tmp/Every 5.0s: ls -lh /tmp/ ubuntu: Sat Apr 13 17:30:02 2019总用量 18G-rw-r--r-- 1 root root 15G 4月 13 17:29 raspberrypi.img 系统备份文件压缩 下载树莓派镜像压缩工具： 1wget https://raw.githubusercontent.com/Drewsif/PiShrink/master/pishrink.sh 设置可执行权限： 1chmod +x pishrink.sh 将工具添加到环境变量 1sudo mv pishrink.sh /usr/local/bin 对备份的镜像文件进行压缩 12345678910sudo pishrink.sh raspberrypi.imgrootfs: 92570/209664 files (0.1% non-contiguous), 679211/837632 blocksresize2fs 1.44.1 (24-Mar-2018)resize2fs 1.44.1 (24-Mar-2018)Resizing the filesystem on /dev/loop12 to 813317 (4k) blocks.Begin pass 3 (max = 26)Scanning inode table XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXThe filesystem on /dev/loop12 is now 813317 (4k) blocks long.Shrunk raspberrypi.img from 15G to 3.2G 可以看到这里已经将备份文件从15G压缩到了3.2G。不过，这里的压缩处理，会直接修改备份的镜像文件，而不是另外生成一个压缩版本的镜像文件，可先自行备份原镜像文件。 注意： 这个压缩工具是通过shell脚本实现的，仅支持linux操作系统，在MacOS下会因为没有parted工具而失败。 如果你的linux系统是中文本，那么可能会因为字符的问题导致运行报错： 1234567891011121314$ sudo pishrink.sh raspberry-image-2.0-保留root权限.img Creating new /etc/rc.local/dev/loop7：101933/971040 文件（0.3% 为非连续的）， 712924/3872384 块resize2fs 1.44.1 (24-Mar-2018)/usr/local/bin/pishrink.sh: 行 151: [[: 预计文件系统的最小尺寸：770851: 语法错误: 需要操作数 (错误符号是 "预计文件系统的最小尺寸：770851")/usr/local/bin/pishrink.sh: 行 157: 3872384 - 预计文件系统的最小尺寸：770851: 语法错误: 需要操作数 (错误符号是 "预计文件系统的最小尺寸：770851")resize2fs 1.44.1 (24-Mar-2018)resize2fs: 无效的新大小： 预计文件系统的最小尺寸：770851ERROR: resize2fs failed...mount: /tmp/tmp.Y7YrhlFNO7: can't read superblock on /dev/loop7.mv: 无法获取'/tmp/tmp.Y7YrhlFNO7/etc/rc.local.bak' 的文件状态(stat): 没有那个文件或目录umount: /tmp/tmp.Y7YrhlFNO7: not mounted.losetup: /dev/loop7：断开失败: 没有那个设备或地址 解决方法为，指定英文语言运行，命令如下： 12345678910zhengk@ubuntu:/tmp$ sudo LANG=en_US pishrink.sh raspberrypi.img rootfs: 92570/209664 files (0.1% non-contiguous), 679211/837632 blocksresize2fs 1.44.1 (24-Mar-2018)resize2fs 1.44.1 (24-Mar-2018)Resizing the filesystem on /dev/loop12 to 813317 (4k) blocks.Begin pass 3 (max = 26)Scanning inode table XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXThe filesystem on /dev/loop12 is now 813317 (4k) blocks long.Shrunk raspberrypi.img from 15G to 3.2G 可参考issue：https://github.com/Drewsif/PiShrink/issues/85 系统备份文件恢复系统备份文件的恢复与导入系统镜像文件一样，使用备份的镜像导入即可。 其他 dd命令： 上述在进行导入、导出时都用到了dd命令，这是在Liunx和Unix下通用的命令，用于指定大小的块拷贝一个文件，并在拷贝的同时进行指定的转换。 可参考：https://baike.baidu.com/item/dd命令 pishrink.sh 该备份文件压缩脚本是在Github上开源的工具，可访问https://github.com/Drewsif/PiShrink查看更多信息。 fdisk命令 123456789101112131415161718192021222324252627282930313233343536373839404142欢迎使用 fdisk (util-linux 2.31.1)。更改将停留在内存中，直到您决定将更改写入磁盘。使用写入命令前请三思。命令(输入 m 获取帮助)： m帮助： DOS (MBR) a 开关 可启动 标志 b 编辑嵌套的 BSD 磁盘标签 c 开关 dos 兼容性标志 常规 d 删除分区 F 列出未分区的空闲区 l 列出已知分区类型 n 添加新分区 p 打印分区表 t 更改分区类型 v 检查分区表 i 打印某个分区的相关信息 杂项 m 打印此菜单 u 更改 显示/记录 单位 x 更多功能(仅限专业人员) 脚本 I 从 sfdisk 脚本文件加载磁盘布局 O 将磁盘布局转储为 sfdisk 脚本文件 保存并退出 w 将分区表写入磁盘并退出 q 退出而不保存更改 新建空磁盘标签 g 新建一份 GPT 分区表 G 新建一份空 GPT (IRIX) 分区表 o 新建一份的空 DOS 分区表 s 新建一份空 Sun 分区表]]></content>
      <categories>
        <category>操作系统</category>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python数据统计和可视化1]]></title>
    <url>%2FPython%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%961.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 5.1 便捷数据获取 5.2 数据准备 5.3 数据显示 5.4 数据选择]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-46 全排列]]></title>
    <url>%2FLeetCode-46-%E5%85%A8%E6%8E%92%E5%88%97.html</url>
    <content type="text"><![CDATA[题目：全排列给定一个没有重复数字的序列，返回其所有可能的全排列。 示例: 12345678910输入: [1,2,3]输出:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] 思路这题需要用到递归的思想： 当只有两个数时，依次将数字取出放到最后即可。 当有多余两个数时，依次将第i个数取出，放置到剩余部分的最后，剩余部分看做一个整体（即nums[:i] + nums[i + 1:]）。 对剩余部分进行递归即可。 实现： 12345678910class Solution: def permute(self, nums: List[int]) -&gt; List[List[int]]: out = [] if len(nums) == 1: return [nums] for index, item in enumerate(nums): res = nums[:index] + nums[index + 1:] for j in self.permute(res): out.append(j + [item]) return out 其他思考其实Python的itertools库中提供了内置的全排列方法: 12345678class permutations(object): """ permutations(iterable[, r]) --&gt; permutations object Return successive r-length permutations of elements in the iterable. permutations(range(3), 2) --&gt; (0,1), (0,2), (1,0), (1,2), (2,0), (2,1) """ 现实中可以直接调用： 1234567class Solution: def permute(self, nums): import itertools out = [] for i in itertools.permutations(nums, len(nums)): out.append(i) return out 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-43 字符串相乘]]></title>
    <url>%2FLeetCode-43-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E4%B9%98.html</url>
    <content type="text"><![CDATA[题目：字符串相乘给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。 示例 1: 12输入: num1 = &quot;2&quot;, num2 = &quot;3&quot;输出: &quot;6&quot; 示例 2: 12输入: num1 = &quot;123&quot;, num2 = &quot;456&quot;输出: &quot;56088&quot; 说明： num1 和 num2 的长度小于110。 num1 和 num2 只包含数字 0-9。 num1 和 num2 均不以零开头，除非是数字 0 本身。 不能使用任何标准库的大数类型（比如 BigInteger）或直接将输入转换为整数来处理。 思路由于Python中的int类型是支持大数的，所以其实没有处理会不会数值过大的必要。 题目中说不允许直接转换成整数处理，其实就是希望我们实现一下乘法的过程，也就是以前学的竖式运算的过程。 1234567 11x 12----- 22 11----- 132 思路如下： 初始化结果res=0 进行嵌套循环，从右边依次取num2的每个字符串num2[-j]，转换成int类型，依次与num1从右边开始逐个字符串num1[-i]相乘 相乘过程中涉及到进位，可以找到规律是10 ** (i+j-2)。 实现如下： 1234567class Solution: def multiply(self, num1: str, num2: str) -&gt; str: res = 0 for i in range(1, len(num1) + 1): for j in range(1, len(num2) + 1): res += int(num1[-i]) * int(num2[-j]) * 10 ** (i + j - 2) return str(res) 其他思考如果不考虑要求，直接使用int()来转换的话，那么更简单了： 123class Solution: def multiply(self, num1: str, num2: str) -&gt; str: return str(int(num1) * int(num2)) 运行效果与上面相比的话： 1234567891011121314151617181920212223242526import timeclass Solution: def multiply1(self, num1, num2): res = 0 for i in range(1, len(num1) + 1): for j in range(1, len(num2) + 1): res += int(num1[-i]) * int(num2[-j]) * 10 ** (i + j - 2) return str(res) def multiply2(self, num1, num2): return str(int(num1) * int(num2))if __name__ == '__main__': num1 = "1111111" num2 = "2222222" t1 = time.time() print(Solution().multiply1(num1, num2)) t2 = time.time() print('Solution One spent &#123;&#125; seconds'.format(t2-t1)) t1 = time.time() print(Solution().multiply2(num1, num2)) t2 = time.time() print('Solution Two spent &#123;&#125; seconds'.format(t2 - t1)) 输出如下： 12342469135308642Solution One spent 7.581710815429688e-05 seconds2469135308642Solution Two spent 7.152557373046875e-06 seconds 还是差了一个数量级的，再实际使用中，还是用原生的方法比较好。 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-33 搜索旋转排序数组]]></title>
    <url>%2FLeetCode-33-%E6%90%9C%E7%B4%A2%E6%97%8B%E8%BD%AC%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84.html</url>
    <content type="text"><![CDATA[题目：搜索旋转排序数组假设按照升序排序的数组在预先未知的某个点上进行了旋转。 ( 例如，数组 [0,1,2,4,5,6,7] 可能变为 [4,5,6,7,0,1,2] )。 搜索一个给定的目标值，如果数组中存在这个目标值，则返回它的索引，否则返回 -1 。 你可以假设数组中不存在重复的元素。 你的算法时间复杂度必须是 O(log n) 级别。 示例 1: 12输入: nums = [4,5,6,7,0,1,2], target = 0输出: 4 示例 2: 12输入: nums = [4,5,6,7,0,1,2], target = 3输出: -1 思路这题其实还是得使用二分法，但由于数组并不是完全有序的，所以得对二分法进行一定的修改。 初始化low = 0, high = len(nums) - 1， mid = (low + high) // 2 如果刚好target就是中间值，那就直接返回 否则得继续查找，那么就只有两种情况： 旋转点在右边，那么左边为有序数组，右边为无序数组 若target在左边，那么high = mid - 1 若target在右边，那么low = mid + 1 旋转点在左边，那么左边为无序数组，右边为有序数组 若target在右边，那么low = mid + 1 若target在左边，那么high = mid - 1 注意：在判断范围的时候，需要判断边界。 实现： 12345678910111213141516171819202122232425class Solution: def search(self, nums: List[int], target: int) -&gt; int: low = 0 high = len(nums) - 1 while low &lt;= high: mid = (low + high) // 2 if nums[mid] == target: return mid else: # 左边有序 if nums[low] &lt;= nums[mid]: # target在左边 if nums[low] &lt;= target &lt; nums[mid]: high = mid - 1 # target在右边 else: low = mid + 1 # 右边有序 else: # target在右边 if nums[mid] &lt; target &lt;= nums[high]: low = mid + 1 else: high = mid - 1 return -1 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-26 删除排序数组中的重复项]]></title>
    <url>%2FLeetCode-26-%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9.html</url>
    <content type="text"><![CDATA[题目：删除排序数组中的重复项给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素只出现一次，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 示例 1: 12345给定数组 nums = [1,1,2], 函数应该返回新的长度 2, 并且原数组 nums 的前两个元素被修改为 1, 2。 你不需要考虑数组中超出新长度后面的元素。 示例 2: 12345给定 nums = [0,0,1,1,1,2,2,3,3,4],函数应该返回新的长度 5, 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4。你不需要考虑数组中超出新长度后面的元素。 说明: 为什么返回数值是整数，但输出的答案是数组呢? 请注意，输入数组是以“引用”方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。 你可以想象内部操作如下: 12345678// nums 是以“引用”方式传递的。也就是说，不对实参做任何拷贝int len = removeDuplicates(nums);// 在函数里修改输入数组对于调用者是可见的。// 根据你的函数返回的长度, 它会打印出数组中该长度范围内的所有元素。for (int i = 0; i &lt; len; i++) &#123; print(nums[i]);&#125; 思路这题的题目表述稍微有点绕，其实意思比较简单，给定一个有序数组，存在重复的情况，要求在不新建一个数组的情况下，无视重复的数组，把非重复的数值往往前移动，并返回非重复数组的长度。 比如给定： 1nums = [0,0,1,1,1,2,2,3,3,4] 处理完后的数组为： 1nums = [0,1,2,3,4,2,2,3,3,4] 返回值为5，系统会自动输出前5个数： 1[0,1,2,3,4] 可以用双指针法： 初始化指针i从0开始，j从1开始一直遍历到len(lists) 如果j指向的数与i指向的数一样，那么继续遍历 如果j指向的数与i指向的数不一样，那么把i指向的数修改为j指向的数并继续 最后返回长度是i+1，此时数组中前i+1个数也都是非重复的数值了。 实现： 12345678910class Solution: def removeDuplicates(self, nums) -&gt; int: if len(nums) == 0: return 0 i = 0 for j in range(1, len(nums)): if nums[i] != nums[j]: i += 1 nums[i] = nums[j] return i + 1 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-23 合并K个排序链表]]></title>
    <url>%2FLeetCode-23-%E5%90%88%E5%B9%B6K%E4%B8%AA%E6%8E%92%E5%BA%8F%E9%93%BE%E8%A1%A8.html</url>
    <content type="text"><![CDATA[题目：合并K个排序链表合并 k 个排序链表，返回合并后的排序链表。请分析和描述算法的复杂度。 示例: 1234567输入:[ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6]输出: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 思路这题可以看成是上一题合并两个有序列表）的扩展，可以直接遍历K个链表，然后依次进行两两排序。 实现如下： 1234567891011121314151617181920212223242526272829303132# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Nonedef mergeTwoLists(l1: ListNode, l2: ListNode) -&gt; ListNode: outNode = ListNode(None) curNode = outNode while l1 and l2: if l1.val &lt; l2.val: curNode.next = l1; l1 = l1.next else: curNode.next = l2; l2 = l2.next curNode = curNode.next if l1: curNode.next = l1; else: curNode.next = l2 return outNode.nextclass Solution: def mergeKLists(self, lists: List[ListNode]) -&gt; ListNode: if len(lists) == 0: return None tempNode = lists[0] for i in range(1, len(lists)): tempOutNode = mergeTwoLists(tempNode, lists[i]) tempNode = tempOutNode return tempNode 这里要注意输入为空列表的情况。 优化方法 1上面的方法在提交运行中，会超时，那么怎么加速一下呢，参考了大神们的解题，可以考虑通过递归的方式，思路如下： 当输入列表长度为0时，直接返回None 当输入列表长度为1时，直接返回列表内容：lists[0] 当输入列表长度为2时，返回这两个列表合并的运算结果：mergeTwoLists(lists[0], lists[1]) 当输入的列表长度大于2时，则先取长度的中位数mid，将列表分列两部分:l1 = lists[:mid]，l2=lists[mid:]。然后分别进行递归：self.mergeKLists(l1)，self.mergeKLists(l2)，并将最终递归的结果进行合并。 实现如下： 123456789101112131415161718192021222324252627282930313233343536# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Nonedef mergeTwoLists(l1: ListNode, l2: ListNode) -&gt; ListNode: outNode = ListNode(None) curNode = outNode while l1 and l2: if l1.val &lt; l2.val: curNode.next = l1; l1 = l1.next else: curNode.next = l2; l2 = l2.next curNode = curNode.next if l1: curNode.next = l1; else: curNode.next = l2 return outNode.nextclass Solution: def mergeKLists(self, lists: List[ListNode]) -&gt; ListNode: if len(lists) == 0: return None if len(lists) == 1: return lists[0] if len(lists) == 2: return mergeTwoLists(lists[0], lists[1]) tempNode = lists[0] mid = len(lists) // 2 l1 = lists[:mid] l2 = lists[mid:] return mergeTwoLists(self.mergeKLists(l1), self.mergeKLists(l2)) 优化方法 2其实换一种思路，正所谓不要重复的造轮子，已经有的常用的排序sort()方法其实很高效了，虽然有违算法题的初衷。 思路如下： 遍历所有的列表，把所有的值全部都写入一个list 通过list.sort()进行排序 然后将list转化为ListNode() 实现如下： 1234567891011121314151617181920# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeKLists(self, lists: List[ListNode]) -&gt; ListNode: allList = [] outNode = ListNode(0) curNode = outNode for item in lists: while item: allList.append(item.val) item = item.next allList.sort() for val in allList: curNode.next = ListNode(val) curNode = curNode.next return outNode.next 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[强大的数据结构和Python扩展库3]]></title>
    <url>%2F%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8CPython%E6%89%A9%E5%B1%95%E5%BA%933.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 4.6 变长字典Series 4.7 DataFrame 创建DataFrame小练习已知有一个列表中存放了一组音乐数据: music_data = [(“the rolling stones”,”Satisfaction”),(“Beatles”,”Let It Be”),(“Guns N’ Roses”,”Don’t Cry”),(“Metallica”,”Nothing Else Matters”)] 请根据这组数据创建一个如下的 DataFrame: 12345 singer song_name1 the rolling stones Satisfaction2 Beatles Let It Be3 Guns N&apos; Roses Don&apos;t Cry4 Metallica Nothing Else Matters 实现： 1234567import pandas as pdmusic_data = [("the rolling stones","Satisfaction"),("Beatles","Let It Be"),("Guns N' Roses","Don't Cry"),("Metallica","Nothing Else Matters")]df = pd.DataFrame(music_data, index=range(1, 5), columns=['singer', 'song_name'])print(df) 字典相关编程题 1. 找人程序 题目内容： 有5名某界大佬xiaoyun、xiaohong、xiaoteng、xiaoyi和xiaoyang，其QQ号分别是88888、5555555、11111、12341234和1212121，用字典将这些数据组织起来。编程实现以下功能：用户输入某一个大佬的姓名后输出其QQ号，如果输入的姓名不在字典中则输出字符串“Not Found”。 输入格式: ​ 字符串 输出格式： ​ 字符串 输入样例： ​ xiaoyun 输出样例： ​ 88888 1234567891011121314151617def find_person(dict_users, strU): if strU in dict_users: return dict_users[strU] else: return 'Not Found'if __name__ == "__main__": dict_users = &#123; 'xiaoyun': '88888', 'xiaohong': '5555555', 'xiaoteng': '11111', 'xiaoyang': '12341234', 'xiaoyi': '1212121' &#125; strU = input() print(find_person(dict_users, strU)) 2.统计句子中的词频 题目内容： 对于一个已分词的句子（可方便地扩展到统计文件中的词频）： 我/是/一个/测试/句子/，/大家/赶快/来/统计/我/吧/，/大家/赶快/来/统计/我/吧/，/大家/赶快/来/统计/我/吧/，/重要/事情/说/三遍/！ 可以用collections模块中的Counter()函数方便地统计词频，例如可用如下代码： 1234567import collectionsimport copys = "我/是/一个/测试/句子/，/大家/赶快/来/统计/我/吧/，/大家/赶快/来/统计/我/吧/，/大家/赶快/来/统计/我/吧/，/重要/事情/说/三遍/！/"s_list = s.split('/') # 为避免迭代时修改迭代对象本身，创建一个列表的深拷贝，也可用浅拷贝s_list_backup = s_list[:]s_list_backup = copy.deepcopy(s_list)[s_list.remove(item) for item in s_list_backup if item in '，。！”“'] 输入格式: 字符串 输出格式： 整数 输入样例（因为oj系统限制，测试用例设为判断英文单词个数（不区分大小写，全部转换成小写字符处理），请注意英文标点，假设仅包含,和.）： not 输出样例： 2 123456789101112131415def countfeq(s): out = &#123;&#125; words = s.split() for word in words: if word[-1] in [',', '.', ')', ':']: word = word[:-1] out[word] = out.get(word, 0) + 1 return outif __name__ == "__main__": s = "Not clumsy person in this world, only lazy people, only people can not hold out until the last." s_dict = countfeq(s.lower()) word = input() print(s_dict.get(word, 0))]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[强大的数据结构和Python扩展库2]]></title>
    <url>%2F%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8CPython%E6%89%A9%E5%B1%95%E5%BA%932.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 4.4 扩展库SciPy 4.5 ndarray 讨论NumPy中的通用函数与math库中函数的比较： 1234567891011121314151617import timeimport mathimport numpy as npx = np.random.randint(100, size=(1000, 1000))m_sum = 0t_m1 = time.process_time()for row in x: m_sum += math.fsum(row)t_m2 = time.process_time()print(m_sum)t_n1 = time.process_time()n_sum = x.sum()t_n2 = time.process_time()print(n_sum)print('Running time of math:', t_m2 - t_m1)print('Running time of numpy:',t_n2 - t_n1) 比较一个1000*1000的矩阵求和，输出结果为： 12Running time of math: 0.059509000000000034Running time of numpy: 0.0006780000000000674 相差两个数量级]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[强大的数据结构和Python扩展库1]]></title>
    <url>%2F%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8CPython%E6%89%A9%E5%B1%95%E5%BA%931.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 4.1 &amp; 4.2 为什么需要字典 &amp; 字典的使用 测验题：字典变成小练习项目用字典创建一个平台的用户信息（包含用户名和密码）管理系统，新用户可以用与现有系统帐号不冲突的用户名创建帐号，已存在的老用户则可以用用户名和密码登陆重返系统。你完成了吗？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445# -*- coding: utf-8 -*-# author：zhengkusers = &#123;&#125;def newusers(): global users name = input('Please input a name:') if name in users: name = input('This name is already used, Please input an other name:') else: password = input('Please input your password:') users[name] = password print('Your information is recorded!')def oldusers(): global users name = input('Please input your name:') password = input('Please input your password:') if password == users.get(name): print(name, 'welcome back ') else: print('login incorrect')def login(): while True: option = input('''(N)ew User Login (O)ld User Login(E)xit"""Enter the option''') if option == 'N': newusers() elif option == 'O': oldusers() elif option == 'E': breakif __name__ == '__main__': login() 字典经典应用编程小例 1、从键盘输入一个英文句子，除单词和空格外句子中只包含“,”、“.”、“’”、“””和“!”这几个标点符号，统计句子中包括的每个单词(将句中大写全部转换成小写)的词频并将结果存入字典中并输出。 123456789if __name__ == '__main__': out = &#123;&#125; sentence = input('Please input a sentence:') words = sentence.lower().split() for word in words: if word[-1] in ',.\'"!': word = word[:-1] out[word] = out.get(word, 0) + 1 print(out) 2、自定义函数 twonums_sum(n, lst)，在列表 lst 中查找是否有两数之和等于 n，若有 则返回两数的下标，否则返回-1。对于一个不包含重复数字的有序列表[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 18, 19, 20, 21, 29, 34, 54, 65]，从键盘输入 n，调用函数 twonums_sum()输出满足条件的两个数的下标(找到一组即可且要求其中的一个数尽量小)， 若所有数均不满足条件则输出“not found”。 [输入样例] 17 [输出样例] (1, 10) 123456789101112def twonums_sum(n, lst): for x in range(len(lst)): other = n - lst[x] if other in lst: return x, lst.index(other) return 'not found'if __name__ == '__main__': lst = [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 18, 19, 20, 21, 29, 34, 54, 65] two_sum = eval(input('Please input the sum:')) print(twonums_sum(two_sum, lst)) 4.3 集合]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-21 合并两个有序列表]]></title>
    <url>%2FLeetCode-21-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%88%97%E8%A1%A8.html</url>
    <content type="text"><![CDATA[题目：合并两个有序列表将两个有序链表合并为一个新的有序链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 示例： 12输入：1-&gt;2-&gt;4, 1-&gt;3-&gt;4输出：1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 思路上次的链表题可参考: LeetCode-2 两数相加 首先需要理解ListNode这个class。 12345# Definition for singly-linked list.class ListNode: def __init__(self, x): self.val = x self.next = None 每个ListNode()都有两个属性，一个是本身的值(val)，另一个则是指向另一个ListNode()对象。比如示例中的输出：7 -&gt; 0 -&gt; 8。其实是三个ListNode()对象，产生的过程如下： 12345678tmp = ListNode(0)res = tmptmp = ListNode(7)tmp = tmp.nexttmp.next = ListNode(0)tmp = tmp.nexttmp.next = ListNode(8)return res.next 对ListNode这样的链表有了理解之后，本题就相对简单了： 初始化一个outNode=ListNode(None)，用于最终输出，再初始化一个curNode=outNode用于记录当前ListNode。 当l1和l2均非空时: 若l1的值比l2的值大，则curNode.next指向l1，l1向后推移一位：l1=l1.next 若l2的值比l1的值大，则curNode.next指向l2，l2向后推移一位：l2=l2.next 当前节点向后推移一位：curNode = curNode.next 将当前节点指向剩余非空节点 返回结果 实现： 1234567891011121314151617181920212223# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -&gt; ListNode: outNode = ListNode(None) curNode = outNode while l1 and l2: if l1.val &lt; l2.val: curNode.next = l1; l1 = l1.next else: curNode.next = l2; l2 = l2.next curNode = curNode.next if l1: curNode.next = l1; else: curNode.next = l2 return outNode.next 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据获取与表示2]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E4%B8%8E%E8%A1%A8%E7%A4%BA2.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 进阶：爬虫小项目(3项) “迷你爬虫编程小练习”进阶:抽取某本书的前 50 条短评内容并计算评分(star)的平 均值。提示:有的评论中并不包含评分。 12345678910111213141516171819202122232425262728293031323334import requestsfrom bs4 import BeautifulSoupimport timeif __name__ == '__main__': num = 1 count = 1 stars = [] # 都挺好 while count &lt; 50: url = 'https://book.douban.com/subject/20492971/comments/hot?p=' + str(num) try: res = requests.get(url) except Exception as e: print(e) break if res.status_code == 200: soup = BeautifulSoup(res.content) for comment in soup.find_all('div', 'comment'): comment_text = comment.find('span', 'short').string # 过滤一些没有评分的短评 try: star = int(''.join(filter(str.isdigit, comment.find('span', 'user-stars').attrs['class'][1]))) except Exception as e: continue stars.append(star) print('&#123;&#125;:&#123;&#125;:&#123;&#125;'.format(count, star, comment_text)) count += 1 if count &gt; 50: break num += 1 time.sleep(5) print('最新50条短评平均得分：&#123;&#125;'.format(sum(stars)//50)) 在“http://money.cnn.com/data/dow30/”上抓取道指成分股数据并将 30 家公司 的代码、公司名称和最近一次成交价放到一个列表中输出。 12345678910import requestsimport reif __name__ == '__main__': url = 'http://money.cnn.com/data/dow30/' res = requests.get(url) if res.status_code == 200: pattern = re.compile('class="wsod_symbol"&gt;(.*?)&lt;\/a&gt;.*?&lt;span.*?&gt;(.*?)&lt;\/span&gt;.*?\n.*?class="wsod_aRight"&gt;&lt;span.*?class="wsod_stream"&gt;(.*?)&lt;\/span&gt;') out_list = re.findall(pattern, res.text) print(out_list) 请爬取网页(http://www.volleyball.world/en/vnl/2018/women/results-and-ranking/round1)上的数据(包括 TEAMS and TOTAL, WON, LOST of MATCHES) 1234567891011import requestsimport reif __name__ == '__main__': url = 'http://www.volleyball.world/en/vnl/2018/women/results-and-ranking/round1' res = requests.get(url) if res.status_code == 200: pattern = re.compile('href="/en/vnl/2018/women/teams.*?&gt;(.*?)&lt;/a&gt;&lt;/figcaption&gt;\s+&lt;/figure&gt;\s+&lt;/td&gt;\s+&lt;td&gt;(.*?)&lt;/td&gt;\s+&lt;td class="table-td-bold"&gt;(.*?)&lt;/td&gt;\s+&lt;td class="table-td-rightborder"&gt;(.*?)&lt;/td&gt;') out_list = re.findall(pattern, res.text) print(out_list) 基础编程练习 1.从键盘输入整数 n(1-9 之间)，对于 1-100之间的整数删除包含 n 并且能被 n 整除的数，例如如果 n 为 6，则要删掉包含 6 的如 6，16 这样的数及是 6 的倍数的如 12 和18 这样的数，输出所有满足条件的数，要求每满 10 个数换行。 测试数据: Enter the number: 6 屏幕输出: 1,2,3,4,5,7,8,9,10,11 13,14,15,17,19,20,21,22,23,25 27,28,29,31,32,33,34,35,37,38 39,40,41,43,44,45,47,49,50,51 52,53,55,57,58,59,70,71,73,74 75,77,79,80,81,82,83,85,87,88 89,91,92,93,94,95,97,98,99,100 方法一： 123456789101112131415161718if __name__ == '__main__': n = eval(input('Please input a number(1-9):')) out_string = '' if 1 &lt;= n &lt;= 9: count = 1 for num in range(101): if num % n == 0 or str(num).find(str(n)) != -1: continue else: out_string = out_string + str(num) + ',' if count % 10 == 0: print(out_string[:-1]) out_string = '' count += 1 if out_string: print(out_string[:-1]) else: print('Wrong Number! Please input a number between 1 and 9!') 方法二： 12345678910111213141516if __name__ == '__main__': n = eval(input('Please input a number(1-9):')) out_string = '' if 1 &lt;= n &lt;= 9: list = list(filter(lambda x: x % n != 0 and str(x).find(str(n)) == -1, range(1, 101))) count = 1 for num in list: out_string = out_string + str(num) + ',' if count % 10 == 0: print(out_string[:-1]) out_string = '' count += 1 if out_string: print(out_string[:-1]) else: print('Wrong Number! Please input a number between 1 and 9!') 2.请用随机函数产生 500 行 1-100 之间的随机整数存入文件 random.txt 中，编程寻找这些整数的众数并输出，众数即为一组数中出现最多的数。 1234567891011121314151617181920212223import randomif __name__ == '__main__': with open('random.txt', 'w+') as f: for i in range(500): f.write(str(random.randint(1, 100))) f.write('\n') f.seek(0) nums = f.readlines() nums_dict = &#123;&#125; for num in nums: num = num.strip() if num in nums_dict: nums_dict[num] += 1 else: nums_dict[num] = 1 sorted_nums = sorted(nums_dict.items(), key= lambda d: d[1], reverse=True) # 可能存在重复次数的数字 max_num = sorted_nums[0][1] for k, v in sorted_nums: if v == max_num: print(k) 文件 article.txt 中存放了一篇英文文章(请自行创建并添加测试文本)，假设文章中的标点符号仅包括“,”、“.”、“!”、“?”和“…”，编程找出其中最长的单词并输出。 12345678910111213141516171819if __name__ == '__main__': with open('article.txt', 'r') as f: data = f.read() raw_words = data.split() words = set() for word in raw_words: if word[-3:] == '...': words.add(word[:-3]) elif word[-1:] in ',.?!': words.add(word[:-1]) else: words.add(word) result = sorted(words, key=len, reverse=True) max_len = len(result[0]) for word in result: if len(word) == max_len: print(word) else: break 数据表示编程题 1. 统计字符串中的字符个数 题目内容： 定义函数countchar()按字母表顺序统计字符串中所有出现的字母的个数（允许输入大写字符，并且计数时不区分大小写）。形如： 输入格式: 字符串 输出格式： 列表 输入样例： Hello, World! 输出样例： [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 3, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0] 1234567891011121314def countchar(string): alphbet = 'abcdefghijklmnopkrstuvwxyz' count_list = [0 for x in range(len(alphbet))] for char in string: try: count_list[alphbet.index(char.lower())] += 1 except ValueError: continue return count_listif __name__ == "__main__": string = input() print(countchar(string)) 2.寻找输入数字中的全数字（pandigital） 题目内容： 如果一个n位数刚好包含了1至n中所有数字各一次则称它们是全数字（pandigital）的，例如四位数1324就是1至4全数字的。从键盘上输入一组整数，输出其中的全数字，若找不到则输出“not found”。 输入格式: 多个数字串，中间用一个逗号隔开 输出格式： 满足条件的数字串，分行输出 输入样例： 1243,322,321,1212,2354 输出样例： 1243 321 1234567891011121314151617181920def pandigital(nums): lst = [] if type(nums) == int: nums = [nums] for num in nums: origin_list = list(str(str(num))) check_list = list(map(lambda x: str(x), range(1, len(str(num)) + 1))) origin_list.sort() if origin_list == check_list: lst.append(num) return lstif __name__ == "__main__": lst = pandigital(eval(input())) if lst: for num in lst: print(num) else: print('not found')]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据获取与表示1]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E4%B8%8E%E8%A1%A8%E7%A4%BA1.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 3.1 本地数据获取 3.2 网络数据获取 爬虫小实验请在豆瓣任意找一本图书，抓取它某一页的短评并进行页面解析将短评文字抽取后输出，再对其中的评分进行抽取计算其总分。 1234567891011121314151617import requestsfrom bs4 import BeautifulSoupimport reres = requests.get('https://book.douban.com/subject/30407571/comments/')soup = BeautifulSoup(res.text, 'lxml')comments = soup.find_all('span', 'short')for comment in comments: print(comment.string)s = 0pattern = re.compile('&lt;span class="user-stars allstar(.*?) rating')p = re.findall(pattern, res.text)for star in p: s += int(star)print(s) 3.3 序列 3.4 字符串 编程练习题 使用以下语句存储一个字符串： string = ‘My moral standing is: 0.98765’将其中的数字字符串转换成浮点数并输出。（提示：可以使用find()方法和字符串切片或split()方法，提取出字符串中冒号后面的部分，然后使用float函数，将提取出来的字符串转换为浮点数） 1234if __name__ == '__main__': string = 'My moral standing is: 0.98765' out = string.split(' ')[-1] print(float(out)) 自定义函数move_substr(s, flag, n)，将传入的字符串s按照flag（1代表循环左移，2代表循环右移）的要求左移或右移n位（例如对于字符串abcde12345，循环左移两位后的结果为cde12345ab，循环右移两位后的结果为45abcde123），结果返回移动后的字符串，若n超过字符串长度则结果返回-1。main模块中从键盘输入字符串、左移和右移标记以及移动的位数，调用move_substr()函数若移动位数合理则将移动后的字符串输出，否则输出“the n is too large”。 12345678910111213141516def move_substr(s, flag, n): if n &lt;= len(s): if flag == 1: s = s[n:] + s[:n] elif flag == 2: s = s[-n:] + s[:-n] return s else: return 'the n is too large' if __name__ == '__main__': s = input('Please input s:') flag = eval(input('Please input flag:')) n = eval(input('Please input n:')) print(move_substr(s, flag, n)) 讨论有一天一个江湖传说中的奥数高手冷冷地问我：你知道2000个5（55555…55555）除以84的余数是多少吗？给你5分钟时间。我打开Python IDLE，5秒钟告诉了他答案，看到屏幕上只有一行代码，他的脸在风中抽搐着。如果你遇到他，你几秒可以KO他？快来亮出你的KO秘籍吧。 1print(eval('5'*2000) % 84) 3.5 列表 讨论假设要生成如下的列表x（1-100之间）：[5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]请写出你会用的列表解析表达式。 1234[x for x in range(5, 101, 5)][x for x in range(1, 101) if x % 5 == 0]list(map(lambda x:x*5, range(1,21)))list(filter(lambda x: x%5==0, range(1,101))) 文件处理实例创建一个文件 Blowing in the wind.txt，其内容是:How many roads must a man walk downBefore they call him a manHow many seas must a white dove sailBefore she sleeps in the sandHow many times must the cannon balls flyBefore they’re forever bannedThe answer my friend is blowing in the windThe answer is blowing in the wind(2) 在文件头部插入歌名“Blowin’ in the wind”(3) 在歌名后插入歌手名“Bob Dylan”(4) 在文件末尾加上字符串“1962 by Warner Bros. Inc.”(5) 在屏幕上打印文件内容 12345678if __name__ == '__main__': with open('Blowing in the wind.txt', 'r') as f: lines = f.readlines() lines.insert(0, "Blowin' in the wind\n") lines.insert(1, "Bob Dylan\n") lines.append('1962 by Warner Bros. Inc.\n') for line in lines: print(line) 编程练习题有一个咖啡列表[‘32Latte’, ‘_Americano30’, ‘/34Cappuccino’, ‘Mocha35’]，列表中每一个元素都是由咖啡名称、价格和一些其他非字母字符组成，编写一个函数clean_list()处理此咖啡列表，处理后列表中只含咖啡名称，并将此列表返回。初始化咖啡列表，调用clean_list()函数获得处理后的咖啡列表，并利用zip()函数给咖啡名称进行编号后输出，输出形式如下： 1 Latte 2 Americano 3 Cappuccino 4 Mocha 1234567891011121314import redef clean_list(list): for i in range(len(list)): list[i] = ''.join(re.findall(r'[a-zA-Z]', list[i])) return listif __name__ == '__main__': list = ['32Latte', '_Americano30', '/34Cappuccino', 'Mocha35'] list = clean_list(list) for i, name in zip(range(1, len(list)+1), list): print(i, name) 3.6 元组]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-20 有效的括号]]></title>
    <url>%2FLeetCode-20-%E6%9C%89%E6%95%88%E7%9A%84%E6%8B%AC%E5%8F%B7.html</url>
    <content type="text"><![CDATA[题目：有效的括号给定一个只包括 &#39;(&#39;，&#39;)&#39;，&#39;{&#39;，&#39;}&#39;，&#39;[&#39;，&#39;]&#39; 的字符串，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。 注意空字符串可被认为是有效字符串。 示例 1: 12输入: &quot;()&quot;输出: true 示例 2: 12输入: &quot;()[]&#123;&#125;&quot;输出: true 示例 3: 12输入: &quot;(]&quot;输出: false 示例 4: 12输入: &quot;([)]&quot;输出: false 示例 5: 12输入: &quot;&#123;[]&#125;&quot;输出: true 思路这题可以通过堆栈的概念来解决，利用堆栈先进先出的特点实现 先初始化一个括号对应关系的字典 依次遍历字符串 如果遇到左括号，则写入堆栈 遇到非左括号的，则从堆栈中弹出最新写入的字符，比较是否成对 成对的话，则匹配出了一个括号对，继续遍历 如果当前堆栈还是空的，说明先出现右括号，直接返回False 如果不成对，说明括号顺序有误，无法成对，返回False 结束遍历时，如果堆栈已空说明全部成对，返回True，否则返回False 实现： 123456789101112131415class Solution: def isValid(self, s: str) -&gt; bool: flag_map = &#123;'(': ')', '[': ']', '&#123;': '&#125;'&#125; stack = [] for char in s: if char in flag_map: stack.append(char) else: if stack: element = stack.pop() if flag_map[element] != char: return False else: return False return not stack 优化方法把上述方法中的map进行反向，通过把右括号压入堆栈，来进行匹配，可以更简洁一点。 实现： 123456789101112class Solution: def isValid(self, s: str) -&gt; bool: flag_map = &#123;")": "(", "&#125;": "&#123;", "]": "["&#125; stack = [] for char in s: if char in flag_map: element = stack.pop() if stack else '#' if flag_map[char] != element: return False else: stack.append(char) return not stack 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-16 最接近的三数之和]]></title>
    <url>%2FLeetCode-16-%E6%9C%80%E6%8E%A5%E8%BF%91%E7%9A%84%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C.html</url>
    <content type="text"><![CDATA[题目：最接近的三数之和给定一个包括 n 个整数的数组 nums 和 一个目标值 target。找出 nums 中的三个整数，使得它们的和与 target 最接近。返回这三个数的和。假定每组输入只存在唯一答案。 123例如，给定数组 nums = [-1，2，1，-4], 和 target = 1.与 target 最接近的三个数的和为 2. (-1 + 2 + 1 = 2). 思路这题和之前的三数之和类似，在遍历是比较保留与target最近接的值即可。 使用sort()方法进行排序，如果长度小于3直接返回sum(nums) 初始化三数之和为out_sum=None 依次取i为第0到len(nums)-2个数作为第一个数num1 为了防止重复，如果i不是0的情况下，当前num1与上一轮的num1相同则跳过 初始化第2个数的index为l，第三个数的index为r=len(nums)-1 在确保l&lt;r的情况下开始移动 如果out_sum为None，即第一次遍历，或者out_sum与target的距离比当前三数之和temp_sum大，则将temp_sum赋值给out_sum 如果当前三数之和等于target：直接返回，必定是最接近target的值 如果当前三数之和大于target：r向左移动 如果当前三数之和小于target：l向右移动 实现： 1234567891011121314151617181920212223class Solution: def threeSumClosest(self, nums: List[int], target: int) -&gt; int: if len(nums) &lt; 3: return sum(nums) nums.sort() out_sum = None for i in range(0, len(nums) - 2): if i &gt; 0 and nums[i] == nums[i - 1]: continue l = i + 1 r = len(nums) - 1 while l &lt; r: temp_sum = nums[i] + nums[l] + nums[r] if out_sum is None or abs(out_sum - target) &gt; abs(temp_sum - target): out_sum = temp_sum continue if temp_sum == target: return temp_sum elif temp_sum &gt; target: r -= 1 else: l += 1 return out_sum 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-15 三数之和]]></title>
    <url>%2FLeetCode-15-%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C.html</url>
    <content type="text"><![CDATA[题目： 三数之和给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。 注意：答案中不可以包含重复的三元组。 1234567例如, 给定数组 nums = [-1, 0, 1, 2, -1, -4]，满足要求的三元组集合为：[ [-1, 0, 1], [-1, -1, 2]] 思路之前曾有两数之和的题目，这次其实可以先把给定数组进行排序之和，依次取0到len(nums)-2的数当做第一个数，然后计算剩下的数组的两数之和是否加上第一个数为0即可。 使用sort()方法进行排序，如果长度小于3直接返回空 依次取i为第0到len(nums)-2个数作为第一个数num1 为了防止重复，如果i不是0的情况下，当前num1与上一轮的num1相同则跳过 依次取j从i+1开始到len(nums)-1个数做为第二个数num2 如果当前j不等于i+1的情况下，当前num2与上一轮的num2相同则跳过 判断num3=-(num1+num2)是否在剩余数组中，存在则加入到输出 实现： 12345678910111213141516171819class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: out = [] nums.sort() if len(nums) &lt; 3: return [] for i in range(0, len(nums) -2): if i &gt; 0 and nums[i] == nums[i - 1]: continue num1 = nums[i] for j in range(i + 1, len(nums) - 1): num2 = nums[j] if j &gt; i + 1 and num2 == nums[j - 1]: continue num3 = -(num1 + num2) if num3 in nums[j + 1:]: temp_out = [num1, num2, num3] out.append(temp_out) return out 优化方法上述方法嵌套了两次层循环，且第二层循环需完全遍历剩余数组。可以使用双指针的方式前后同时移动来提升效率。 使用sort()方法进行排序，如果长度小于3直接返回空 依次取i为第0到len(nums)-2个数作为第一个数num1 为了防止重复，如果i不是0的情况下，当前num1与上一轮的num1相同则跳过 初始化第2个数的index为l，第三个数的index为r=len(nums)-1 在确保l&lt;r的情况下开始移动 如果当前三个数相加为0：记录当前组合后，l向后移动，r向前移动 如果移动后的l,r值与上一值相等则继续移动 如果当前三个数相加大于0：r向左移动 如果当前三个数相加小于0：l向右移动 实现： 12345678910111213141516171819202122232425262728class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: out = [] nums.sort() if len(nums) &lt; 3: return [] for i in range(0, len(nums) -2): if i &gt; 0 and nums[i] == nums[i - 1]: continue num1 = nums[i] l = i + 1 r = len(nums) - 1 while l &lt; r: temp_sum = nums[l] + nums[r] + num1 if temp_sum == 0: temp_out = [num1, nums[l], nums[r]] out.append(temp_out) l += 1 r -= 1 while l &lt; r and nums[l] == nums[l - 1]: l += 1 while l &lt; r and nums[r] == nums[r + 1]: r -= 1 elif temp_sum &gt; 0: r -= 1 else: l += 1 return out 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-14 最长公共前缀]]></title>
    <url>%2FLeetCode-14-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%89%8D%E7%BC%80.html</url>
    <content type="text"><![CDATA[题目：最长公共前缀编写一个函数来查找字符串数组中的最长公共前缀。 如果不存在公共前缀，返回空字符串 &quot;&quot;。 示例 1: 12输入: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]输出: &quot;fl&quot; 示例 2: 123输入: [&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;]输出: &quot;&quot;解释: 输入不存在公共前缀。 说明: 所有输入只包含小写字母 a-z 。 思路这个题免不了得一个个的对比，由于是公共字符串，那么最长公共前缀的可能性就是list中长度最短的那个字符串的长度，然后依次递减即可 遍历一次找到长度最短的字符串str[i]，长度为length 初始化公共前缀为str[i] 开始进入循环直到length&lt;=0 如果前length个字符都相同则返回 否则length减1 判断前length个字符是否相同可单独定义一个函数遍历来判断。 实现： 1234567891011121314151617181920212223class Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: if len(strs) == 0: return '' length = len(strs[0]) for str in strs: length = min(length, len(str)) while length &gt; 0: if isCommonPrefix(strs, length): return strs[0][:length] else: length -= 1 return ''def isCommonPrefix(strs, length): commonPrefix = strs[0][:length] for i in range(1, len(strs)): if strs[i][:length] == commonPrefix: continue else: return False return True 其他方法既然要依次遍历，也可以直接把字符串中的第一个字符串作为初始的最长公共字符串，然后从首字符开始依次判断 将第一个字符串strs[0]作为初始字符 从头开始依次遍历strs[0]每个字符 和剩下的字符串中对应位置的字符进行对比 直到跟某个字符串长度相等或者跟某个字符串对应位置字符不一致退出 否则返回strs[0] 实现： 12345678910class Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: if len(strs) == 0: return '' for i in range(0, len(strs[0])): char = strs[0][i] for j in range(1, len(strs)): if len(strs[j]) == i or strs[j][i] != char: return strs[0][:i] return strs[0] 注意：if len(strs[j]) == i or strs[j][i] != char:这条判断的前后顺序不可以颠倒，否则会有index溢出的报错。 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-11 盛最多水的容器]]></title>
    <url>%2FLeetCode-11-%E7%9B%9B%E6%9C%80%E5%A4%9A%E6%B0%B4%E7%9A%84%E5%AE%B9%E5%99%A8.html</url>
    <content type="text"><![CDATA[题目：盛最多水的容器给定 n 个非负整数 a1，a2，…，an，每个数代表坐标中的一个点 (i, ai) 。在坐标内画 n 条垂直线，垂直线 i 的两个端点分别为 (i, ai) 和 (i, 0)。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 说明：你不能倾斜容器，且 n 的值至少为 2。 图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。 示例: 12输入: [1,8,6,2,5,4,8,3,7]输出: 49 思路这个其实就是在数组height中找到两值，height[i]和height[j]使min(height[i], height[j]) * (j-i)最大。 直接最简单方法，经过两次遍历即可： 第一个点i，从第1个数到倒数第二个数 第二个点j，从第i+1个数到最后一个数 计算每次的面积，保留最大面积即可 实现： 123456789class Solution: def maxArea(self, height: List[int]) -&gt; int: length = len(height) area = 0 for i in range(0, length - 1): for j in range(i + 1, length): temp_area = min(height[i], height[j]) * (j - i) area = max(temp_area, area) return area 优化方法上面的方法会有两层循环，那么怎么做到一层循环呢？ 需要通过比较的方式来遍历。 取第一个数i，和最后一个数j最后初始的两个数。此时围成面积的长(j-i)最大。 后续只可能缩短长(j-i)，但这时必须时高更大才可能比之前的面积大，所以是移动i，j两个位置中数值较小的那个 height[i]&lt;height[j]，把i往后移动：i++ height[i]&gt;height[j]，把j往前移动：j– 直到i=j这时循环结束，返回最大值 实现： 123456789101112class Solution: def maxArea(self, height: List[int]) -&gt; int: area = 0 l = 0 r = len(height) - 1 while r &gt; l: area = max(area, min(height[l], height[r]) * (r - l)) if height[l] &gt; height[r]: r -= 1 else: l += 1 return area 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络基础]]></title>
    <url>%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80.html</url>
    <content type="text"><![CDATA[该系列为吴恩达《神经网络和深度学习》学习笔记，主要以思维导图的记录 神经网络基础]]></content>
      <categories>
        <category>神经网络和深度学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-9 回文数]]></title>
    <url>%2FLeetCode-9-%E5%9B%9E%E6%96%87%E6%95%B0.html</url>
    <content type="text"><![CDATA[题目：回文数判断一个整数是否是回文数。回文数是指正序（从左向右）和倒序（从右向左）读都是一样的整数。 示例 1: 12输入: 121输出: true 示例 2: 123输入: -121输出: false解释: 从左向右读, 为 -121 。 从右向左读, 为 121- 。因此它不是一个回文数。 示例 3: 123输入: 10输出: false解释: 从右向左读, 为 01 。因此它不是一个回文数。 进阶: 你能不将整数转为字符串来解决这个问题吗？ 思路该提如果使用字符串解决，那是相当方便的，可直接通过字符串的切片[::-1]获得反转的字符串进行比较即可，也无需判断正负号的问题： 1234567class Solution: def isPalindrome(self, x: int) -&gt; bool: s = str(x)[::-1] if s == str(x): return True else: return False 既然进阶中要求不用字符串，那么如何获取反转数字呢？其实也很简单，对原数字进行%10取余，对反转数字不停的*10加上余数，即可。大体思路如下： 先判断特殊情况，负数，最后一位为0的非0数肯定不是回文数，直接返回 获取反转数字： 初始化反转数字reversedNum=0 x%10获取个位数字 revservedNum进位再加上x的个位数字 x=x/10 比较反转数字与原数字即可 实现： 12345678910111213class Solution: def isPalindrome(self, x: int) -&gt; bool: if x &lt; 0 or (x % 10 ==0 and x != 0): return False reversedNum = 0 num = x while num: reversedNum = reversedNum * 10 + num % 10 num //= 10 if reversedNum == x: return True else: return False 优化方法在上述计算反转数字中，其实没有必要获取全部反转数字reversedNum，只需要计算到一半，然后比较反转数字与剩余的x即可。 如果x个位数为偶数，那么reversedNum==x 如果为奇数，那么reversedNum//10 == x 比如x = 12321，根据上述逻辑，计算到reversedNum=123时，x=12，满足奇数条件 比如x = 1221，根据上述逻辑，计算到reversedNum=12是，x=12，满足偶数条件 那么怎么判断已经计算到一半了呢： 由于反转过程中是对reversedNum的进位，和x的退位，当计算到一半时，reversedNum必然大于x。 实现如下： 123456789101112class Solution: def isPalindrome(self, x: int) -&gt; bool: if x &lt; 0 or (x % 10 ==0 and x != 0): return False reversedNum = 0 while x &gt; reversedNum: reversedNum = reversedNum * 10 + x % 10 x //= 10 if x == reversedNum or x == reversedNum // 10: return True else: return False 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-8 字符串转换整数]]></title>
    <url>%2FLeetCode-8-%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%95%B4%E6%95%B0.html</url>
    <content type="text"><![CDATA[题目: 字符串转换整数(atoi)请你来实现一个 atoi 函数，使其能将字符串转换成整数。 首先，该函数会根据需要丢弃无用的开头空格字符，直到寻找到第一个非空格的字符为止。 当我们寻找到的第一个非空字符为正或者负号时，则将该符号与之后面尽可能多的连续数字组合起来，作为该整数的正负号；假如第一个非空字符是数字，则直接将其与之后连续的数字字符组合起来，形成整数。 该字符串除了有效的整数部分之后也可能会存在多余的字符，这些字符可以被忽略，它们对于函数不应该造成影响。 注意：假如该字符串中的第一个非空格字符不是一个有效整数字符、字符串为空或字符串仅包含空白字符时，则你的函数不需要进行转换。 在任何情况下，若函数不能进行有效的转换时，请返回 0。 说明： 假设我们的环境只能存储 32 位大小的有符号整数，那么其数值范围为 [$2^{31}-1, 2^{31}$]。如果数值超过这个范围，qing返回 INT_MAX ($2^{31}-1$) 或 INT_MIN ($-2^{31}​$) 。 示例 1: 12输入: &quot;42&quot;输出: 42 示例 2: 1234输入: &quot; -42&quot;输出: -42解释: 第一个非空白字符为 &apos;-&apos;, 它是一个负号。 我们尽可能将负号与后面所有连续出现的数字组合起来，最后得到 -42 。 示例 3: 123输入: &quot;4193 with words&quot;输出: 4193解释: 转换截止于数字 &apos;3&apos; ，因为它的下一个字符不为数字。 示例 4: 1234输入: &quot;words and 987&quot;输出: 0解释: 第一个非空字符是 &apos;w&apos;, 但它不是数字或正、负号。 因此无法执行有效的转换。 示例 5: 1234输入: &quot;-91283472332&quot;输出: -2147483648解释: 数字 &quot;-91283472332&quot; 超过 32 位有符号整数范围。 因此返回 INT_MIN (−2^31) 。 思路这题题目中其实已经把代码思路表述的比较明显了，直接对字符串中的字符逐个处理即可： 通过except…try…方式定义一个函数判断字符是否为数字 初始化两个标志符，flag：是否为正数，start_flag：是否开始记录数字，初始化输出out=0 开始逐个处理字符： 开头是空格直接continue，下一个 字符为+或者-或者数字则记录并初始化两个标志符 将连续的数字通过out = out * 10 + int(char)来计算需转换的数字 遇到非数字字符退出 根据flag标识符判断是否需要加上负号，并返回 实现： 123456789101112131415161718192021222324252627282930313233343536373839class Solution: def myAtoi(self, str: str) -&gt; int: flag = True start_flag = False out = 0 max_num = 2 ** 31 - 1 min_num = -2 ** 31 for char in str: if not start_flag: # 忽略开头的空格 if char == ' ': continue # 处理符号 if char == '-': flag = False start_flag = True continue elif char == '+': start_flag = True continue if isNumber(char): out = out*10 + int(char) start_flag = True else: break out = out if flag else out * -1 if out &gt; max_num: return max_num if out &lt; min_num: return min_num return outdef isNumber(s): try: int(s) return True except Exception as e: return False 优化方法在python中，string的内建函数有strip()功能，可以直接删除字符串收尾的空格，所以可以先对字符串进行strip()操作，然后遍历时无需判断空格。 12345678910111213141516171819202122232425262728293031323334353637class Solution: def myAtoi(self, str: str) -&gt; int: flag = True start_flag = False out = 0 max_num = 2 ** 31 - 1 min_num = -2 ** 31 str = str.strip() for char in str: if not start_flag: # 处理符号 if char == '-': flag = False start_flag = True continue elif char == '+': start_flag = True continue if isNumber(char): out = out*10 + int(char) start_flag = True else: break out = out if flag else out * -1 if out &gt; max_num: return max_num if out &lt; min_num: return min_num return outdef isNumber(s): try: int(s) return True except Exception as e: return False 当然也可以用正则表达式直接提取，或者判断字符，如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-7 整数反转]]></title>
    <url>%2FLeetCode-7-%E6%95%B4%E6%95%B0%E5%8F%8D%E8%BD%AC.html</url>
    <content type="text"><![CDATA[题目：整数反转给出一个 32 位的有符号整数，你需要将这个整数中每位上的数字进行反转。 示例 1: 12输入: 123输出: 321 示例 2: 12输入: -123输出: -321 示例 3: 12输入: 120输出: 21 注意: 假设我们的环境只能存储得下 32 位的有符号整数，则其数值范围为 [$-2^{31}, 2^{31}-1$]。请根据这个假设，如果反转后整数溢出那么就返回 0。 思路对于反转，python中对于列表有内建的反转函数，可以直接调用 初始化flag用于确认输入x是正数还是负数，如果是负数将其变成正数 将x转换为string，再转换为list 调用list.reverse()进行反转 合并list的值，并转换为string，再转换为int 根据flag确认是否增加正负号 判断最终结果是否超出范围，超出则返回0 实现： 123456789101112class Solution: def reverse(self, x: int) -&gt; int: flag = True if x &lt; 0: flag = False x *= -1 out = list(str(x)) out.reverse() out = int(''.join(out)) if flag else int(''.join(out)) * -1 if -2 ** 31 &lt; out &lt; 2 ** 31 - 1: return out return 0 优化方法上述方法调用了list的内建函数，但是涉及两次类型转换int -&gt; string -&gt; list，list -&gt; string -&gt; int。 直接进行计算反转会快一些。 初始化flag用于确认输入x是正数还是负数，如果是负数将其变成正数 初始化结果out=0 进入循环迭代 取个位数：pop = x % 10 将x值删除个位：x \\= 10 计算当前的结果: out = out * 10 + pop 根据flag确认是否增加正负号 判断最终结果是否超出范围，超出则返回0 实现： 123456789101112131415class Solution: def reverse(self, x: int) -&gt; int: flag = True out = 0 if x &lt; 0: flag = False x *= -1 while x: pop = x % 10 x //= 10 out = out * 10 + pop out = out if flag else -1 * out if -2 ** 31 &lt; out &lt; 2 ** 31 - 1: return out return 0 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-6 Z字形变换]]></title>
    <url>%2FLeetCode-6-Z%E5%AD%97%E5%BD%A2%E5%8F%98%E6%8D%A2.html</url>
    <content type="text"><![CDATA[题目：Z字形变换将一个给定字符串根据给定的行数，以从上往下、从左到右进行 Z 字形排列。 比如输入字符串为 &quot;LEETCODEISHIRING&quot; 行数为 3 时，排列如下： 123L C I RE T O E S I I GE D H N 之后，你的输出需要从左往右逐行读取，产生出一个新的字符串，比如：&quot;LCIRETOESIIGEDHN&quot;。 请你实现这个将字符串进行指定行数变换的函数： 1string convert(string s, int numRows); 示例 1: 12输入: s = &quot;LEETCODEISHIRING&quot;, numRows = 3输出: &quot;LCIRETOESIIGEDHN&quot; 示例 2: 12345678输入: s = &quot;LEETCODEISHIRING&quot;, numRows = 4输出: &quot;LDREOEIIECIHNTSG&quot;解释:L D RE O E I IE C I H NT S G 思路这道题乍一看有点困惑，如果把字符串用下标写出来就会清晰一些： 12345line0: 1 5 9line1: 2 4 6 8 line2: 3 7 out: 159246837 按Z字形(或者N字形)的方式重新排列字符串中的字符，最后按行的顺序重新组合起来。其实每行中的字符在字符串中的下标是有规律的，可以总结出数学规律来排列。 用程序实现的话，可以采用迭代法，这样就更直观，也不需要总结数学规律了： 先初始化一个list rows，里面包含numRows个子list用来表示numRows个行 初始化cur_row，用来表示当前的应插入的行数，初始化一个flag用于表示当前Z字形方向是向上还是向下 从第0行开始，依次将字符插入到每一行的子list rows[cur_row]。 当cur_row等于0或者等于numRows-1时，说明到达了第一行或者最后一行，需更改方向 正向时：cur_row += 1 反向时：cur_row += -1 PS:对于numRows=1的情况，需做单独处理，直接返回字符串即可。 实现： 123456789101112131415161718192021class Solution: def convert(self, s: str, numRows: int) -&gt; str: if numRows == 1: return s rows = [] for i in range(numRows): rows.append([]) flag = False cur_row = 0 for char in s: rows[cur_row].append(char) if cur_row == 0 or cur_row == numRows - 1: flag = not flag if flag: cur_row += 1 else: cur_row -= 1 out = [] for row in rows: out += row return ''.join(out) 优化方法这里根据flag来判断cur_row是加1还是减1，可以使用python的三目运算： 1res = 值1 if 条件1 else 值2 上述例子中： 1234if flag: cur_row += 1else: cur_row -= 1 就可改写为： 1cur_row += 1 if flag else -1 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[selenium之等待:WebDriverWait]]></title>
    <url>%2Fselenium%E4%B9%8B%E7%AD%89%E5%BE%85-WebDriverWait.html</url>
    <content type="text"><![CDATA[背景在selenium模拟网页操作时，由于网页加载和JS运行需要消耗一定的时间，可能会导致元素找不到而出现异常： 12345678910111213141516172019-03-18 16:05:21,108 - E:\Python\WorkSpace\firewall\fwConfigGetNew.py[line:80] - ERROR: Message: no such element (Session info: chrome=42.0.2311.90) (Driver info: chromedriver=2.14.313457 (3d645c400edf2e2c500566c9aa096063e707c9cf),platform=Windows NT 6.1 SP1 x86_64)Traceback (most recent call last): File "E:\Python\WorkSpace\firewall\fwConfigGetNew.py", line 62, in getHuaweiConfig driver.find_element_by_id("curConfigFileExport-menu-btn").click() File "C:\Python27\lib\site-packages\selenium-3.0.1-py2.7.egg\selenium\webdriver\remote\webdriver.py", line 269, in find_element_by_id return self.find_element(by=By.ID, value=id_) File "C:\Python27\lib\site-packages\selenium-3.0.1-py2.7.egg\selenium\webdriver\remote\webdriver.py", line 752, in find_element 'value': value&#125;)['value'] File "C:\Python27\lib\site-packages\selenium-3.0.1-py2.7.egg\selenium\webdriver\remote\webdriver.py", line 236, in execute self.error_handler.check_response(response) File "C:\Python27\lib\site-packages\selenium-3.0.1-py2.7.egg\selenium\webdriver\remote\errorhandler.py", line 192, in check_response raise exception_class(message, screen, stacktrace)NoSuchElementException: Message: no such element (Session info: chrome=42.0.2311.90) (Driver info: chromedriver=2.14.313457 (3d645c400edf2e2c500566c9aa096063e707c9cf),platform=Windows NT 6.1 SP1 x86_64) 思路手工登陆下这个网址，看了下，打开之后会先出现一个loading提示： 等loading结束后才会出现对应的元素按钮： 那么就需要想办法延迟，来等待页面加载结束： 方法一： sleep这是最简单直接的方法，在页面打开之后，增加一个sleep()等待加载。因为这个时间很少会出现加载失败，超过时间仍未加载出来那就当做超时处理。 1sleep(5) 方法二：WebDriverWait这是selenium提供的等待方法， 12345678910111213141516171819from selenium.webdriver.support.ui import WebDriverWaitclass WebDriverWait(object): def __init__(self, driver, timeout, poll_frequency=POLL_FREQUENCY, ignored_exceptions=None): """Constructor, takes a WebDriver instance and timeout in seconds. :Args: - driver - Instance of WebDriver (Ie, Firefox, Chrome or Remote) - timeout - Number of seconds before timing out - poll_frequency - sleep interval between calls By default, it is 0.5 second. - ignored_exceptions - iterable structure of exception classes ignored during calls. By default, it contains NoSuchElementException only. Example: from selenium.webdriver.support.ui import WebDriverWait \n element = WebDriverWait(driver, 10).until(lambda x: x.find_element_by_id("someId")) \n is_disappeared = WebDriverWait(driver, 30, 1, (ElementNotVisibleException)).\ \n until_not(lambda x: x.find_element_by_id("someId").is_displayed()) """ 通过传入driver，并设置超时时间t，和循环检查时间p（默认为0.5秒），就可以实现每p秒检查一次是否已经存在对应的元素，一直到超时，存在该元素的话，返回元素。这种方式比较正规，相对方法一会节省一点时间，但其实原理类似。 该方法还需配合另外几个selenium包使用： 12345from selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.common.by import Byels = WebDriverWait(driver, 10, 0.5).until((EC.presence_of_element_located((By.ID, 'topmenu_system')))) 总结在使用selenium中，由于网络、浏览器、机器配置等因素，时不时就会发生找不到元素的问题，还是要注意多做一些异常处理，在可能涉及到等待的地方参考上述方法等待。如果可以直接使用request请求的话，那还是优先request请求获取相关数据。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[chromedriver之“”提示处理]]></title>
    <url>%2Fchromedriver%E4%B9%8B%E2%80%9C%E6%AD%A4%E7%B1%BB%E5%9E%8B%E6%96%87%E4%BB%B6%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%8D%9F%E5%AE%B3%E6%82%A8%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E2%80%9D%E6%8F%90%E7%A4%BA%E5%A4%84%E7%90%86.html</url>
    <content type="text"><![CDATA[背景最近在使用selenium进行自动文件下载时，突然出现了一个报错： 下载进行不下去了。 思路经过各种谷歌、百度，均告诉我在要增加params，关闭浏览器安全选项，配置如下： 1234567chromeOptions = webdriver.ChromeOptions()prefs = &#123;"profile.default_content_settings.popups": 0, "download.default_directory": path, "download.prompt_for_download": False, # "download.directory_upgrade": 'true', "safebrowsing.enabled": True&#125;chromeOptions.add_experimental_option("prefs", prefs) 事实证明，可能以前的版本是可行的，现在的真心不行。 上面配置重点是&quot;safebrowsing.enabled&quot;: True。在MacOS的环境下，哪怕不配也是没有问题的，Windows就不行了。 最后在谷歌上找到一篇相关文章，大意是说这个是无解的，可能是windows系统安全的问题， 对于这个解释我还是比较认可的，所以在mac上就不会提示。 Let’s start frankly: you can’t disable this feature. You can merely tweak the download settings in order to avoid it. https://windowsreport.com/type-of-file-can-harm-computer/ 那么问题来了，既然这样，有什么曲线救国的办法呢？ 当chromedriver弹出这个提示的时候，其实文件已经下载完成，如下图： 我们只需要将文件名修改为正确的名字和后缀即可(比如test.txt)，直接无视警告提醒。思路如下： 找到最新下载的文件：通过对下载目录的文件按照创建时间排序，找到最新的 判断是否该文件是否已下载完成：通过判断时间间隔前后该文件是否有大小的变化 结论根据上面思路，实现的关键代码如下： 1234567891011121314151617181920212223242526272829303132def sort_file(): global path dir_lists = os.listdir(path) dir_lists.sort(key=lambda fn: os.path.getmtime(os.path.join(path, fn))) return dir_lists[-1] def changeName(path, oldname, newname): old_path = os.path.join(path, oldname) new_path = os.path.join(path, newname + '.txt') if os.path.exists(old_path): if os.path.exists(new_path): os.remove(new_path) os.rename(old_path, new_path) print ('rename done!' + newname) else: print ('no file found!') def download(): ... temp_filename = sort_file() if u'未确认' in temp_filename: temp_filesize_old = os.path.getsize(os.path.join(path, temp_filename)) while True: time.sleep(1) temp_filesize_new = os.path.getsize(os.path.join(path, temp_filename)) if temp_filesize_old == temp_filesize_new: changeName(path, temp_filename, ip) return else: temp_filesize_old = temp_filesize_new else: print(u'下载失败') 需要注意的是，在文件重命名的时候，先检查下文件是否已经存在，先删除，在创建。 以上。如果有更好的思路，欢迎分享。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础之Python面面观3]]></title>
    <url>%2FPython%E5%9F%BA%E7%A1%80%E4%B9%8BPython%E9%9D%A2%E9%9D%A2%E8%A7%823.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 控制结构和函数编程练习 1. 身体质量指数(BMI，Body Mass Index)是国际上常用的衡量人体肥胖程度和是否健 康的重要标准，计算公式为:BMI=体重/身高的平方(国际单位 kg/m²)。中国的成年人 BMI 数值定义为: 过轻:低于 18.5 正常:18.5-23.9 过重:24-27.9 肥胖:高于 28 请输入体重和身高，输出相应的 BMI 值和体重肥胖程度判断结果(too thin、normal、 overweight 或 fat)。 [输入样例] 60,1.6 [输出样例] Your BMI is 23.4 normal [提示] 程序中体重和身高的输入可用“weight, height = eval(input())”语句表示。 1234567891011def exercise1(): weight, height = eval(input('Please input your weight and height:')) bmi = weight / (height ** 2) if bmi &lt; 18.5: print('Your BMI is &#123;:.1f&#125; &#123;&#125;'.format(bmi, 'too thin')) elif bmi &lt; 24: print('Your BMI is &#123;:.1f&#125; &#123;&#125;'.format(bmi, 'normal')) elif bmi &lt; 28: print('Your BMI is &#123;:.1f&#125; &#123;&#125;'.format(bmi, 'overweight')) else: print('Your BMI is &#123;:.1f&#125; &#123;&#125;'.format(bmi, 'fat')) 2. 按公式:C= 5/9×(F-32) ，将华氏温度转换成摄氏温度，并产生一张华氏 0~300 度与对应的摄氏温度之间的对照表(每隔 20 度输出一次) 1234def exercise2(): for i in range(0, 301, 20): c = 5 / 9 * (i - 32) print('&#123;:d&#125;华氏度：&#123;:.0f&#125;摄氏度'.format(i, c)) 3. 角谷静夫是日本的一位著名学者，他提出了一个猜想(称为角谷猜想):对于一个正整数 n，若为偶数则除以 2，若为奇数则乘以 3 加 1，得到一个新的数后按照之前的两条规则继续演算，若干次后得到的结果必然为 1。输入任一正整数，输出演算过程。[输入样例]10[输出样例]10/2=55*3+1=1616/2=88/2=44/2=22/2=1 123456789def exercise3(): n = eval(input('Please input a number:')) while n != 1: if n % 2 == 1: print("&#123;&#125;*3+1=&#123;&#125;".format(n, 3 * n + 1)) n = n * 3 + 1 else: print("&#123;&#125;/2=&#123;&#125;".format(n, n // 2)) n //= 2 4. 输入 n，用递推法(例如前项之间的关系推导后项，本题为一重循环)编程求 1+2!+3!+…+n!的和并输出。 [输入样例] 5 [输出样例] 153 12345678910111213141516def exercise4(): n = eval(input('Please input a number:')) # 方法一 # out = 0 # for i in range(1, n + 1): # if i == 1: # out += 1 # else: # out += reduce(lambda x, y: x*y, range(1, i + 1)) # print(out) # 方法二 s = term = 1 for i in range(2, n + 1): term *= i s += term print(s) 5. 编程求解 1-4 这 4 个数字可以组成多少个无重复的三位数，按从小到大的顺序输出这些数字。 123456def exercise5(): for i in range(1, 5): for j in range(1, 5): for k in range(1, 5): if i !=j and j!=k and i !=k: print(i*100 + j*10 + k) 6. 验证命题:如果一个三位整数是 37 的倍数，则这个整数循环左移后得到的另两个 3 位数也是 37 的倍数。(注意验证命题的结果输出方式，只要输出命题为真还是假即可，而非每一个三位数都有一个真假的输出) 12345678910def exercise6(): for n in range(100, 1001): if n % 37 == 0: n1 = n // 100 + n % 100 * 10 n2 = n // 10 + n % 10 * 100 if n1 % 37 != 0 or n2 % 37 != 0: print("It's a false proposition.") break else: print("It's a true proposition.") 7. 一个数如果等于它的因子之和则称这个数为完数，例如 6，6=1+2+3，编程计算 1000之内的所有完数并输出。 12345678910def exercise7(): for n in range(1, 1001): s = 0 factors = [] for i in range(1, n): if n % i == 0: s += i factors.append(str(i)) if s == n: print('&#123;&#125;=&#123;&#125;'.format(n, '+'.join(factors))) 8. 验证哥德巴赫猜想之一:2000 以内的正偶数(大于等于 4)都能够分解为两个质数之和。每个偶数表达成形如:4=2+2 的形式。 123456789101112131415def exercise8(): for n in range(4, 2001, 2): for i in range(2, n): if isPrime(i) and isPrime(n-i) and i &lt; n-i: print('&#123;&#125;=&#123;&#125;+&#123;&#125;'.format(n,i,n-i))def isPrime(n): if n == 1: return False for i in range(2, n): if n % i == 0: return False else: return True 测试：控制结构和函数编程题 题目内容： 对于两个不同的整数A和B，如果整数A的全部因子（包括1，不包括A本身）之和等于B；且整数B的全部因子（包括1，不包括B本身）之和等于A，则将A和B称为亲密数。自定义函数fac(x)计算x包括1但不包括本身的所有因子和并返回。从键盘输入整数n，调用fac()函数寻找n以内的亲密数并输出。注意每个亲密数对只输出一次，小的在前大的在后，例如220-284。 输入格式: 按提示用input()函数输入 输出格式： 按样例形式，可使用形如“print(“{}-{}”.format(参数1, 参数2))”输出语句进行亲密数对的输出 输入样例： 500 输出样例： 220-284 12345678910111213def fac(n): out = 0 for i in range(1, n): if n % i == 0: out += i return outn = int(input())for a in range(1, n): b = fac(a) if fac(b)== a and a &lt; b: print("&#123;&#125;-&#123;&#125;".format(a, b)) 题目内容： 找第n个默尼森数。P是素数且M也是素数，并且满足等式M=2^P-1，则称M为默尼森数。例如，P=5，M=2^P-1=31，5和31都是素数，因此31是默尼森数。 输入格式: 按提示用input()函数输入 输出格式： int类型 输入样例： 4 输出样例： 127 1234567891011121314151617181920212223242526def prime(num): if num == 1: return False for i in range(2, num): if num % i == 0: return False else: return Truedef monisen(no): i = 0 j = 1 while i &lt; no: while True: if prime(j): m = 2 ** j - 1 if prime(m): j += 1 break j += 1 i += 1 return mprint(monisen(int(input())))]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础之Python面面观2]]></title>
    <url>%2FPython%E5%9F%BA%E7%A1%80%E4%B9%8BPython%E9%9D%A2%E9%9D%A2%E8%A7%822.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 2.6 递归 2.7 变量作用域 拓展1 Python常用标准库函数 拓展2 异常 异常编程小练习编写一个程序，让用户输入苹果个数和单价，然后计算出价格总额。 Enter count: 10 Enter price for each one: 3.5 Pay: 35 运用try-except语句让程序可以处理非数字输入的情况，如果是非数字输入，打印消息并允许用户再次输入，直到输入正确类型值计算出结果后退出。以下是程序的执行结果： Enter count: 20 Enter price for each one: four Error, please enter numeric one. Enter count: twenty Error, please enter numeric one. Enter count: 20 Enter price for each one: 4 The price is 80. 123456789if __name__ == '__main__': while True: try: count = eval(input('Please input a count:')) price = eval(input('Please input the price for each one:')) pay = count * price print('The price is &#123;&#125;'.format(pay)) except: print('Error, please enter numeric one.')]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础之Python面面观1]]></title>
    <url>%2FPython%E5%9F%BA%E7%A1%80%E4%B9%8BPython%E9%9D%A2%E9%9D%A2%E8%A7%821.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 2.1 条件 编程练习题 编写一个输入分数，输出分数等级的程序，具体为： Score Grade 90~100 A 70~89 B 60~69 C 0~59 D others Invalid score 请添加必要的输入输出语句，尽量让程序友好。 123456789101112if __name__ == '__main__': score = int(input('please input your score:')) if score &gt;= 90: print('Your grade is A!') elif score &gt;= 70: print('Your grade is B!') elif score &gt;= 60: print('Your grade is C!') elif score &gt;=0: print('Your grade is D!') else: print('Invalid score!') 编写程序，从键盘输入一个二元一次方程ax^2+bx+c=0的三个参数a、b、c（均为整数），求此方程的实根。如果方程有实根，则输出实根（保留一位小数），如果没有实根则输出没有实根的信息。 [输入样例1] 1,0,-1 [输出样例1] x1 = 1.0, x2 = -1.0 [输入样例2] 1,2,1 [输出样例2] x = -1.0 [输入样例3] 2,2,3 [输出样例3] no real solution 123456789101112if __name__ == '__main__': a, b, c = eval(input("please input a,b,c values of a*x^2+b*x+c:")) t = b ** 2 - 4 * a * c if t &gt; 0: x1 = (-b + sqrt(t)) / (2 * a) x2 = (-b - sqrt(t)) / (2 * a) print('x1 = &#123;&#125;, x2 = &#123;&#125;'.format(x1, x2)) elif t == 0: x = -b / (2 * a) print('x = &#123;&#125;'.format(x)) else: print('no real solution') 2.2 Range函数 2.3 循环 编程练习 输入一个整数，求其逆序数。注：虽然可通过字符串切片等方法轻松获得一个数的逆序数，但用整数通过循环来获得逆序数是锻炼逻辑思维的一个好例子。 1234567891011121314151617181920212223def reversed1(number): length = len(number) out = '' for i in range(len(number)): out += number[length - 1 - i] return outdef reversed2(number): number = int(number) out = 0 while number != 0: out = out * 10 + number % 10 number = number // 10 return outif __name__ == '__main__': number = input('input a number:') out1 = reversed1(number) out2 = reversed2(number) print(out1) print(out2) 将一个正整数分解质因数。例如：输入90,打印出90=233*5。 1234567891011121314151617181920212223242526272829def sp1(number): start = str(number) out = [] i = 2 while number != 1: while number % i == 0: number //= i out.append(str(i)) i += 1 print(start + '=' + '*'.join(out))def sp2(number): print(str(number) + '=', end='') i = 2 while number != 1: while number % i ==0: number //= i if number == 1: print('&#123;:d&#125;'.format(i), end='') else: print('&#123;:d&#125;*'.format(i), end='') i += 1if __name__ == '__main__': number = eval(input('input a number:')) sp1(number) sp2(number) 2.4 循环中的break、continue和else 2.5 自定义函数]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础之走进Python]]></title>
    <url>%2FPython%E5%9F%BA%E7%A1%80%E4%B9%8B%E8%B5%B0%E8%BF%9BPython.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 1.1 Python简介 1.2 第一个Python程序 1.3 Python语法基础 1.4 Python数据类型 1.5 Python基本运算 1.6 Python的函数、模块和包 测试题目：简单的输入输出： 12345surname = input('input your surname:')lastname = input('input your lastname:)print('your surname is:', surname)pirnt('your lastname is:', lastname)print('your full name is:', lastname, surname) 讨论关于round函数： 在python2.x中：0.5会近似到距离0远的一端，比如 : 12round(0.5) = 1round(-0.5) = -1 Values are rounded to the closest multiple of 10 to the power minus ndigits; if two multiples are equally close, rounding is done away from 0. 在python3.x中：0.5会近似到偶数那边，比如： 12round(2.5) = 2round(3.5) = 4 values are rounded to the closest multiple of 10 to the power minus ndigits; if two multiples are equally close, rounding is done toward the even choice.]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas分组统计不重复值的数量]]></title>
    <url>%2Fpandas%E5%88%86%E7%BB%84%E7%BB%9F%E8%AE%A1%E4%B8%8D%E9%87%8D%E5%A4%8D%E5%80%BC%E7%9A%84%E6%95%B0%E9%87%8F.html</url>
    <content type="text"><![CDATA[基础数据数据格式如下，其中行只有一个值，所以每列均存在重复项（比如，一条策略含多个源地址，多个目标地址和多个端口情况）。 策略ID 策略描述 源地址 目标地址 端口 开通时间 结束时间 id description src dst port st et 12345678import pandas as pddf = pd.read_csv('/Users/zhengk/PycharmProjects/Mine/firewall/output/policy.csv', delimiter=';')df.columns = ['id', 'description', 'src', 'dst', 'port', 'st', 'et']df.head(2)Out[3]: id description src dst port st et0 5 icmp Any Any PING forever forever1 3 Tntrust2AllUntrust new_AG58 All_Untrust ANY forever forever 统计开通策略数最多的前5个源地址即对应的策略数根据要求，只要按照源地址src分组，统计每个不同的src有多少个不重复的策略ID数，在进行排序即可。 先筛选src和id两列进行计算 12345678df[['src', 'id']].head(5)Out[1]: src id0 Any 51 new_AG58 32 122.249.125.98/32 19733 122.249.125.160/32 19744 122.249.125.18/32 2100 先使用groupby进行分组，使用nunique()来统计不重复的个数 12345678df[['src', 'id']].groupby('src')['id'].nunique().head(5)Out[2]: src122.249.125.1/32 2122.249.125.100/32 1122.249.125.101/32 38122.249.125.102/32 227122.249.125.103/32 255 使用sort_values()对输出进行排序 123456789df[['src', 'id']].groupby('src')['id'].nunique().sort_values(ascending=False).head(5)Out[3]: src122.249.125.71/32 1272122.249.125.209/32 529122.249.125.73/32 424122.249.125.113/32 391122.249.125.210/32 361Name: id, dtype: int64 拓展 DataFrame.nunique(*axis=0*, *dropna=True*) 该函数根据axis轴参数统计非重复的个数，默认是按列来统计，如果指定axis=1则按行来统计。 123456789101112131415161718df1 = pd.DataFrame(&#123;'A': [1, 2, 3], 'B': [1, 1, 1]&#125;)df1Out[13]: A B0 1 11 2 12 3 1df1.nunique()Out[14]: A 3B 1dtype: int64df1.nunique(1)Out[15]: 0 11 22 2dtype: int64 http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nunique.html?highlight=nunique#pandas-dataframe-nunique Series.unique() 该函数返回非重复的具体值。是对series的操作，而不能直接操作dataframe。 1234df1['A'].unique()Out[22]: array([1, 2, 3])df1['B'].unique()Out[23]: array([1]) 比如上例中，获取每个源地址可所有不重复的策略ID： 123456789df[['src', 'id']].groupby('src')['id'].unique().head(5)Out[24]: src122.249.125.1/32 [42690, 42691]122.249.125.100/32 [31563]122.249.125.101/32 [12458, 12488, 12536, 14394, 19550, 22348, 227...122.249.125.102/32 [12124, 12134, 12148, 12159, 12222, 12688, 126...122.249.125.103/32 [26711, 26713, 26714, 26719, 26721, 26736, 267...Name: id, dtype: object http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html?highlight=unique#pandas-series-unique DataFrame.sort_values(*by*, *axis=0*, *ascending=True*, *inplace=False*, *kind=&#39;quicksort&#39;*, *na_position=&#39;last&#39;*) 该函数用于对指定axis轴中的值进行排序，默认为升序排列。需要注意的是其中的na_position参数，当源数据中存在空值时，确定空值的处理是很重要的，默认是空值放在最后。 123456df1.sort_values(by=['A'], ascending=False)Out[26]: A B2 3 11 2 10 1 1 http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html#pandas-dataframe-sort-values]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用selenium实现批量文件下载]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8selenium%E5%AE%9E%E7%8E%B0%E6%89%B9%E9%87%8F%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD.html</url>
    <content type="text"><![CDATA[背景实现需求：批量下载联想某型号的全部驱动程序。 一般在做网络爬虫的时候，都是保存网页信息为主，或者下载单个文件。当涉及到多文件批量下载的时候，由于下载所需时间不定，下载的文件名不定，所以有一定的困难。 思路 参数配置 在涉及下载的时候，需要先对chromedriver进行参数配置，设定默认下载目录： 12345678global base_pathprofile = &#123; 'download.default_directory': base_path&#125;chrome_options = webdriver.ChromeOptions()chrome_options.add_experimental_option('prefs', profile)driver = webdriver.Chrome(executable_path='../common/chromedriver', options=chrome_options)driver.implicitly_wait(10) 页面分析 联想官网上每个型号的驱动下载页面如上图所示，虽然前面有一个登陆的遮罩，但是实际上并不影响点击。需要注意的是： 驱动列表，需要点击才可以显示具体的下载项目表格 每个下载列表的表头建议做跳过处理 下载处理 在页面中，找到“普通下载”的元素，点击即可下载。最终实现结果是我们希望根据网页的列表进行重命名和重新归档到文件夹，但是我们会发现如下几个问题： 下载过来的文件名无法控制。 依次下载的话，我们无法确认需要下载多久。并行下载的话，无法有效的区分重命名。 在网上找了很久，也没找到在下载时直接重命名的方法，所以最终选择依次下载，当每次下载完成后进行重命名和归档，思路如下： 对每个驱动目录，先新建一个文件夹，如：主板 点击下载后开始下载文件 通过os模块，找到下载目录中所有文件，并按创建时间排序，找到最新创建的文件 由于未完成的文件后缀为.crdownload（chrome），那么根据后缀来判断是否已完成下载，未完成的话继续等待 待下载完成，将文件重命名并剪切到开始建立的归档目录 在后期测试的时候，发现还有几个坑需要注意： 在查找最新创建的文件时，需要注意.DS_Store文件的处理。（Mac系统，Windows则需要考虑thumbs.db） 需要判断一下最新创建的文件是否为文件夹，可以通过filter函数来处理 最新文件的排序查找实现如下： 12345678910111213141516def sort_file(): """排序文件""" dir_link = base_path dir_lists = list(filter(check_file, os.listdir(dir_link))) if len(dir_lists) == 0: return '' else: dir_lists.sort(key=lambda fn: os.path.getmtime(dir_link + os.sep + fn)) return os.path.join(base_path, dir_lists[-1])def check_file(filename): if filename == '.DS_Store' or filename == 'thumbs.db': return False global base_path return os.path.isfile(os.path.join(base_path, filename)) 总结最终实现效果如下： 完整代码参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/Lenovo 如果大家有更好的方法，也欢迎分享。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib画图时标注最大值]]></title>
    <url>%2Fmatplotlib%E7%94%BB%E5%9B%BE%E6%97%B6%E6%A0%87%E6%B3%A8%E6%9C%80%E5%A4%A7%E5%80%BC.html</url>
    <content type="text"><![CDATA[背景在上一篇使用matplotlib绘制时间序列图表中，本来想只展示最大值，一直没找到方法，就先标注了所有的点的数值，看起来有点不够直接。今天终于搞定了，记录一下。 思路源数据：index 为 ‘data’，数据为’title’ 123456789cacu.head(5)Out[5]: titledate 2015-01-01 22015-02-01 02015-03-01 02015-04-01 02015-05-01 2 matpoltlib标注数值 在绘图的时候，可以使用ax.text()方法在坐标系中标注文字，使用方法如下： 123# 显示全部数值for a,b in zip(cacu.index, cacu.values): ax.text(a, b, b[0]) 寻找最大值： 在pandas中有现成的方法可以找到最大值和最大值的索引： 最大值的索引： 1234cacu.idxmax()Out[6]: title 2018-07-01dtype: datetime64[ns] 最大值： 1234cacu.max()Out[8]: title 50dtype: int64 也可根据索引取值： 12345cacu.loc[cacu.idxmax()]Out[7]: titledate 2018-07-01 50 画图标注 根据最上面的标注，应该直接输入最大值的坐标（x为索引，y为值）就可以了，结果有了如下的报错： 1234567891011121314151617ax.text(cacu.idxmax(), cacu.max(), cacu.max())/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison result = method(y)Traceback (most recent call last): File "/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py", line 3291, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File "&lt;ipython-input-9-cebfed8d464d&gt;", line 1, in &lt;module&gt; ax.text(cacu.idxmax(), cacu.max(), cacu.max()) File "/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/matplotlib/axes/_axes.py", line 722, in text x=x, y=y, text=s) File "/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/matplotlib/text.py", line 163, in __init__ self.set_text(text) File "/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/matplotlib/text.py", line 1191, in set_text if s != self._text: File "/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/pandas/core/generic.py", line 1479, in __nonzero__ .format(self.__class__.__name__))ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). 我们仔细看上面最大值的返回结果类型，而并非一个坐标。 12type(cacu.idxmax())Out[11]: pandas.core.series.Series 而我们真正需要的应该是title列的最大索引： 1234type(cacu['title'].idxmax())Out[12]: pandas._libs.tslibs.timestamps.Timestampcacu['title'].idxmax()Out[13]: Timestamp('2018-07-01 00:00:00', freq='MS') 对应的最大值也一样： 1234cacu['title'].max()Out[15]: 50type(cacu['title'].max())Out[16]: numpy.int64 再次画图： 123x = cacu['title'].idxmax()y = cacu['title'].max()ax.text(x, y, y, verticalalignment='bottom', horizontalalignment='center', fontsize='large') 结果 结论这里出现问题其实还是对matplotlib和pandas的基本概念没有弄明白，什么时候获取的是值，什么时候获取的是序列，还需要多加练习。 另外，给图添加标注还有可以使用plt.annotate()方法： 123x = cacu['title'].idxmax()y = cacu['title'].max()plt.annotate(y, xy=(x,y)) 完整代码参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/weixin]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用matplotlib绘制时间序列图表]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8matplotlib%E7%BB%98%E5%88%B6%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%9B%BE%E8%A1%A8.html</url>
    <content type="text"><![CDATA[背景前面通过selenium爬取了微信公众号“新世相”的所有文章链接，详见使用Selenium获取微信公众号的所有文章。获取到的信息有：文章发表的时间、标题以及对应的url。那么根据时间可以绘制出文章发表情况的图表，先上结果图： 思路 读取csv 1df_ori = pd.read_csv('articles.csv', sep=';', header=None) 筛选数据 我们获取到的源数据如下： 123&quot;2019-03-02;看到44岁潘粤明的脸，我发现面对痛苦的最好方式是温柔&quot;;http://mp.weixin.qq.com/s?__biz=MzI2OTA3MTA5Mg==&amp;mid=2651791151&amp;idx=1&amp;sn=d24b36c7af7f3a13e8ab9b8d4d11143f&amp;chksm=f11e58b4c669d1a2fe70afe9fd2f2fdfe3cc1a17d0c48d8efa51ae8e10d17239c7ff0f8649b4&amp;scene=21#wechat_redirect&quot;2019-03-01;“同时被3个男人追求，我选了最丑的”：看完这些70岁老人的遗憾，我啥都想开了&quot;;http://mp.weixin.qq.com/s?__biz=MzI2OTA3MTA5Mg==&amp;mid=2651791096&amp;idx=1&amp;sn=eb68116d46fddf0e827475d9b26941b0&amp;chksm=f11e5963c669d075ec8716073bec685f819df8c2dbc03b87f23347ba5b01ea4219864877b4a0&amp;scene=21#wechat_redirect&quot;2019-02-28;在同学会上遇见初恋：他盯了我 5 秒，说出我等了 10 年的那句话&quot;;http://mp.weixin.qq.com/s?__biz=MzI2OTA3MTA5Mg==&amp;mid=2651790909&amp;idx=1&amp;sn=f642cd31fbc8d41d0b4fbe949b51425a&amp;chksm=f11e59a6c669d0b0e5e7ee12cece0a4afaa995aa123b8e993f8ec10d8ba38cccdcf5290de25a&amp;scene=21#wechat_redirect | 第一列 | 第二列 || ——— | —— || 日期;标题 | url | 那么我们只要获取第一列，并根据分隔符;进行拆分就可以得到日期与标题的对应关系： 123# 取第一列并分割日期与标题df = df_ori.iloc[:, 0]df = df.str.split(';', expand=True) 格式化数据 现在给数据设置列别名并进行格式化操作 1234# 格式化日期，设置column，并将日期设置为indexdf.columns = ['date', 'title']df.date = pd.to_datetime(df.date)df = df.set_index('date') 数据统计 我们需要计算每个月新世相公众号发表的文章数，那么需要按月重新取样，并汇总统计文章数 12# 按月统计文章数"MS"为月初cacu = df.resample("MS").count() 画图 配置画布大小 由于时间跨度从2015年至今比较长，我们需要先配置一下画布大小。 12# 画图fig, ax = plt.subplots(figsize=[20, 5]) 配置线条展示：值处用圆点加粗 12# 线条ax.plot(cacu, 'o-') 配置中文展示 12345# 通过设置中文字体方式解决中文展示问题font = FontProperties(fname='../font/PingFang.ttc')ax.set_title("新世相文章统计", fontproperties=font)ax.set_xlabel("日期", fontproperties=font)ax.set_ylabel("文章数", fontproperties=font) 配置横轴展示 由于按月画图，我们横轴显示为年-月，我们这里对横轴设置两层坐标，第一层仅展示月份，第二层展示年份，这样可以防止因为横轴字符挤在一起。第二层坐标格式中增加\n\n用于换行，给第一层月份留出空间。 1234567# 设置时间轴formater = mdate.DateFormatter('%Y-%m')ax.xaxis.set_major_formatter(formater)ax.xaxis.set_minor_locator(mdate.MonthLocator())ax.xaxis.set_minor_formatter(mdate.DateFormatter('%m'))ax.xaxis.set_major_locator(mdate.YearLocator())ax.xaxis.set_major_formatter(mdate.DateFormatter('\n\n%Y')) 显示数值 123# 显示数值for a,b in zip(cacu.index, cacu.values): ax.text(a, b, b[0]) 显示图片 1plt.show() 结论最后画图如下： 可见，新世相也是从16年开始逐步兴起，基本上保持每天一篇的频率一致稳定发展至今。 完整代码参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/weixin]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用selenium把网页保存为PDF]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8selenium%E6%8A%8A%E7%BD%91%E9%A1%B5%E4%BF%9D%E5%AD%98%E4%B8%BAPDF.html</url>
    <content type="text"><![CDATA[背景前面通过selenium爬取了微信公众号“新世相”的所有文章链接，详见使用Selenium获取微信公众号的所有文章。那么接下来就该获取具体文章了。由于网页是含有图片的，想想还是通过浏览器把网页打印成PDF保存好了，同时保存一份不含图片的文本文件，可以用于后续分析。 那么怎么使用selenium打印PDF呢？ 思路在网上找了找解决方案，主要有如下几种： 利用第三方包：pdfkit，可参考：https://www.cnblogs.com/silence-cc/p/9463227.html 使用chrome的—print-to-pdf模式，将请求到html导出为pdf，可参考：http://osask.cn/front/ask/view/1029784 使用js命令&#39;window.print();来调用浏览器打印，可参考：https://gitee.com/shinemic/codes/09y87ph6vf2c5zamwls3q48 这里我们选用第三种，相对来说适应性比较好，也方便查看进展，如果想隐藏页面，只需要加入—headlss选项即可。 实现如下： 配置chromedriver的options 1234567891011121314151617appState = &#123; "recentDestinations": [ &#123; "id": "Save as PDF", "origin": "local" &#125; ], "selectedDestinationId": "Save as PDF", "version": 2 &#125;profile = &#123; 'printing.print_preview_sticky_settings.appState': json.dumps(appState), 'savefile.default_directory': './articles'&#125;chrome_options = webdriver.ChromeOptions()chrome_options.add_experimental_option('prefs', profile)chrome_options.add_argument('--kiosk-printing') 这里savefile.default_directory用来指定保存文章的路径，需自行配置。 保存pdf 12345driver.get(url)time.sleep(5)# 保存PDFtemp_title = driver.titledriver.execute_script('window.print();') 这里chrome打印网页时默认文件名为网页的title，所以这里先保存一下temp_title=driver.title。 改名 1os.rename('./articles/' + temp_title + '.pdf', './articles/' + title + '.pdf') 由于如果打开同一个网站的多个页面并保存pdf，那么很可能就会出现由于网站title相同而覆盖的情况，所以每次保存完毕后，改一下pdf的文件名。 注意：当网页异常等情况可能出现title为空的情况，那么这里改名的时候就会报异常错误，需要进行异常处理。 实现根据上述思路，在打开网页、导出pdf、改名之后加上sleep，防止异常。实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344def get_articles(): appState = &#123; "recentDestinations": [ &#123; "id": "Save as PDF", "origin": "local" &#125; ], "selectedDestinationId": "Save as PDF", "version": 2 &#125; profile = &#123; 'printing.print_preview_sticky_settings.appState': json.dumps(appState), 'savefile.default_directory': './articles' &#125; chrome_options = webdriver.ChromeOptions() chrome_options.add_experimental_option('prefs', profile) chrome_options.add_argument('--kiosk-printing') driver = webdriver.Chrome(executable_path='./chromedriver', options=chrome_options) driver.implicitly_wait(60) count = 1 with open('articles.csv', newline='') as csvfile: spamreader = csv.reader(csvfile, delimiter=';') for line in spamreader: try: title = line[0].split(';')[1] url = line[1] print("下载第" + str(count) + "篇，标题：" + title) driver.get(url) time.sleep(5) # 保存PDF temp_title = driver.title driver.execute_script('window.print();') time.sleep(10) os.rename('./articles/' + temp_title + '.pdf', './articles/' + title + '.pdf') # 保存txt content = driver.find_element_by_id('js_article').text with open('./text/' + title + '.txt', 'w') as f: f.write(content) count += 1 except Exception as e: logging.exception(e) driver.quit() return 完整代码参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/weixin 如果大家有更好的方法，也欢迎分享。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Selenium获取微信公众号的所有文章]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8Selenium%E8%8E%B7%E5%8F%96%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E7%9A%84%E6%89%80%E6%9C%89%E6%96%87%E7%AB%A0.html</url>
    <content type="text"><![CDATA[背景前段时间有人在群里分享了爬虫咪蒙公众号的所有文章，可以通过深度学习进行各种分析，但由于咪蒙账号已封，所以链接点进去也看不到了。个人还是比较喜欢看新世相的公众号的，看看怎么把它的文章也都爬下来。 思路 从哪里爬？ 爬虫一般得用浏览器访问，然后找到相关的请求接口，通过修改参数来伪造请求获取数据。微信公众号文章浏览器上哪里看呢？搜了下，有三种方式： 搜狗的微信搜索：http://weixin.sogou.com 该方式只能搜到公众号最新10篇文章，放弃 使用fiddler抓包，并用手机模拟器模拟手机访问 难度比较大，放弃 在微信公众平台：https://mp.weixin.qq.com/ 通过插入文章的方式可以查找公众号的所有文章，相对简单，pick 爬取过程 由于微信公众平台登陆需要手机扫描二维码，不可避免的要“人机耦合”了，所以打算全程采取使用selenium来做： 新建ChromeDriver并登陆网页，sleep10秒钟用于手工扫码登陆 12345678910111213url = "https://mp.weixin.qq.com/"option = webdriver.ChromeOptions()# option.add_argument('headless')driver = webdriver.Chrome(executable_path='./chromedriver', options=option)driver.implicitly_wait(60)driver.get(url)driver.find_element_by_name('account').clear()driver.find_element_by_name('password').clear()driver.find_element_by_name('account').send_keys(config.account)driver.find_element_by_name('password').send_keys(config.password)driver.find_element_by_class_name('btn_login').click()# 手动扫码time.sleep(10) 登陆后打开素材管理，并新建图文素材： 1234# 打开素材管理driver.find_element_by_link_text('素材管理').click()# 点击新建图文素材driver.find_element_by_class_name('weui-desktop-btn_primary').click() 需要注意的是，这里会新建一个窗口，在selenium中页面切换操作，需要获取windows_handler然后切换： 1234# 新建图文素材 会有新建页面all_handles = driver.window_handles# 切换到新窗口driver.switch_to.window(all_handles[1]) 在新建素材页面，点击插入链接的按钮，并选择查找文章，输入“新世相”搜索找到对应的微信号 12# 点击插入链接driver.find_element_by_id('edui24_body').click() 12345678910# 选择查找文章driver.find_element_by_xpath('//*[@id="myform"]/div[3]/div[1]/div/label[2]/span').click()# 输入公众号名称driver.find_element_by_class_name('js_acc_search_input').send_keys('新世相')# 搜索driver.find_element_by_class_name('js_acc_search_btn').click()# 选择新世相# driver.find_element_by_class_name('search_biz_item').text# Out[53]: '订阅号\n新世相 微信号：thefair2'driver.find_element_by_class_name('search_biz_item').click() 新建一个csv文件用于保存文章标题与固定链接 123# 新建Csvf = open('articles.csv', 'w')writer = csv.writer(f, delimiter=';') 找到搜索结果部分，获取文章标题与链接，并通过不停的点击“下一页”，来爬取全部文章 12345678910111213141516171819202122# 选择结果输出部分search_article_result = driver.find_element_by_class_name('search_article_result')count = 1while True: # 获取列表 temp_list = search_article_result.find_elements_by_class_name('my_link_item') logging.info("抓取第" + str(count) + "页") for item in temp_list: title = item.text.replace('\n', ';') logging.info(title) # 文章链接 href = item.find_element_by_tag_name('a').get_attribute('href') writer.writerow([title, href]) try: search_article_result.find_element_by_class_name('page_next').click() count += 1 time.sleep(random.randint(20,40)) except Exception as e: logging.exception("获取列表失败") driver.quit() return 这里需要注意，爬取过程要多通过sleep来暂停下，不然很快就会提示“操作过于频繁”而被暂停了 爬取结果 如果中途被反爬了，就在上次爬到的页数那儿开始继续吧，多耦合几次还是可以爬完的 总结由于公众号管理这块登陆涉及到了扫描二维码，安全性高了很多，大大增加了爬虫的难度。相对来说，腾讯的反爬还是做的非常好的，直接使用selenium来模拟人操作，中间sleep也相当长的时间了，还是会被提示“操作太频繁”而被中断，且中断时间长达好几个小时，那么如果采用请求的方式，那估计会被封的更快吧。。。 完整代码参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/weixin 如果大家有更好的方法，也欢迎分享。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-5 最长回文子串]]></title>
    <url>%2FLeetCode-5-%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2.html</url>
    <content type="text"><![CDATA[题目：最长回文子串给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。 示例 1： 123输入: &quot;babad&quot;输出: &quot;bab&quot;注意: &quot;aba&quot; 也是一个有效答案。 示例 2： 12输入: &quot;cbbd&quot;输出: &quot;bb&quot; 思路 如何判断字符串是否为回文： 所谓回文，就是正反都一样，在python中反转字符串非常简单，如果源字符串等于反转后的字符串，那么就可以判断为回文： 12345def isPalindorme(str: str): if str == str[::-1]: return True else: return False 找出所有字符串 需要遍历两次，设i为初始index，j为index+1到结尾，就可以获取所有的子串 对所有子串来判断是否为回文，并记录长度，即可找到最大的回文子串 对于s长度为0，或者1的，需特殊处理一下 实现如下： 12345678910111213141516class Solution: def longestPalindrome(self, s: str) -&gt; str: res_len = 0 res = '' if len(s) == 0: return '' if len(s) == 1: return s for i in range(len(s)): for j in range(i+1, len(s) + 1): tempstr = s[i:j] if isPalindorme(tempstr): if len(tempstr) &gt; res_len: res_len = len(tempstr) res = tempstr return res 优化方法该方法参考了官方解答中java版，思路如下： 如何判断回文： 随便找字符串中的一个字符 如果是回文的话有两种情况： 当回文字符串长度为奇数：那么该字符串左边的字符和右边的相等（从第0个开始，即自己等于自己开始，然后依次往左右扩展，得到该字符为中间的最长回文子串。）如aba。 当回文字符串长度为偶数：那么左边第i个字符和右边第i+1个字符相等（从第0个开始，即自己等于下一个，然后依次往左右扩展，得到该字符为中间的最长回文子串。）如abba。 从字符串s开头起遍历整个字符串，并对每个字符分别做两种回文判断，得出最大回文字符串。 实现如下： 12345678910111213141516171819202122class Solution: def longestPalindrome(self, s: str) -&gt; str: if len(s) == 0: return '' if len(s) == 1: return s start = 0 end = 0 for i in range(len(s)): len1 = self.expandAroundCenter(s, i, i) len2 = self.expandAroundCenter(s, i, i + 1) max_len = max(len1, len2) if max_len &gt; (end - start): start = i - (max_len - 1) // 2 end = i + max_len // 2 return s[start:end + 1] def expandAroundCenter(self, s, left, right): while left &gt;= 0 and right &lt; len(s) and s[left] == s[right]: left -= 1 right += 1 return right - left - 1 如果大家有更好的方法或者思路，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-4 寻找两个有序数组的中位数]]></title>
    <url>%2FLeetCode-4-%E5%AF%BB%E6%89%BE%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0.html</url>
    <content type="text"><![CDATA[题目：寻找两个有序数组的中位数给定两个大小为 m 和 n 的有序数组 nums1 和 nums2。 请你找出这两个有序数组的中位数，并且要求算法的时间复杂度为 O(log(m + n))。 你可以假设 nums1 和 nums2 不会同时为空。 示例 1: 1234nums1 = [1, 3]nums2 = [2]则中位数是 2.0 示例 2: 1234nums1 = [1, 2]nums2 = [3, 4]则中位数是 (2 + 3)/2 = 2.5 思路这题目乍一看有点绕，其实简单一点考虑就是找这两个数组集合的中位数。 先把两个数组合并成一个数组temp = nums1 + nums2 然后再对这个新数组进行排序 找中位数： 当这个数组长度是奇数时：中位数就是排在最中间的那个数 当这个数组长度是偶数时：中位数就是排在最中间的两个数的平均数 实现如下： 12345678class Solution: def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: temp = nums1 + nums2 temp.sort() if len(temp) % 2 ==1: return temp[int(len(temp) / 2)] else: return (temp[int(len(temp) / 2) - 1] + temp[int(len(temp) / 2)]) / 2.0 官方解答这题的官方解答看起来比较复杂，但考虑到上面的解法用到了sort()方法，可能本身就存在算法复杂度高的问题。所以官方解答使用了递归排序的方法。 先不考虑特殊情况，分别将两个数组划分成两部分 123 left_part | right_partA[0], A[1], ..., A[i-1] | A[i], A[i+1], ..., A[m-1]B[0], B[1], ..., B[j-1] | B[j], B[j+1], ..., B[n-1] 那么如果left_part的长度等于right_part的长度，且right_part的值都大于left_part的值（即min(right_part)&gt;=max(left_part)）。我们就找到了中位数： $median=\frac{max(left_part)+min(right_part)}{2}​$ 那么i和j有什么关系呢？ 当总长度$m+n​$为偶数时： 两边长度相等：$i+j=m-i+n-j$ ，所以可以得到$j=\frac{m+n}{2}-i$。 当总长度$m+n​$为奇数时： 左边比右边多1个数：$i+j=m-i+n-j+1$，所以得到：$j=\frac{m+n+1}{2}-i$。 由于$i, j$会不断移动，我们取较大值：$j=\frac{m+n+1}{2}-i$。 先不考虑临界情况，并假设$m&lt;n$，我们需要在$[0,m]$中找到$i$可以使$B[j-1]&lt;=A[i]$且$A[i-1]&lt;=B[j]$。 现在我们可以开始按二叉树进行查找： 初始化i的查找范围$imin = 0​$,$ imax = m​$,$i=\frac{imin+imax}{2}​$。并动态调整$imin,imax​$的值,为了方便可以不需要考虑奇数偶数，通过int()取整即可，如果是奇数那么i与原值一样，会继续调整$imin,imax​$的值，直到下一个 当$i&lt;m, B[j-1]&gt;A[i]$时，我们需要增加$imin = imin + 1$ 当$i&gt;0, A[i-1]&gt;B[j]$时，我们需要减小$imax = imax -1$。 最终结果，要考虑下特殊情况： 左边最大值max_of_left： 当$i=0$时：$B[j-1]$ 当$j=0$时：$A[i-1]$ max(a[i-1], B[j-1]) 当总长度m+n为奇数时，直接返回左边最大值 右边最小值min_of_right: 当$i=m$时：$B[j]$ 当$j=0$时：$A[i]$ Min(A[i], B[j]) 当总长度为偶数时，返回（max_of_left + min_of_right）/ 2 实现如下： 1234567891011121314151617181920212223242526272829303132333435363738class Solution: def median(self, nums1, nums2): # Solution 2 m, n = len(nums1), len(nums2) if m &gt; n: nums1, nums2, m, n = nums2, nums1, n, m if n == 0: raise ValueError imin, imax, half_len = 0, m, int((m + n + 1) / 2) while imin &lt;= imax: i = int((imin + imax) / 2) j = half_len - i if i &lt; m and nums2[j - 1] &gt; nums1[i]: # i is too small, must increase it imin = i + 1 elif i &gt; 0 and nums1[i - 1] &gt; nums2[j]: # i is too big, must decrease it imax = i - 1 else: # i is perfect if i == 0: max_of_left = nums2[j - 1] elif j == 0: max_of_left = nums1[i - 1] else: max_of_left = max(nums1[i - 1], nums2[j - 1]) if (m + n) % 2 == 1: return max_of_left if i == m: min_of_right = nums2[j] elif j == n: min_of_right = nums1[i] else: min_of_right = min(nums1[i], nums2[j]) return (max_of_left + min_of_right) / 2.0 还是有点绕，如果大家有更好的方法或者思路，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-3 无重复字符串的最长子串]]></title>
    <url>%2FLeetCode-3-%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2.html</url>
    <content type="text"><![CDATA[题目：无重复字符串的最长子串给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。 示例 1: 123输入: &quot;abcabcbb&quot;输出: 3 解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。 示例 2: 123输入: &quot;bbbbb&quot;输出: 1解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。 示例 3: 1234输入: &quot;pwwkew&quot;输出: 3解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，&quot;pwke&quot; 是一个子序列，不是子串。 思路由于是在字符串中找无重复的子串，那么就可以用到set()类型。还是得遍历字符串，基本思路如下： 初始化最长无重复子串长度record=0 从头开始遍历字符串中的每一个字符 初始化一个临时的li1=set()来存放子串 判断当前字符是否存在于li1 如果不存在就存入li1，并判断当前子串长度是否大于record，是则更新record 如果存在，那么说明当前子串中有了重复字符，开始下一轮的查找 实现代码如下： 12345678910111213class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: record = 0 for i in range(len(s)): li1 = set() for st in s[i:]: if st not in li1: li1.add(st) if len(li1) &gt; record: record = len(li1) else: break return record 优化方法在上述方法中，每当扎到一个重复字符时，就从下一个i开始重新遍历。我们可以在每个j的遍历过程中，记录下每个子字符串的index和值，并设置一个偏移量carry。如果j和j‘是重复字符，那么偏移量carry就等于i到j的长度，下一次遍历从i=i+carry+1即可。 123456789101112131415161718class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: record = 0 i = 0 while i &lt; len(s): li1 = &#123;&#125; for j, char in enumerate(s[i:]): carry = 0 if char not in li1: li1[char] = j carry += j if len(li1) &gt; record: record = len(li1) else: carry = li1[char] break i = i + carry + 1 return record 优化方法二以上方法均需要做两层的循环，那么有没有只做一层循环的呢： 还是要用dict，我们需要寻找到两个下标i,j，使他们之间的长度最长，思路如下： 初始化i = 0，最长无重复子串长度record=0，一个存放无重复字符串和对应index的dict：li1 通过enumerate(s)的方式在遍历中获取每个字符char和其对应的index：j 如果字符char不存在于li1，那么把char和j存入li1，更新record=max(record, j-i+1) 如果字符char存在于li1，那么说明最长无重复子串需要从li1[char]后开始计算，即i=max(i,li1[char] + 1) 代码如下： 1234567891011class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: record = 0 i = 0 li1 = &#123;&#125; for j, char in enumerate(s): if char in li1: i = max(i, li1[char] + 1) record = max(record, j - i + 1) li1[char] = j return record 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-2 两数相加]]></title>
    <url>%2FLeetCode-2-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0.html</url>
    <content type="text"><![CDATA[题目： 两数相加给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。 如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。 您可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例： 123输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)输出：7 -&gt; 0 -&gt; 8原因：342 + 465 = 807 思路这题涉及数据结构和链表，这在Python中比较少，参考了官方解题思路才大概明白。 首先需要理解ListNode这个class。 12345# Definition for singly-linked list.class ListNode: def __init__(self, x): self.val = x self.next = None 每个ListNode()都有两个属性，一个是本身的值(val)，另一个则是指向另一个ListNode()对象。比如示例中的输出：7 -&gt; 0 -&gt; 8。其实是三个ListNode()对象，产生的过程如下： 12345678tmp = ListNode(0)res = tmptmp = ListNode(7)tmp = tmp.nexttmp.next = ListNode(0)tmp = tmp.nexttmp.next = ListNode(8)return res.next 对ListNode这样的链表有了理解之后，解题就比较方便了： 先初始化一个tmp = ListNode(0) 的临时变量，并将res.next指向tmp，并初始化一个进位临时变量carry=0 遍历l1,l2直到都为None: 如果l1为None，则x值为0，否则x值为l1.val 如果l2为None，则y值为0，否则y值为l2.val x,y相加并加上上一轮相加的进位carry得到tmpSum，该轮tmp的值为和的tmpSum的个位(tmpSum%10)，carry更新为tmpSum的十位(tmpSum//10) 将tmp在指向自己的下一个tmp.next 遍历结束后检查是否还有进位carry，有的话则tmp的下一位为ListNode(1) 返回结果：res.next 完整代码如下： 12345678910111213141516171819202122232425262728293031# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1, l2): tmp = ListNode(0) res = tmp carry=0 while l1 or l2: if l1: x = l1.val else: x = 0 if l2: y = l2.val else: y = 0 tmpSum = carry + x + y carry = tmpSum // 10 tmp.next = ListNode(tmpSum % 10) tmp = tmp.next if l1: l1 = l1.next if l2: l2 = l2.next if carry &gt; 0: tmp.next=ListNode(1) return res.next 优化方法上面方法中需要新建一个临时变量来存放l1,l2相加的结果，由于是相加，我们考虑开始就将结果指向l1，在l1的基础上进行计算，思路如下： 设置结果res=l1,初始化一个进位临时变量carry=0 第一阶段l1,l2都有值的时候 l1.val,l2.val相加并加上上一轮相加的进位carry得到tmpSum，该轮l1.val的值为更新为tmpSum的个位(tmpSum%10)，carry更新为tmpSum的十位(tmpSum//10) 设定一个临时变量prev指向l1 l1,l2均指向自己的下一个l1.next,l2.next 第一阶段结束时，说明l1,l2有一个已经结束了。这时候把l1设定为还未结束的序列（l1 = l1 or l2），并将临时变量prev.next指向新的l1(为了保证序列的连续性，如果前面没有设置的prev临时变量，那么返回结果就中断了) 第二阶段遍历新的l1： l1.val加上上一轮的进位carry得到tmpSum，该轮l1.val的值为更新为tmpSum的个位(tmpSum%10)，carry更新为tmpSum的十位(tmpSum//10) 设定一个临时变量prev指向l1 l1指向自己的下一个l1.next 第二阶段结束后，检查是否还有进位carry，有的话则tmp的下一位为ListNode(1) 返回结果：res 完整代码如下： 12345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1, l2): res = l1 carry = 0 while l1 and l2: tmpSum = l1.val + l2.val + carry carry = tmpSum // 10 l1.val = tmpSum % 10 prev = l1 l1 = l1.next l2 = l2.next l1 = l1 or l2 prev.next = l1 while l1: tmpSum = l1.val + carry carry = tmpSum // 10 l1.val = tmpSum % 10 prev = l1 l1 = l1.next if carry &gt; 0: prev.next = ListNode(1) return res 这道题对Python好像不是很友好，经常会出现超时的情况，如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-1 两数之和]]></title>
    <url>%2FLeetCode-1-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C.html</url>
    <content type="text"><![CDATA[题目：两数之和给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 示例: 1234给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 思路由于题目说明了只有一个对应答案，那么简单粗暴的想法就是进行两次遍历： 先从头取第一个数 将该数依次与后面的每个数相加判断是否等于target 实现： 1234567891011class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ for i in range(len(nums)): for j in range(i+1, len(nums)): if nums[i] + nums[j] == target: return [i, j] 优化方法上面的方法做了两次遍历，那么有没有办法做一次呢 参考了其他网友的答案，果真还是有办法的，先上代码： 12345678910111213class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ nums_dict = &#123;&#125; for index, num in enumerate(nums): another_num = target - num if another_num in nums_dict: return [nums_dict[another_num], index] nums_dict[num] = index 这里主要用到了dict()，思路是： 先新建一个空dict()用于存放列表中的数和对应的索引 先将使用enumerate()函数将列表转组合为一个索引序列，同时列出数据和下标： 12345list(enumerate(nums))[(0, 4), (1, 3), (2, 2), (3, 1)]nums = [2, 7, 11, 15]list(enumerate(nums))[(0, 2), (1, 7), (2, 11), (3, 15)] 然后用目标减去当前的值就获取我们需要找的另一个值 判断另一个值是否存在于dict中，在的话，直接返回两个数值的索引即可，否则将这个数也加入到dict中。 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10分钟了解Pandas基础知识]]></title>
    <url>%2F10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3Pandas%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html</url>
    <content type="text"><![CDATA[背景在数据分析中pandas举足轻重，学习pandas最好的方法就是看官方文档，以下是根据官方文档10 Minutes to pandas学习记录。（官方标题10分钟，感觉起码得半个小时吧） 在pandas中主要有两种数据类型，可以简单的理解为： Series：一维数组 DateFrame：二维数组（矩阵） 有了大概的概念之后，开始正式认识pandas: 首先要引入对应的包： 12import numpy as npimport pandas as pd 新建对象 Object Creation Series 可以通过传入一个list对象来新建Series，其中空值为np.nan: 12345678910s = pd.Series([1,3,4,np.nan,7,9])sOut[5]: 0 1.01 3.02 4.03 NaN4 7.05 9.0dtype: float64 pandas会默认创建一列索引index（上面的0-5）。我们也可以在创建时就指定索引： 123456789pd.Series([1,3,4,np.nan,7,9], index=[1,1,2,2,'a',4])Out[9]: 1 1.01 3.02 4.02 NaNa 7.04 9.0dtype: float64 要注意的是，索引是可以重复的，也可以是字符。 DataFrame 新建一个DataFrame对象可以有多种方式： 通过传入一个numpy的数组、指定一个时间的索引以及一个列名。 12345678910111213141516dates = pd.date_range('20190101', periods=6)datesOut[11]: DatetimeIndex(['2019-01-01', '2019-01-02', '2019-01-03', '2019-01-04', '2019-01-05', '2019-01-06'], dtype='datetime64[ns]', freq='D')df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))dfOut[18]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.1743452019-01-06 0.221597 -0.753038 -1.741256 0.287280 通过传入一个dict对象 12345678910111213df2 = pd.DataFrame(&#123;'A':1., 'B':pd.Timestamp('20190101'), 'C':pd.Series(1, index=list(range(4)), dtype='float32'), 'D':np.array([3]*4, dtype='int32'), 'E':pd.Categorical(["test", "tain", "test", "train"]), 'F':'foo'&#125;)df2Out[27]: A B C D E F0 1.0 2019-01-01 1.0 3 test foo1 1.0 2019-01-01 1.0 3 tain foo2 1.0 2019-01-01 1.0 3 test foo3 1.0 2019-01-01 1.0 3 train foo 这里我们指定了不同的类型，可以通过如下查看： 123456789df2.dtypesOut[28]: A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 可以看出DataFrame和Series一样，在没有指定索引时，会自动生成一个数字的索引，这在后续的操作中十分重要。 查看 Viewing Data 查看开头几行或者末尾几行： 1234567891011121314df.head()Out[30]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.174345df.tail(3)Out[31]: A B C D2019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.1743452019-01-06 0.221597 -0.753038 -1.741256 0.287280 可以通过添加行数参数来输出，默认为输出5行。 查看索引和列名 1234567df.indexOut[32]: DatetimeIndex(['2019-01-01', '2019-01-02', '2019-01-03', '2019-01-04', '2019-01-05', '2019-01-06'], dtype='datetime64[ns]', freq='D')df.columnsOut[33]: Index(['A', 'B', 'C', 'D'], dtype='object') 使用DataFrame.to_numpy()转化为numpy数据。需要注意的是由于numpy array类型数据只可包含一种格式，而DataFrame类型数据可包含多种格式，所以在转换过程中，pandas会找到一种可以处理DateFrame中国所有格式的numpy array格式，比如object。这个过程会耗费一定的计算量。 123456789101112131415df.to_numpy()Out[35]: array([[ 0.67162219, 0.78572584, 0.39243527, 0.87469243], [-2.42070338, -1.11620768, -0.34607048, 0.78594081], [ 1.36442543, -0.94764138, 2.38688005, 0.58537186], [-0.48597971, -1.28145415, 0.35406263, -1.41885798], [-1.12271697, -2.78904135, -0.79181242, -0.17434484], [ 0.22159737, -0.75303807, -1.74125564, 0.28728004]])df2.to_numpy()Out[36]: array([[1.0, Timestamp('2019-01-01 00:00:00'), 1.0, 3, 'test', 'foo'], [1.0, Timestamp('2019-01-01 00:00:00'), 1.0, 3, 'tain', 'foo'], [1.0, Timestamp('2019-01-01 00:00:00'), 1.0, 3, 'test', 'foo'], [1.0, Timestamp('2019-01-01 00:00:00'), 1.0, 3, 'train', 'foo']], dtype=object) 上面df全部为float类型，所以转换会很快，而df2涉及多种类型转换，最后全部变成了object类型元素。 查看数据的简要统计结果 1234567891011df.describe()Out[37]: A B C Dcount 6.000000 6.000000 6.000000 6.000000mean -0.295293 -1.016943 0.042373 0.156680std 1.356107 1.144047 1.396030 0.860725min -2.420703 -2.789041 -1.741256 -1.41885825% -0.963533 -1.240143 -0.680377 -0.05893950% -0.132191 -1.031925 0.003996 0.43632675% 0.559116 -0.801689 0.382842 0.735799max 1.364425 0.785726 2.386880 0.874692 转置 1234567df.TOut[38]: 2019-01-01 2019-01-02 2019-01-03 2019-01-04 2019-01-05 2019-01-06A 0.671622 -2.420703 1.364425 -0.485980 -1.122717 0.221597B 0.785726 -1.116208 -0.947641 -1.281454 -2.789041 -0.753038C 0.392435 -0.346070 2.386880 0.354063 -0.791812 -1.741256D 0.874692 0.785941 0.585372 -1.418858 -0.174345 0.287280 按坐标轴排序，其中axis参数为坐标轴，axis默认为0，即横轴（对行排序），axis=1则为纵轴（对列排序）；asceding参数默认为True，即升序排序，ascending=False则为降序排序： 123456789101112131415161718df.sort_index(axis=1)Out[44]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.1743452019-01-06 0.221597 -0.753038 -1.741256 0.287280df.sort_index(axis=1, ascending=False)Out[45]: D C B A2019-01-01 0.874692 0.392435 0.785726 0.6716222019-01-02 0.785941 -0.346070 -1.116208 -2.4207032019-01-03 0.585372 2.386880 -0.947641 1.3644252019-01-04 -1.418858 0.354063 -1.281454 -0.4859802019-01-05 -0.174345 -0.791812 -2.789041 -1.1227172019-01-06 0.287280 -1.741256 -0.753038 0.221597 可见df.sort_index(axis=1)是按列名升序排序，所以看起来没有变化，当设置ascending=False时，列顺序变成了DCBA。 按数值排序： 123456789101112131415161718df.sort_values(by='B')Out[46]: A B C D2019-01-05 -1.122717 -2.789041 -0.791812 -0.1743452019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-06 0.221597 -0.753038 -1.741256 0.2872802019-01-01 0.671622 0.785726 0.392435 0.874692df.sort_values(by='B', ascending=False)Out[47]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-06 0.221597 -0.753038 -1.741256 0.2872802019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 筛选 Selection 获取某列 1234567891011df['A']Out[49]: 2019-01-01 0.6716222019-01-02 -2.4207032019-01-03 1.3644252019-01-04 -0.4859802019-01-05 -1.1227172019-01-06 0.221597Freq: D, Name: A, dtype: float64type(df.A)Out[52]: pandas.core.series.Series 也可直接用df.A，注意这里是大小写敏感的，这时候获取的是一个Series类型数据。 选择多行 123456789101112df[0:3]Out[53]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.585372df['20190102':'20190104']Out[54]: A B C D2019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-04 -0.485980 -1.281454 0.354063 -1.418858 通过一个[]会通过索引对行进行切片，由于前面设置了索引为日期格式，所以可以方便的直接使用日期范围进行筛选。 通过标签选择 选择某行 1234567df.loc[dates[0]]Out[57]: A 0.671622B 0.785726C 0.392435D 0.874692Name: 2019-01-01 00:00:00, dtype: float64 选择指定行列的数据 1234567891011121314151617df.loc[:, ('A', 'C')]Out[58]: A C2019-01-01 0.671622 0.3924352019-01-02 -2.420703 -0.3460702019-01-03 1.364425 2.3868802019-01-04 -0.485980 0.3540632019-01-05 -1.122717 -0.7918122019-01-06 0.221597 -1.741256df.loc['20190102':'20190105', ('A', 'C')]Out[62]: A C2019-01-02 -2.420703 -0.3460702019-01-03 1.364425 2.3868802019-01-04 -0.485980 0.3540632019-01-05 -1.122717 -0.791812 传入第一个参数是行索引标签范围，第二个是列索引标签，:代表全部。 选定某值 1234df.loc['20190102', 'A']Out[69]: -2.420703380445092df.at[dates[1], 'A']Out[70]: -2.420703380445092 可以通过loc[]和at[]两种方式来获取某值，但需要注意的是，由于行索引为datetime类型，使用loc[]方式获取时，可直接使用20190102字符串来代替，而在at[]中，必须传入datetime类型，否则会有报错： 1234567df.at['20190102', 'A'] File "pandas/_libs/index.pyx", line 81, in pandas._libs.index.IndexEngine.get_value File "pandas/_libs/index.pyx", line 89, in pandas._libs.index.IndexEngine.get_value File "pandas/_libs/index.pyx", line 449, in pandas._libs.index.DatetimeEngine.get_loc File "pandas/_libs/index.pyx", line 455, in pandas._libs.index.DatetimeEngine._date_check_typeKeyError: '20190102' 通过位置选择 选择某行 1234567df.iloc[3]Out[71]: A -0.485980B -1.281454C 0.354063D -1.418858Name: 2019-01-04 00:00:00, dtype: float64 iloc[]方法的参数，必须是数值。 选择指定行列的数据 123456789101112131415161718192021df.iloc[3:5, 0:2]Out[72]: A B2019-01-04 -0.485980 -1.2814542019-01-05 -1.122717 -2.789041df.iloc[:,:]Out[73]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.1743452019-01-06 0.221597 -0.753038 -1.741256 0.287280df.iloc[[1, 2, 4], [0, 2]]Out[74]: A C2019-01-02 -2.420703 -0.3460702019-01-03 1.364425 2.3868802019-01-05 -1.122717 -0.791812 同loc[]，:代表全部。 选择某值 1234df.iloc[1, 1]Out[75]: -1.1162076820700824df.iat[1, 1]Out[76]: -1.1162076820700824 可以通过iloc[]和iat[]两种方法获取数值。 按条件判断选择 按某列的数值判断选择 123456df[df.A &gt; 0]Out[77]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-06 0.221597 -0.753038 -1.741256 0.287280 筛选出符合要求的数据 123456789df[df &gt; 0]Out[78]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 NaN NaN NaN 0.7859412019-01-03 1.364425 NaN 2.386880 0.5853722019-01-04 NaN NaN 0.354063 NaN2019-01-05 NaN NaN NaN NaN2019-01-06 0.221597 NaN NaN 0.287280 不符合要求的数据均会被赋值为空NaN。 使用isin()方法筛选 12345678910111213141516171819202122232425df2 = df.copy()df2['E'] = ['one', 'one', 'two', 'three', 'four', 'three']df2Out[88]: A B C D E2019-01-01 0.671622 0.785726 0.392435 0.874692 one2019-01-02 -2.420703 -1.116208 -0.346070 0.785941 one2019-01-03 1.364425 -0.947641 2.386880 0.585372 two2019-01-04 -0.485980 -1.281454 0.354063 -1.418858 three2019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 four2019-01-06 0.221597 -0.753038 -1.741256 0.287280 threedf2['E'].isin(['two', 'four'])Out[89]: 2019-01-01 False2019-01-02 False2019-01-03 True2019-01-04 False2019-01-05 True2019-01-06 FalseFreq: D, Name: E, dtype: booldf2[df2['E'].isin(['two', 'four'])]Out[90]: A B C D E2019-01-03 1.364425 -0.947641 2.386880 0.585372 two2019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 four 注意：isin必须严格一致才行，df中的默认数值小数点位数很长，并非显示的5位，为了方便展示，所以新增了E列。直接用原数值，情况如下，可看出[1,1]位置符合要求。 123456789df.isin([-1.1162076820700824])Out[95]: A B C D2019-01-01 False False False False2019-01-02 False True False False2019-01-03 False False False False2019-01-04 False False False False2019-01-05 False False False False2019-01-06 False False False False 设定值 通过指定索引设定列 1234567891011121314151617181920s1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range('20190102', periods=6))s1Out[98]: 2019-01-02 12019-01-03 22019-01-04 32019-01-05 42019-01-06 52019-01-07 6Freq: D, dtype: int64df['F']=s1dfOut[101]: A B C D F2019-01-01 0.671622 0.785726 0.392435 0.874692 NaN2019-01-02 -2.420703 -1.116208 -0.346070 0.785941 1.02019-01-03 1.364425 -0.947641 2.386880 0.585372 2.02019-01-04 -0.485980 -1.281454 0.354063 -1.418858 3.02019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 4.02019-01-06 0.221597 -0.753038 -1.741256 0.287280 5.0 空值会自动填充为NaN。 通过标签设定值 12345678910df.at[dates[0], &apos;A&apos;] = 0dfOut[103]: A B C D F2019-01-01 0.000000 0.785726 0.392435 0.874692 NaN2019-01-02 -2.420703 -1.116208 -0.346070 0.785941 1.02019-01-03 1.364425 -0.947641 2.386880 0.585372 2.02019-01-04 -0.485980 -1.281454 0.354063 -1.418858 3.02019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 4.02019-01-06 0.221597 -0.753038 -1.741256 0.287280 5.0 通过为止设定值 12345678910df.iat[0, 1] = 0dfOut[105]: A B C D F2019-01-01 0.000000 0.000000 0.392435 0.874692 NaN2019-01-02 -2.420703 -1.116208 -0.346070 0.785941 1.02019-01-03 1.364425 -0.947641 2.386880 0.585372 2.02019-01-04 -0.485980 -1.281454 0.354063 -1.418858 3.02019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 4.02019-01-06 0.221597 -0.753038 -1.741256 0.287280 5.0 通过NumPy array设定值 12345678910df.loc[:, 'D'] = np.array([5] * len(df))dfOut[109]: A B C D F2019-01-01 0.000000 0.000000 0.392435 5 NaN2019-01-02 -2.420703 -1.116208 -0.346070 5 1.02019-01-03 1.364425 -0.947641 2.386880 5 2.02019-01-04 -0.485980 -1.281454 0.354063 5 3.02019-01-05 -1.122717 -2.789041 -0.791812 5 4.02019-01-06 0.221597 -0.753038 -1.741256 5 5.0 通过条件判断设定值 1234567891011df2 = df.copy()df2[df2 &gt; 0] = -df2df2Out[112]: A B C D F2019-01-01 0.000000 0.000000 -0.392435 -5 NaN2019-01-02 -2.420703 -1.116208 -0.346070 -5 -1.02019-01-03 -1.364425 -0.947641 -2.386880 -5 -2.02019-01-04 -0.485980 -1.281454 -0.354063 -5 -3.02019-01-05 -1.122717 -2.789041 -0.791812 -5 -4.02019-01-06 -0.221597 -0.753038 -1.741256 -5 -5.0 空值处理 Missing Datapandas默认使用np.nan来表示空值，在统计计算中会直接忽略。 通过reindex()方法可以新增、修改、删除某坐标轴（行或列）的索引，并返回一个数据的拷贝： 123456789df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])df1.loc[dates[0]:dates[1], 'E'] = 1df1Out[115]: A B C D F E2019-01-01 0.000000 0.000000 0.392435 5 NaN 1.02019-01-02 -2.420703 -1.116208 -0.346070 5 1.0 1.02019-01-03 1.364425 -0.947641 2.386880 5 2.0 NaN2019-01-04 -0.485980 -1.281454 0.354063 5 3.0 NaN 删除空值 1234df1.dropna(how='any')Out[116]: A B C D F E2019-01-02 -2.420703 -1.116208 -0.34607 5 1.0 1.0 填充空值 1234567df1.fillna(value=5)Out[117]: A B C D F E2019-01-01 0.000000 0.000000 0.392435 5 5.0 1.02019-01-02 -2.420703 -1.116208 -0.346070 5 1.0 1.02019-01-03 1.364425 -0.947641 2.386880 5 2.0 5.02019-01-04 -0.485980 -1.281454 0.354063 5 3.0 5.0 判断是否为空值 1234567pd.isna(df1)Out[118]: A B C D F E2019-01-01 False False False False True False2019-01-02 False False False False False False2019-01-03 False False False False False True2019-01-04 False False False False False True 运算 Operations 统计 注意 所有的统计默认是不包含空值的 平均值 默认情况是按列求平均值： 12345678df.mean()Out[119]: A -0.407230B -1.147897C 0.042373D 5.000000F 3.000000dtype: float64 如果需要按行求平均值，需指定轴参数： 123456789df.mean(1)Out[120]: 2019-01-01 1.3481092019-01-02 0.4234042019-01-03 1.9607332019-01-04 1.3173262019-01-05 0.8592862019-01-06 1.545461Freq: D, dtype: float64 数值移动 1234567891011121314151617181920s = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates)sOut[122]: 2019-01-01 1.02019-01-02 3.02019-01-03 5.02019-01-04 NaN2019-01-05 6.02019-01-06 8.0Freq: D, dtype: float64s = s.shift(2)sOut[125]: 2019-01-01 NaN2019-01-02 NaN2019-01-03 1.02019-01-04 3.02019-01-05 5.02019-01-06 NaNFreq: D, dtype: float64 这里将s的值移动两个，那么空出的部分会自动使用NaN填充。 不同维度间的运算，pandas会自动扩展维度： 123456789df.sub(s, axis='index')Out[128]: A B C D F2019-01-01 NaN NaN NaN NaN NaN2019-01-02 NaN NaN NaN NaN NaN2019-01-03 0.364425 -1.947641 1.386880 4.0 1.02019-01-04 -3.485980 -4.281454 -2.645937 2.0 0.02019-01-05 -6.122717 -7.789041 -5.791812 0.0 -1.02019-01-06 NaN NaN NaN NaN NaN 应用 通过apply()方法，可以对数据进行逐一操作: 累计求和 123456789df.apply(np.cumsum)Out[130]: A B C D F2019-01-01 0.000000 0.000000 0.392435 5 NaN2019-01-02 -2.420703 -1.116208 0.046365 10 1.02019-01-03 -1.056278 -2.063849 2.433245 15 3.02019-01-04 -1.542258 -3.345303 2.787307 20 6.02019-01-05 -2.664975 -6.134345 1.995495 25 10.02019-01-06 -2.443377 -6.887383 0.254239 30 15.0 这里使用了apply()方法调用np.cumsum方法，也可直接使用df.cumsum(): 123456789df.cumsum()Out[133]: A B C D F2019-01-01 0.000000 0.000000 0.392435 5.0 NaN2019-01-02 -2.420703 -1.116208 0.046365 10.0 1.02019-01-03 -1.056278 -2.063849 2.433245 15.0 3.02019-01-04 -1.542258 -3.345303 2.787307 20.0 6.02019-01-05 -2.664975 -6.134345 1.995495 25.0 10.02019-01-06 -2.443377 -6.887383 0.254239 30.0 15.0 自定义方法 通过自定义函数，配合apply()方法，可以实现更多数据处理： 12345678df.apply(lambda x: x.max() - x.min())Out[134]: A 3.785129B 2.789041C 4.128136D 0.000000F 4.000000dtype: float64 矩阵 统计矩阵中每个元素出现的频次： 1234567891011121314151617181920212223s = pd.Series(np.random.randint(0, 7, size=10))sOut[136]: 0 21 02 43 04 35 36 67 48 69 5dtype: int64s.value_counts()Out[137]: 6 24 23 20 25 12 1dtype: int64 String方法 所有的Series类型都可以直接调用str的属性方法来对每个对象进行操作。 比如转换成大写： 12345678910111213s = pd.Series([&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;Aaba&apos;, &apos;Baca&apos;, np.nan, &apos;CABA&apos;, &apos;dog&apos;, &apos;cat&apos;])s.str.upper()Out[139]: 0 A1 B2 C3 AABA4 BACA5 NaN6 CABA7 DOG8 CATdtype: object 分列： 1234567891011s = pd.Series(['A,b', 'c,d'])sOut[142]: 0 A,b1 c,ddtype: objects.str.split(',', expand=True)Out[143]: 0 10 A b1 c d 其他方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647dir(str)Out[140]: [&apos;capitalize&apos;, &apos;casefold&apos;, &apos;center&apos;, &apos;count&apos;, &apos;encode&apos;, &apos;endswith&apos;, &apos;expandtabs&apos;, &apos;find&apos;, &apos;format&apos;, &apos;format_map&apos;, &apos;index&apos;, &apos;isalnum&apos;, &apos;isalpha&apos;, &apos;isascii&apos;, &apos;isdecimal&apos;, &apos;isdigit&apos;, &apos;isidentifier&apos;, &apos;islower&apos;, &apos;isnumeric&apos;, &apos;isprintable&apos;, &apos;isspace&apos;, &apos;istitle&apos;, &apos;isupper&apos;, &apos;join&apos;, &apos;ljust&apos;, &apos;lower&apos;, &apos;lstrip&apos;, &apos;maketrans&apos;, &apos;partition&apos;, &apos;replace&apos;, &apos;rfind&apos;, &apos;rindex&apos;, &apos;rjust&apos;, &apos;rpartition&apos;, &apos;rsplit&apos;, &apos;rstrip&apos;, &apos;split&apos;, &apos;splitlines&apos;, &apos;startswith&apos;, &apos;strip&apos;, &apos;swapcase&apos;, &apos;title&apos;, &apos;translate&apos;, &apos;upper&apos;, &apos;zfill&apos;] 合并 Mergepandas`可以提供很多方法可以快速的合并各种类型的Series、DataFrame以及Panel Object。 Concat方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344df = pd.DataFrame(np.random.randn(10, 4))dfOut[145]: 0 1 2 30 -0.227408 -0.185674 -0.187919 0.1856851 1.132517 -0.539992 1.156631 -0.0224682 0.214134 -1.283055 -0.862972 0.5189423 0.785903 1.033915 -0.471496 -1.4037624 -0.676717 -0.529971 -1.161988 -1.2650715 0.670126 1.320960 -0.128098 0.7186316 0.589902 0.349386 0.221955 1.7491887 -0.328885 0.607929 -0.973610 -0.9284728 1.724243 -0.661503 -0.374254 0.4092509 1.346625 0.618285 0.528776 -0.628470# break it into piecespieces = [df[:3], df[3:7], df[7:]]piecesOut[147]: [ 0 1 2 3 0 -0.227408 -0.185674 -0.187919 0.185685 1 1.132517 -0.539992 1.156631 -0.022468 2 0.214134 -1.283055 -0.862972 0.518942, 0 1 2 3 3 0.785903 1.033915 -0.471496 -1.403762 4 -0.676717 -0.529971 -1.161988 -1.265071 5 0.670126 1.320960 -0.128098 0.718631 6 0.589902 0.349386 0.221955 1.749188, 0 1 2 3 7 -0.328885 0.607929 -0.973610 -0.928472 8 1.724243 -0.661503 -0.374254 0.409250 9 1.346625 0.618285 0.528776 -0.628470]pd.concat(pieces)Out[148]: 0 1 2 30 -0.227408 -0.185674 -0.187919 0.1856851 1.132517 -0.539992 1.156631 -0.0224682 0.214134 -1.283055 -0.862972 0.5189423 0.785903 1.033915 -0.471496 -1.4037624 -0.676717 -0.529971 -1.161988 -1.2650715 0.670126 1.320960 -0.128098 0.7186316 0.589902 0.349386 0.221955 1.7491887 -0.328885 0.607929 -0.973610 -0.9284728 1.724243 -0.661503 -0.374254 0.4092509 1.346625 0.618285 0.528776 -0.628470 Merge方法 这是类似sql的合并方法： 12345678910111213141516171819left = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'lval': [1, 2]&#125;)right = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'rval': [4, 5]&#125;)leftOut[151]: key lval0 foo 11 foo 2rightOut[152]: key rval0 foo 41 foo 5pd.merge(left, right, on='key')Out[153]: key lval rval0 foo 1 41 foo 1 52 foo 2 43 foo 2 5 另一个例子： 1234567891011121314151617left = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'lval': [1, 2]&#125;)right = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'rval': [4, 5]&#125;)leftOut[156]: key lval0 foo 11 bar 2rightOut[157]: key rval0 foo 41 bar 5pd.merge(left, right, on='key')Out[158]: key lval rval0 foo 1 41 bar 2 5 Append方法 在DataFrame中增加行 1234567891011121314151617181920212223242526272829303132df = pd.DataFrame(np.random.randn(8, 4), columns=['A', 'B', 'C', 'D'])dfOut[160]: A B C D0 -0.496709 0.573449 0.076059 0.6852851 0.479253 0.587376 -1.240070 -0.9079102 -0.052609 -0.287786 -1.949402 1.1633233 -0.659489 0.525583 0.820922 -1.3685444 1.270453 -1.813249 0.059915 0.5867035 1.859657 0.564274 -0.198763 -1.7941736 -0.649153 -3.129258 0.063418 -0.7279367 0.862402 -0.800031 -1.954784 -0.028607s = df.iloc[3]sOut[162]: A -0.659489B 0.525583C 0.820922D -1.368544Name: 3, dtype: float64df.append(s, ignore_index=True)Out[163]: A B C D0 -0.496709 0.573449 0.076059 0.6852851 0.479253 0.587376 -1.240070 -0.9079102 -0.052609 -0.287786 -1.949402 1.1633233 -0.659489 0.525583 0.820922 -1.3685444 1.270453 -1.813249 0.059915 0.5867035 1.859657 0.564274 -0.198763 -1.7941736 -0.649153 -3.129258 0.063418 -0.7279367 0.862402 -0.800031 -1.954784 -0.0286078 -0.659489 0.525583 0.820922 -1.368544 这里要注意，我们增加了ignore_index=True参数，如果不设置的话，那么增加的新行的index仍然是3，这样在后续的处理中可能有存在问题。具体也需要看情况来处理。 123456789101112df.append(s)Out[164]: A B C D0 -0.496709 0.573449 0.076059 0.6852851 0.479253 0.587376 -1.240070 -0.9079102 -0.052609 -0.287786 -1.949402 1.1633233 -0.659489 0.525583 0.820922 -1.3685444 1.270453 -1.813249 0.059915 0.5867035 1.859657 0.564274 -0.198763 -1.7941736 -0.649153 -3.129258 0.063418 -0.7279367 0.862402 -0.800031 -1.954784 -0.0286073 -0.659489 0.525583 0.820922 -1.368544 分组 Grouping一般分组统计有三个步骤： 分组：选择需要的数据 计算：对每个分组进行计算 合并：把分组计算的结果合并为一个数据结构中 12345678910111213141516df = pd.DataFrame(&#123;'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'], 'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'], 'C': np.random.randn(8), 'D': np.random.randn(8)&#125;)dfOut[166]: A B C D0 foo one -1.252153 0.1728631 bar one 0.238547 -0.6489802 foo two 0.756975 0.1957663 bar three -0.933405 -0.3200434 foo two -0.310650 -1.3882555 bar two 1.568550 -1.9118176 foo one -0.340290 -2.141259 按A列分组并使用sum函数进行计算： 123456df.groupby('A').sum()Out[167]: C DA bar 0.873692 -2.880840foo -1.817027 -5.833961 这里由于B列无法应用sum函数，所以直接被忽略了。 按A、B列分组并使用sum函数进行计算： 12345678910df.groupby(['A', 'B']).sum()Out[168]: C DA B bar one 0.238547 -0.648980 three -0.933405 -0.320043 two 1.568550 -1.911817foo one -1.592443 -1.968396 three -0.670909 -2.673075 two 0.446325 -1.192490 这样就有了一个多层index的结果集。 整形 Reshaping 堆叠 Stack python的zip函数可以将对象中对应的元素打包成一个个的元组： 123456789101112131415161718192021222324252627282930313233343536373839404142434445tuples = list(zip(['bar', 'bar', 'baz', 'baz','foo', 'foo', 'qux', 'qux'],['one', 'two', 'one', 'two','one', 'two', 'one', 'two']))tuplesOut[172]: [('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')]## 设置两级索引index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])indexOut[174]: MultiIndex(levels=[['bar', 'baz', 'foo', 'qux'], ['one', 'two']], codes=[[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]], names=['first', 'second'])## 创建DataFramedf = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])dfOut[176]: A Bfirst second bar one -0.501215 -0.947993 two -0.828914 0.232167baz one 1.245419 1.006092 two 1.016656 -0.441073foo one 0.479037 -0.500034 two -1.113097 0.591696qux one -0.014760 -0.320735 two -0.648743 1.499899## 选取DataFramedf2 = df[:4]df2Out[179]: A Bfirst second bar one -0.501215 -0.947993 two -0.828914 0.232167baz one 1.245419 1.006092 two 1.016656 -0.441073 使用stack()方法，可以通过堆叠的方式将二维数据变成为一维数据： 12345678910111213stacked = df2.stack()stackedOut[181]: first second bar one A -0.501215 B -0.947993 two A -0.828914 B 0.232167baz one A 1.245419 B 1.006092 two A 1.016656 B -0.441073dtype: float64 对应的逆操作为unstacked()方法： 123456789101112131415161718192021222324stacked.unstack()Out[182]: A Bfirst second bar one -0.501215 -0.947993 two -0.828914 0.232167baz one 1.245419 1.006092 two 1.016656 -0.441073stacked.unstack(1)Out[183]: second one twofirst bar A -0.501215 -0.828914 B -0.947993 0.232167baz A 1.245419 1.016656 B 1.006092 -0.441073stacked.unstack(0)Out[184]: first bar bazsecond one A -0.501215 1.245419 B -0.947993 1.006092two A -0.828914 1.016656 B 0.232167 -0.441073 unstack()默认对最后一层级进行操作，也可通过输入参数指定。 表格转置 1234567891011121314151617181920df = pd.DataFrame(&#123;'A': ['one', 'one', 'two', 'three'] * 3,'B': ['A', 'B', 'C'] * 4,'C': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2,'D': np.random.randn(12),'E': np.random.randn(12)&#125;)dfOut[190]: A B C D E0 one A foo -0.933264 -2.3874901 one B foo -0.288101 0.0232142 two C foo 0.594490 0.4185053 three A bar 0.450683 1.9396234 one B bar 0.243897 -0.9657835 one C bar -0.705494 -0.0782836 two A foo 1.560352 0.4199077 three B foo 0.199453 0.9987118 one C foo 1.426861 -1.1082979 one A bar -0.570951 -0.02256010 two B bar -0.350937 -1.76780411 three C bar 0.983465 0.065792 通过pivot_table()方法可以很方便的进行行列的转换： 12345678910111213pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'])Out[191]: C bar fooA B one A -0.570951 -0.933264 B 0.243897 -0.288101 C -0.705494 1.426861three A 0.450683 NaN B NaN 0.199453 C 0.983465 NaNtwo A NaN 1.560352 B -0.350937 NaN C NaN 0.594490 转换中，涉及到空值部分会自动填充为NaN。 时间序列 Time Seriespandas的在时序转换方面十分强大，可以很方便的进行各种转换。 时间间隔调整 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748rng = pd.date_range('1/1/2019', periods=100, freq='S')rng[:5]Out[214]: DatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 00:00:01', '2019-01-01 00:00:02', '2019-01-01 00:00:03', '2019-01-01 00:00:04'], dtype='datetime64[ns]', freq='S')ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)ts.head(5)Out[216]: 2019-01-01 00:00:00 2452019-01-01 00:00:01 3472019-01-01 00:00:02 1132019-01-01 00:00:03 1962019-01-01 00:00:04 131Freq: S, dtype: int64## 按10s间隔进行重新采样ts1 = ts.resample('10S')ts1Out[209]: DatetimeIndexResampler [freq=&lt;10 * Seconds&gt;, axis=0, closed=left, label=left, convention=start, base=0]## 用求平均的方式进行数据整合 ts1.mean()Out[218]: 2019-01-01 00:00:00 174.02019-01-01 00:00:10 278.52019-01-01 00:00:20 281.82019-01-01 00:00:30 337.22019-01-01 00:00:40 221.02019-01-01 00:00:50 277.12019-01-01 00:01:00 171.02019-01-01 00:01:10 321.02019-01-01 00:01:20 318.62019-01-01 00:01:30 302.6Freq: 10S, dtype: float64## 用求和的方式进行数据整合 ts1.sum()Out[219]: 2019-01-01 00:00:00 17402019-01-01 00:00:10 27852019-01-01 00:00:20 28182019-01-01 00:00:30 33722019-01-01 00:00:40 22102019-01-01 00:00:50 27712019-01-01 00:01:00 17102019-01-01 00:01:10 32102019-01-01 00:01:20 31862019-01-01 00:01:30 3026Freq: 10S, dtype: int64 这里先通过resample进行重采样，在指定sum()或者mean()等方式来指定冲采样的处理方式。 显示时区： 123456789101112131415161718192021222324rng = pd.date_range('1/1/2019 00:00', periods=5, freq='D')rngOut[221]: DatetimeIndex(['2019-01-01', '2019-01-02', '2019-01-03', '2019-01-04', '2019-01-05'], dtype='datetime64[ns]', freq='D')ts = pd.Series(np.random.randn(len(rng)), rng)tsOut[223]: 2019-01-01 -2.3276862019-01-02 1.5278722019-01-03 0.0639822019-01-04 -0.2135722019-01-05 -0.014856Freq: D, dtype: float64ts_utc = ts.tz_localize('UTC')ts_utcOut[225]: 2019-01-01 00:00:00+00:00 -2.3276862019-01-02 00:00:00+00:00 1.5278722019-01-03 00:00:00+00:00 0.0639822019-01-04 00:00:00+00:00 -0.2135722019-01-05 00:00:00+00:00 -0.014856Freq: D, dtype: float64 转换时区： 12345678ts_utc.tz_convert(&apos;US/Eastern&apos;)Out[226]: 2018-12-31 19:00:00-05:00 -2.3276862019-01-01 19:00:00-05:00 1.5278722019-01-02 19:00:00-05:00 0.0639822019-01-03 19:00:00-05:00 -0.2135722019-01-04 19:00:00-05:00 -0.014856Freq: D, dtype: float64 时间格式转换 123456789101112131415161718192021222324252627rng = pd.date_range('1/1/2019', periods=5, freq='M')ts = pd.Series(np.random.randn(len(rng)), index=rng)tsOut[230]: 2019-01-31 0.1971342019-02-28 0.5690822019-03-31 -0.3221412019-04-30 0.0057782019-05-31 -0.082306Freq: M, dtype: float64ps = ts.to_period()psOut[232]: 2019-01 0.1971342019-02 0.5690822019-03 -0.3221412019-04 0.0057782019-05 -0.082306Freq: M, dtype: float64ps.to_timestamp()Out[233]: 2019-01-01 0.1971342019-02-01 0.5690822019-03-01 -0.3221412019-04-01 0.0057782019-05-01 -0.082306Freq: MS, dtype: float64 在是时间段和时间转换过程中，有一些很方便的算术方法可以使用，比如我们转换如下两个频率： 1、按季度划分，且每个年的最后一个月是11月。 2、按季度划分，每个月开始为频率一中下一个月的早上9点。 123456789101112131415161718192021222324252627282930prng = pd.period_range('2018Q1', '2019Q4', freq='Q-NOV')prngOut[243]: PeriodIndex(['2018Q1', '2018Q2', '2018Q3', '2018Q4', '2019Q1', '2019Q2', '2019Q3', '2019Q4'], dtype='period[Q-NOV]', freq='Q-NOV')ts = pd.Series(np.random.randn(len(prng)), prng)tsOut[245]: 2018Q1 -0.1126922018Q2 -0.5073042018Q3 -0.3248462018Q4 0.5496712019Q1 -0.8977322019Q2 1.1300702019Q3 -0.3998142019Q4 0.830488Freq: Q-NOV, dtype: float64ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9tsOut[247]: 2018-03-01 09:00 -0.1126922018-06-01 09:00 -0.5073042018-09-01 09:00 -0.3248462018-12-01 09:00 0.5496712019-03-01 09:00 -0.8977322019-06-01 09:00 1.1300702019-09-01 09:00 -0.3998142019-12-01 09:00 0.830488Freq: H, dtype: float64 注意：这个例子有点怪。可以这样理解，我们先将prng直接转换为按小时显示： 123456prng.asfreq('H', 'end') Out[253]: PeriodIndex(['2018-02-28 23:00', '2018-05-31 23:00', '2018-08-31 23:00', '2018-11-30 23:00', '2019-02-28 23:00', '2019-05-31 23:00', '2019-08-31 23:00', '2019-11-30 23:00'], dtype='period[H]', freq='H') 我们要把时间转换为下一个月的早上9点，所以先转换为按月显示，并每个月加1（即下个月），然后按小时显示并加9（早上9点）。 另外例子中s参数是start的简写，e参数是end的简写，Q-NOV即表示按季度，且每年的NOV是最后一个月。 更多了freq简称可以参考：http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#anchored-offsets asfreq（）方法介绍可参考：http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.asfreq.html#pandas-dataframe-asfreq 分类目录类型 Categoricals 关于Categories类型介绍可以参考：http://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html#categorical 类型转换：astype(&#39;category&#39;) 12345678910111213141516171819202122df = pd.DataFrame(&#123;"id": [1, 2, 3, 4, 5, 6],"raw_grade": ['a', 'b', 'b', 'a', 'a', 'e']&#125;)dfOut[255]: id raw_grade0 1 a1 2 b2 3 b3 4 a4 5 a5 6 edf['grade'] = df['raw_grade'].astype('category')df['grade']Out[257]: 0 a1 b2 b3 a4 a5 eName: grade, dtype: categoryCategories (3, object): [a, b, e] 重命名分类：cat 1234567891011df["grade"].cat.categories = ["very good", "good", "very bad"]df['grade']Out[269]: 0 very good1 good2 good3 very good4 very good5 very badName: grade, dtype: categoryCategories (3, object): [very good, good, very bad] 重分类： 1234567891011df['grade'] = df['grade'].cat.set_categories(["very bad", "bad", "medium","good", "very good"])df['grade']Out[271]: 0 very good1 good2 good3 very good4 very good5 very badName: grade, dtype: categoryCategories (5, object): [very bad, bad, medium, good, very good] 排列 123456789df.sort_values(by="grade")Out[272]: id raw_grade grade5 6 e very bad1 2 b good2 3 b good0 1 a very good3 4 a very good4 5 a very good 分组 123456789df.groupby("grade").size()Out[273]: gradevery bad 1bad 0medium 0good 2very good 3dtype: int64 画图 Plotting Series 123456789ts = pd.Series(np.random.randn(1000),index=pd.date_range('1/1/2000', periods=1000))ts = pd.Series(np.random.randn(1000),index=pd.date_range('1/1/2019', periods=1000))ts = ts.cumsum()ts.plot()Out[277]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1135bcc50&gt;import matplotlib.pyplot as pltplt.show() DataFrame画图 使用plot可以把所有的列都通过标签的形式展示出来： 12345678df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index,columns=['A', 'B', 'C', 'D'])df = df.cumsum()plt.figure()Out[282]: &lt;Figure size 640x480 with 0 Axes&gt;df.plot()Out[283]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x11587e4e0&gt;plt.legend(loc='best') 导入导出数据 Getting Data In/Out CSV 写入： 1df.to_csv('foo.csv') 读取： 1pd.read_csv('foo.csv') HDF5 写入： 1df.to_hdf('foo.h5', 'df') 读取： 1pd.read_hdf('foo.h5', 'df') Excel 写入： 1df.to_excel('foo.xlsx', sheet_name='Sheet1') 读取： 1pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA']) 异常处理 Gotchas如果有一些异常情况比如： 12345&gt;&gt;&gt; if pd.Series([False, True, False]):... print("I was true")Traceback ...ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all(). 可以参考如下链接： http://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html#basics-compare http://pandas.pydata.org/pandas-docs/stable/user_guide/gotchas.html#gotchas]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib中文展示乱码问题]]></title>
    <url>%2Fmatplotlib%E4%B8%AD%E6%96%87%E5%B1%95%E7%A4%BA%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景使用Python做数据分析时，不可避免的需要使用matplotlib画图。但它对中文并不友好，使用中文作为标签或者抬头会有乱码方框的问题： 解决方案方案一：设置中文字体matplotlib提供了FontProperties对象，可以通过手工设置字体来格式化生成的展示图片。具体实现如下： 找到中文字体文件 在Mac下，可通过打开“字体册”应用找到对应的字体和字体所在的路径（默认为：/System/Library/Fonts/PingFang.ttc） 在windows下，一般在C:\Windows\Fonts目录下。 也可直接网上下载需要的字体 配置字体属性 12from matplotlib.font_manager import FontPropertiesfont = FontProperties(fname='/System/Library/Fonts/PingFang.ttc') 引用字体属性 1plt.title("中文测试", fontproperties=font) 画图即可： 方案二： 使用第三方库为解决中文画图的问题，强大的网友自己做了第三方支持中文库可以方便的展示中文，且支持 matplotlib 混合编程, 完全相同的API设计。 安装pyployz 1pip install pyplotz 使用pyplotz和matplotlib混合配置 12345678910import matplotlib.pyplot as pltplt.plot()# 新建pltz对象，用于显示中文from pyplotz.pyplotz import PyplotZpltz = PyplotZ()pltz.enable_chinese()pltz.title('中文测试')plt.savefig('Chinese_1')plt.show() 效果如下： 更多介绍可以参考官方说明：https://github.com/songlinhou/pyplotz 总结两种方式都可以实现中文画图，方法一灵活性更好，但每行涉及到中文的地方都需要配置，方法二需要另外引入第三方包，并进行初始化，在首次使用时会自动下载中文字符集。 建议大家还是按需选用，如果有更好的方法，欢迎交流。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用snownlp分析流浪地球评论]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8snownlp%E5%88%86%E6%9E%90%E6%B5%81%E6%B5%AA%E5%9C%B0%E7%90%83%E8%AF%84%E8%AE%BA.html</url>
    <content type="text"><![CDATA[背景前面通过爬虫获取了猫眼上”流浪地球“的评论：获取猫眼电影的评论，并制作了词云，使用pandas初步分析了电影热度情况，还可以分析什么呢？ SnowNLP是个开源的NPL分析库，可以方便的进行中文分词，特点是可以进行情感评测，来分析评论的积极程度，主要用于电商网站中的商品评论的分析。那么分析下电影评论情况如何呢？ 思路前面我们再抓取时，将评论的时间和内容通过csv的格式保存下来，并使用;分割。 读取数据pandas提供read_csv方法来直接独处数据保存为DateFrame格式。 1df = pd.read_csv('comment.csv', sep=';', header=None) 获取情感评分通过pandas读取csv后，评论会保存在df[1]中，我们需要对每一条评论进行情感评分，pandas提供了apply功能配合lambda就可以方便的实现了： 12sentiment = lambda x:SnowNLP(x).sentimentsdf[2] = df[1].apply(sentiment) 数据处理 设置数据列名 1df.columns = ['date', 'comment', 'sentiment'] 时间日期处理 在date列，我们保存的数据格式是string，需要把转换为日期格式才能进一步处理。 1df['date'] = pd.to_datetime(df['date']) 我们需要按时间来统计，所以把date列设置为index: 1df = df.set_index('date') 统计处理 日期筛选 由于我们知道《流浪地球》是2月5日上映的，我们可以对日期进行限定，以免出现有些在上映前的评论，会占用大段的空白情况。 设置index之后，可以参考list类型操作，由于时间是倒序的，所以可以直接使用[:&#39;2019-02-04&#39;]来选取2月4日之后到今天的所有数据，并选取sentiment列。 1cacu_df = df[:'2019-02-04']['sentiment'] 按日期进行数量统计 pandas中，通过resample方法进行重新按日采样，并求汇总的平均值。 1cacu = cacu_df.resample('D').mean() 这样就完成了按日期求和统计操作。 绘图画图需要使用matplotlib库，通过导入该库，可直接对DateFrame对象进行画图处理。画图及图表格式化如下： 12345678910111213141516171819# 通过设置中文字体方式解决中文展示问题font = FontProperties(fname='../font/PingFang.ttc')plt.title("流浪地球评论分析", fontproperties=font)plt.xlabel("日期", fontproperties=font)plt.ylabel("好感度", fontproperties=font)plt.plot(cacu)plt.axis("tight")# 显示网格plt.grid(True)# 自动旋转横轴日期plt.gcf().autofmt_xdate()# 显示数值for a, b in zip(cacu.index, cacu.values): plt.text(a, b, str(round(b, 4)))# 保存图片plt.savefig('comment_sentiment_analysis.png')# 查看图片plt.show() 结论以上就是使用抓取的评论生成情感分析统计图片的大致思路，完成的实现代码请见：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/maoyan 结果如下： 根据SnowNLP，sentiment值是指：positive的概率，可以简单理解为分值大于0.5为积极，小于为消极。那么这个结论就很奇怪了，那么我们看下每个评论的评分情况： 123456789100 中国科幻电影，继续加油↖(^ω^)↗ 0.51 特效什么的真的不错，不过俩小主演感觉演绎经验不是很足，达叔演技是真的很好，，每个他的镜头还是... 0.52 的确是国产中最好的科幻片了 0.53 非常棒，特效完全是大师水准 0.54 完美。。。。。。。。。。 0.55 挺不错，大型科幻片，很有意义 0.56 简直是侮辱我的智商啊，一点常识都没有，。这种片子怎么能九点多分，刷出来的吧。 0.57 太好看了赞...大卖.. 0.58 虽然和最理想的大片上有一定差距，但是已经明显能够感受到慢慢的诚意，值得肯定 0.59 酷炫吊炸天，第一次看我们国家出来这么厉害的科幻，希望以后哆哆出。一定会支持！！ 0.5 这个评分。。。相当不准。。。 再看下官网的说明： 情感分析（现在训练数据主要是买卖东西时的评价，所以对其他的一些可能效果不是很好，待解决） 好吧，希望以后可以尽快解决啦]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Pandas分析流浪地球评论数据]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8Pandas%E5%88%86%E6%9E%90%E6%B5%81%E6%B5%AA%E5%9C%B0%E7%90%83%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE.html</url>
    <content type="text"><![CDATA[背景前面通过爬虫获取了猫眼上”流浪地球“的评论：获取猫眼电影的评论，除了做词云之外，还可以对数据进行分析，看看大家对电影的反响如何。 思路前面我们再抓取时，将评论的时间和内容通过csv的格式保存下来，并使用;分割。读取csv文件并统计处理就要用到大名鼎鼎的pandas了。 读取数据pandas提供read_csv方法来直接独处数据保存为DateFrame格式。 1df = pd.read_csv('comment.csv', sep=';', header=None) 数据处理 设置数据列名 由于我们知道数据有两列，先通过这只列名可以方便后续引用。 1df.columns = ['date', 'comment'] 时间日期处理 在date列，我们保存的数据格式是string，需要把转换为日期格式才能进一步处理。 1df['date'] = pd.to_datetime(df['date']) 我们需要按时间来统计，所以把date列设置为index: 1df = df.set_index('date') 统计处理 日期筛选 由于我们知道《流浪地球》是2月5日上映的，我们可以对日期进行限定，以免出现有些在上映前的评论，会占用大段的空白情况。 设置index之后，可以参考list类型操作，由于时间是倒序的，所以可以直接使用[:&#39;2019-02-04&#39;]来选取2月4日之后到今天的所有数据。pandas在数据筛选方面相当智能，按照datetime的格式直接筛选即可。 1cacu_df = df[:'2019-02-04'] 按日期进行数量统计 pandas中，通过resample方法进行重新采样，通过传入rule参数就可以按需要的频率获取数据，获得一个resampler对象。 1DataFrame.resample(rule, how=None, axis=0, fill_method=None, closed=None, label=None, convention='start', kind=None, loffset=None, limit=None, base=0, on=None, level=None) resampler对象提供了很多的统计方法，比如汇总求和可使用Resampler.count()。 12# 按日统计数量cacu = cacu_df.resample('D').count() 这样就完成了按日期求和统计操作。 绘图画图需要使用matplotlib库，通过导入该库，可直接对DateFrame对象进行画图处理。画图及图表格式化如下： 123456789101112131415161718# 设置中文字体font = FontProperties(fname='/System/Library/Fonts/PingFang.ttc')plt.plot(cacu)plt.title("流浪地球评论分析", fontproperties=font)plt.xlabel("日期", fontproperties=font)plt.ylabel("评论数", fontproperties=font)plt.axis("tight")# 显示网格plt.grid(True)# 自动旋转横轴日期plt.gcf().autofmt_xdate()# 显示数值for a, b in zip(cacu.index, cacu.values):plt.text(a, b, str(b[0]))# 保存图片plt.savefig('comment_analysis.png')# 查看图片plt.show() 结论以上就是使用抓取的评论生成日期统计图片的大致思路，完成的实现代码请见：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/maoyan 结果如下： 可见从上映之后，关注度直线飙升，到2月10日之后（上映5天），大家关注度逐渐下降。其中2月14日为情人节，大家的关注又有了小幅的上升。也许很多人在这天通过看《流浪地球》过节吧。 拓展 matplotlib画图说明 该库是默认不支持中文的，所以如果不经过配置直接画图，设置中文图标会显示为空白方框。详细配置参考上文。 pandas使用 更多的resampler的方法，可见：http://pandas.pydata.org/pandas-docs/stable/reference/resampling.html 更多resample的rule参考下表： Date Offset Frequency String Description DateOffset None Generic offset class, defaults to 1 calendar day BDay or BusinessDay &#39;B&#39; business day (weekday) CDay or CustomBusinessDay &#39;C&#39; custom business day Week &#39;W&#39; one week, optionally anchored on a day of the week WeekOfMonth &#39;WOM&#39; the x-th day of the y-th week of each month LastWeekOfMonth &#39;LWOM&#39; the x-th day of the last week of each month MonthEnd &#39;M&#39; calendar month end MonthBegin &#39;MS&#39; calendar month begin BMonthEnd or BusinessMonthEnd &#39;BM&#39; business month end BMonthBegin or BusinessMonthBegin &#39;BMS&#39; business month begin CBMonthEnd or CustomBusinessMonthEnd &#39;CBM&#39; custom business month end CBMonthBegin or CustomBusinessMonthBegin &#39;CBMS&#39; custom business month begin SemiMonthEnd &#39;SM&#39; 15th (or other day_of_month) and calendar month end SemiMonthBegin &#39;SMS&#39; 15th (or other day_of_month) and calendar month begin QuarterEnd &#39;Q&#39; calendar quarter end QuarterBegin &#39;QS&#39; calendar quarter begin BQuarterEnd &#39;BQ business quarter end BQuarterBegin &#39;BQS&#39; business quarter begin FY5253Quarter &#39;REQ&#39; retail (aka 52-53 week) quarter YearEnd &#39;A&#39; calendar year end YearBegin &#39;AS&#39; or &#39;BYS&#39; calendar year begin BYearEnd &#39;BA&#39; business year end BYearBegin &#39;BAS&#39; business year begin FY5253 &#39;RE&#39; retail (aka 52-53 week) year Easter None Easter holiday BusinessHour &#39;BH&#39; business hour CustomBusinessHour &#39;CBH&#39; custom business hour Day &#39;D&#39; one absolute day Hour &#39;H&#39; one hour Minute &#39;T&#39; or &#39;min&#39; one minute Second &#39;S&#39; one second Milli &#39;L&#39; or &#39;ms&#39; one millisecond Micro &#39;U&#39; or &#39;us&#39; one microsecond Nano &#39;N&#39; one nanosecond]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac上如何快速获得文件路径]]></title>
    <url>%2FMac%E4%B8%8A%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E8%8E%B7%E5%BE%97%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84.html</url>
    <content type="text"><![CDATA[背景在使用开发中，免不了要涉及文件的读写。在Mac环境下如何快速的获得某文件的路径呢？ 思路方法一：pwd如果是习惯于使用命令行的用户，那么直接在命令行通过cd来找到对应的文件，再使用pwd即可： 12$ pwd/Users/zhengk/PycharmProjects/Mine/maoyan 该方法简单，直观，但需要对文件所在位置有较明确的认识。 方法二：Finder使用Mac的Finder（达坊）功能： 先打开Finder，在“显示”中勾选“显示路径栏”； 这时候在Finder窗口最下端会有显示文件的路径，选中需要的文件，鼠标右键“路径栏”中的文件名，就可以看到“拷贝为路径名称”的选项； 拷贝即可，结果如下： 1/Users/zhengk/PycharmProjects/Mine/maoyan/jupiter.png 方法三：结合Finder与终端 先在Finder中找到需要确认路径的文件 打开一个空白的终端窗口 将文件拖到终端窗口中，终端中就会出现该文件的路径地址。 1$ /Users/zhengk/PycharmProjects/Mine/maoyan/jupiter.png 结论以上三种方法都可以在Mac中获取对应文件的路径。 方法一针对深度终端用户会比较好，如果明确知道文件的位置也会很快。 方法二和方法三通过可视化界面获取，比较直观。]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用wordcloud绘制词云]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8wordcloud%E7%BB%98%E5%88%B6%E8%AF%8D%E4%BA%91.html</url>
    <content type="text"><![CDATA[背景前面通过爬虫获取了猫眼上”流浪地球“的评论：获取猫眼电影的评论 那么就做一个酷炫的词云吧，直观的看看大家都在想什么，先上效果图： 思路数据清洗首先由于评论是用户发表的，可能什么字符都会有，要先把一些特殊符号去掉，这里就用到了正则替换： 1msg = re.sub("[\s+\.\!\/_,$%^*()+\"\'\?]+|[+——！，。？、~@#￥%……&amp;*（）【】；：]+|\[.+\]|\［.+\］", "", line) 分词与标签清洗后的数据，可以使用jieba分词包来进行分词，并把所有的分词保存在一个list中，然后计算出每个分词出现的次数。 12345678910# 分词tags = jieba.analyse.extract_tags(msg)for t in tags: word_list.append(t)# 计算词频for word in word_list: if word not in word_dict: word_dict[word] = 1 else: word_dict[word] += 1 生成词云使用wordcloud包，就可以很方便的生成词云图片了。 先新建一个WordCloud对象，进行配置，然后利用前面的分词词频就可以生成对应的图片了。 12345678# 计算图片颜色alice_coloring = np.array(img)my_wordcloud = WordCloud(background_color="white", max_words=500, mask=alice_coloring, max_font_size=200, random_state=42, font_path=(os.path.join(d, "font/msyh.ttf"))) my_wordcloud = my_wordcloud.generate_from_frequencies(wordList) 这里需要注意的是： mask=alice_coloring：这里通过numpy将图片矩阵化，来获取图片的颜色作为WordCloud的mask，是为了最后生成的图云不仅外形与我们输入的图片保持一致，而且整体颜色也保持一致。 输入的原图，背景色需要设置为白色而不是透明色，否则会全屏幕都是字。。。 对于中文的词云，需要制定中文的字体，这里用的是微软雅黑 保存图片最后使用matplotlib.pyplot来保存图片，保存前要进行图片属性的一些设置。 12345678910width = img.width/80height = img.height/80plt.figure(figsize=(width, height))plt.imshow(my_wordcloud.recolor(color_func=image_colors))plt.imshow(my_wordcloud)plt.axis("off")# 通过设置subplots_adjust来控制画面外边框plt.subplots_adjust(bottom=.01, top=.99, left=.01, right=.99)plt.savefig("jupiter_wordcloud_1.png")plt.show() 这里需要注意的是： 建议根据原图片的长宽比例进行一定的缩小，以免生成的图片像素过大而产生报错。 1ValueError: Image size of 98400x46500 pixels is too large. It must be less than 2^16 in each direction. 结论以上就是使用抓取的评论生成词云的大致思路，完成的实现代码请见：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/maoyan 最后放一张原图，你能看的出来嘛，抠图技术有限O(∩_∩)O哈哈~]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用WMIC获取系统硬件信息]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8WMIC%E8%8E%B7%E5%8F%96%E7%B3%BB%E7%BB%9F%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF.html</url>
    <content type="text"><![CDATA[WMIC是扩展WMI（Windows Management Instrumentation，Windows管理工具），提供了从命令行接口和批命令脚本执行系统管理的支持。 在日常工作中，我们可以通过WMIC来获取计算机的硬件信息用于一些统计和分析。 使用WMIC直接在cmd中输入wmic即可进入交互式命令行： 12C:\Users&gt;wmicwmic:root\cli&gt; 获取硬盘信息硬盘信息可以通过输入diskdrive来获取相关信息 12345wmic:root\cli&gt;diskdriveAvailability BytesPerSector Capabilities CapabilityDescriptions Caption CompressionMethod ConfigManagerErrorCode ConfigManagerUserConfig CreationClassName DefaultBlockSize Description DeviceID ErrorCleared ErrorDescription ErrorMethodology FirmwareRevision Index InstallDate InterfaceType LastErrorCode Manufacturer MaxBlockSize MaxMediaSize MediaLoaded MediaType MinBlockSize Model Name NeedsCleaning NumberOfMediaSupported Partitions PNPDeviceID PowerManagementCapabilities PowerManagementSupported SCSIBus SCSILogicalUnit SCSIPort SCSITargetId SectorsPerTrack SerialNumber Signature Size Status StatusInfo SystemCreationClassName SystemName TotalCylinders TotalHeads TotalSectors TotalTracks TracksPerCylinder 512 &#123;3, 4, 7&#125; &#123;"Random Access", "Supports Writing", "Supports Removable Media"&#125; SanDisk Ultra Fit USB Device 0 FALSE Win32_DiskDrive 磁盘驱动器 \\.\PHYSICALDRIVE2 1.00 2 USB (标准磁盘驱动器) TRUE Removable Media SanDisk Ultra Fit USB Device \\.\PHYSICALDRIVE2 1 USBSTOR\DISK&amp;VEN_SANDISK&amp;PROD_ULTRA_FIT&amp;REV_1.00\4C530001140919121090&amp;0 0 0 0 0 63 4C530001140919121090 4047072857 250443325440 OK Win32_ComputerSystem KFZXZHENGKQ 30448 255 489147120 7764240 255 512 &#123;3, 4, 10&#125; &#123;"Random Access", "Supports Writing", "SMART Notification"&#125; PLEXTOR PX-128M6G-2242 0 FALSE Win32_DiskDrive 磁盘驱动器 \\.\PHYSICALDRIVE1 1.01 1 IDE (标准磁盘驱动器) TRUE Fixed hard disk media PLEXTOR PX-128M6G-2242 \\.\PHYSICALDRIVE1 2 SCSI\DISK&amp;VEN_PLEXTOR&amp;PROD_PX-128M6G-2242\4&amp;28BCE847&amp;0&amp;010000 1 0 0 0 63 P02512105359 1914799601 128034708480 OK Win32_ComputerSystem KFZXZHENGKQ 15566 255 250067790 3969330 255 512 &#123;3, 4, 10&#125; &#123;"Random Access", "Supports Writing", "SMART Notification"&#125; LITEON LCH-256V2S 0 FALSE Win32_DiskDrive 磁盘驱动器 \\.\PHYSICALDRIVE0 3C87901 0 IDE (标准磁盘驱动器) TRUE Fixed hard disk media LITEON LCH-256V2S \\.\PHYSICALDRIVE0 2 SCSI\DISK&amp;VEN_LITEON&amp;PROD_LCH-256V2S\4&amp;28BCE847&amp;0&amp;000000 0 0 0 0 63 SD0F66157L1TH57102RR 2070240311 256052966400 OK Win32_ComputerSystem KFZXZHENGKQ 31130 255 500103450 7938150 255 信息有点多，使用\?来看下有什么选项： 1234567891011121314wmic:root\cli&gt;diskdrive /?DISKDRIVE - 物理磁盘驱动器管理。提示: BNF 的别名用法。(&lt;别名&gt; [WMI 对象] | &lt;别名&gt; [&lt;路径 where&gt;] | [&lt;别名&gt;] &lt;路径 where&gt;) [&lt;谓词子句&gt;]。用法:DISKDRIVE ASSOC [&lt;格式说明符&gt;]DISKDRIVE CREATE &lt;分配列表&gt;DISKDRIVE DELETEDISKDRIVE GET [&lt;属性列表&gt;] [&lt;获取开关&gt;]DISKDRIVE LIST [&lt;列表格式&gt;] [&lt;列表开关&gt;] 根据帮助，我们可以使用get来获取需要的信息，比如获取容量信息： 12345wmic:root\cli&gt;diskdrive get SizeSize250443325440128034708480256052966400 获取内存信息内存信息可以使用memorychip来获取，我们试下不进入CLI交互模式（在命令前加上wmic即可）： 1234C:\Users&gt;wmic memorychipAttributes BankLabel Capacity Caption ConfiguredClockSpeed ConfiguredVoltage CreationClassName DataWidth Description DeviceLocator FormFactor HotSwappable InstallDate InterleaveDataDepth InterleavePosition Manufacturer MaxVoltage MemoryType MinVoltage Model Name OtherIdentifyingInfo PartNumber PositionInRow PoweredOn Removable Replaceable SerialNumber SKU SMBIOSMemoryType Speed Status Tag TotalWidth TypeDetail Version0 BANK 0 4294967296 物理内存 1600 Win32_PhysicalMemory 64 物理内存 ChannelA-DIMM0 12 Samsung 24 物理内存 K4B8G1646B-MYK0 78111110 24 1600 Physical Memory 0 64 128 0 BANK 2 4294967296 物理内存 1600 Win32_PhysicalMemory 64 物理内存 ChannelB-DIMM0 12 Samsung 24 物理内存 M471B5173QH0-YK0 18128315 24 1600 Physical Memory 1 64 128 获取内存的容量信息： 1234C:\Users&gt;wmic memorychip get CapacityCapacity42949672964294967296 WMIC其他功能通过\?可以发现WMIC的其他所有功能，有需要的话可以关注下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118wmic:root\cli&gt;/?[全局开关] &lt;命令&gt;可以使用以下全局开关:/NAMESPACE 别名在其上操作的命名空间的路径。/ROLE 包含别名定义的角色的路径。/NODE 别名在其上操作的服务器。/IMPLEVEL 客户端模拟级别。/AUTHLEVEL 客户端身份验证级别。/LOCALE 客户端应使用的语言 ID。/PRIVILEGES 启用或禁用所有权限。/TRACE 将调试信息输出到 stderr。/RECORD 记录所有输入命令和输出内容。/INTERACTIVE 设置或重置交互模式。/FAILFAST 设置或重置 FailFast 模式。/USER 会话期间要使用的用户。/PASSWORD 登录会话时要使用的密码。/OUTPUT 指定输出重定向模式。/APPEND 指定输出重定向模式。/AGGREGATE 设置或重置聚合模式。/AUTHORITY 指定连接的 &lt;授权类型&gt;。/?[:&lt;BRIEF|FULL&gt;] 用法信息。有关特定全局开关的详细信息，请键入: switch-name /?当前角色中可以使用以下别名:ALIAS - 对本地系统上可用别名的访问BASEBOARD - 基板(也称为主板或系统板)管理。BIOS - 基本输入/输出服务(BIOS)管理。BOOTCONFIG - 启动配置管理。CDROM - CD-ROM 管理。COMPUTERSYSTEM - 计算机系统管理。CPU - CPU 管理。CSPRODUCT - SMBIOS 中的计算机系统产品信息。DATAFILE - 数据文件管理。DCOMAPP - DCOM 应用程序管理。DESKTOP - 用户的桌面管理。DESKTOPMONITOR - 桌面监视器管理。DEVICEMEMORYADDRESS - 设备内存地址管理。DISKDRIVE - 物理磁盘驱动器管理。DISKQUOTA - 用于 NTFS 卷的磁盘空间使用量。DMACHANNEL - 直接内存访问(DMA)通道管理。ENVIRONMENT - 系统环境设置管理。FSDIR - 文件系统目录项管理。GROUP - 组帐户管理。IDECONTROLLER - IDE 控制器管理。IRQ - 中断请求线路(IRQ)管理。JOB - 提供对使用计划服务安排的作业的访问。LOADORDER - 定义执行依赖关系的系统服务的管理。LOGICALDISK - 本地存储设备管理。LOGON - 登录会话。MEMCACHE - 缓存内存管理。MEMORYCHIP - 内存芯片信息。MEMPHYSICAL - 计算机系统的物理内存管理。NETCLIENT - 网络客户端管理。NETLOGIN - 网络登录信息(属于特定用户)管理。NETPROTOCOL - 协议(及其网络特征)管理。NETUSE - 活动网络连接管理。NIC - 网络接口控制器(NIC)管理。NICCONFIG - 网络适配器管理。NTDOMAIN - NT 域管理。NTEVENT - NT 事件日志中的项目。NTEVENTLOG - NT 事件日志文件管理。ONBOARDDEVICE - 主板(系统板)中内置的通用适配器设备的管理。OS - 已安装操作系统的管理。PAGEFILE - 虚拟内存文件交换管理。PAGEFILESET - 页面文件设置管理。PARTITION - 物理磁盘的已分区区域的管理。PORT - I/O 端口管理。PORTCONNECTOR - 物理连接端口管理。PRINTER - 打印机设备管理。PRINTERCONFIG - 打印机设备配置管理。PRINTJOB - 打印作业管理。PROCESS - 进程管理。PRODUCT - 安装程序包任务管理。QFE - 快速修复工程。QUOTASETTING - 卷上的磁盘配额设置信息。RDACCOUNT - 远程桌面连接权限管理。RDNIC - 对特定网络适配器的远程桌面连接管理。RDPERMISSIONS - 特定远程桌面连接的权限。RDTOGGLE - 远程打开或关闭远程桌面侦听程序。RECOVEROS - 操作系统出现故障时将从内存收集的信息。REGISTRY - 计算机系统注册表管理。SCSICONTROLLER - SCSI 控制器管理。SERVER - 服务器信息管理。SERVICE - 服务应用程序管理。SHADOWCOPY - 卷影副本管理。SHADOWSTORAGE - 卷影副本存储区域管理。SHARE - 共享资源管理。SOFTWAREELEMENT - 系统上安装的软件产品元素的管理。SOFTWAREFEATURE - SoftwareElement 的软件产品子集的管理。SOUNDDEV - 声音设备管理。STARTUP - 当用户登录到计算机系统时自动运行的命令的管理。SYSACCOUNT - 系统帐户管理。SYSDRIVER - 基本服务的系统驱动程序管理。SYSTEMENCLOSURE - 物理系统外壳管理。SYSTEMSLOT - 物理连接点(包括端口、插槽和外设以及专用连接点)的管理。TAPEDRIVE - 磁带驱动器管理。TEMPERATURE - 温度传感器(电子温度计)数据管理。TIMEZONE - 时区数据管理。UPS - 不间断电源(UPS)管理。USERACCOUNT - 用户帐户管理。VOLTAGE - 电压传感器(电子电压表)数据管理。VOLUME - 本地存储卷管理。VOLUMEQUOTASETTING - 将磁盘配额设置与特定磁盘卷相关联。VOLUMEUSERQUOTA - 每用户存储卷配额管理。WMISET - WMI 服务操作参数管理。有关特定别名的详细信息，请键入: alias /?CLASS - 按 Esc 键可获取完整 WMI 架构。PATH - 按 Esc 键可获取完整 WMI 对象路径。CONTEXT - 显示所有全局开关的状态。QUIT/EXIT - 退出程序。有关 CLASS/PATH/CONTEXT 的详细信息，请键入: (CLASS | PATH | CONTEXT) /?]]></content>
      <categories>
        <category>操作系统</category>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac上如何快速截图]]></title>
    <url>%2FMac%E4%B8%8A%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E6%88%AA%E5%9B%BE.html</url>
    <content type="text"><![CDATA[在MacOS的系统自带截图工具在：所有程序 - 其他 - 屏幕快照 中。 打开后就可以选择 捕捉整个屏幕、捕捉部分、录屏等功能。 那么在实际使用中，这样截图太麻烦了，怎么快速截图呢。 方法一： 用系统自带的截图工具快捷键： shift + command + 3:全屏截图 shift + command + 4：部分截图 系统自带截图都是默认保存在桌面上。 方法二： 如果你有安装QQ的话，那么QQ也是有截图功能的，默认的快捷键如下： control + command + a：部分截图 QQ的截图默认是保存在剪切板的，不会再屏幕上显示。 PS：不用登陆QQ也可以使用QQ的截图功能，在聊天或者编辑文本的时候使用非常方便。]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取猫眼电影的评论]]></title>
    <url>%2F%E8%8E%B7%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E7%9A%84%E8%AF%84%E8%AE%BA.html</url>
    <content type="text"><![CDATA[背景最近几年猫眼电影越来越热门了，都差不多和豆瓣并驾齐驱了。今年的《流浪地球》这么火，看看猫眼电影上网友对该片的评价如何。 思路找到评论网页地址先打开猫眼官网找到《流浪地球》的介绍页面：https://maoyan.com/films/248906 虽然显示有112.4万人评分，但是页面只有热门短评，其他评论都去哪里了，手机明明是有的。 那么我们用chrome切换到手机页面： 打开开发者工具 开启手机浏览功能 访问手机版地址：http://m.maoyan.com/movie/248906?_v_=yes&amp;channelId=4&amp;$from=canary# 这时候我们就看到了所有的评论。 获取评论请求地址在点击打开“查看全部330613条讨论”后，发现评论分为最热和最新两部分，最热数量有限，而最新则是未经过处理的，也正是我们需要的。通过search来查看下对应的请求： 发现，在chrome 的网络展示中发现只有一个类型为document的请求包含了所需的信息。那么这部分的评论获取就需要解析网页了，我们再把屏幕上的评论往下拉，发现会自动加载更多的评论，对应的chrome网络请求多出来了两个comments.json的请求： 果然这才是我们需要的！把初始页面的url和这两个json请求的url复制到一起比较一下： 123http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=0&amp;limit=15&amp;ts=0&amp;type=3http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=15&amp;limit=15&amp;ts=1549965527295&amp;type=3http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=30&amp;limit=15&amp;ts=1549965527295&amp;type=3 我们可以发现规律： 初始页面的ts值为0，随后会有ts值，且保持不变。这里的ts是当前的时间戳，可以通过转换工具查看： offset是请求评论开始的序号，limit为请求的条数 再看返回的json结果： data.comments中是评论的具体内容 paging中通过hasMore来告诉我们是否还有更多（判断是否继续抓取） 我们再尝试下将offset设置为0，也加上ts参数： 1http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=0&amp;limit=15&amp;ts=1549965527295&amp;type=3 发现也是可以获取数据的： 那么通过offset和limit来控制每次请求获取的数量。 我们还可以通过加大limit参数来尝试，是否可以一次性获取更多的评论: 1http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=0&amp;limit=30&amp;ts=1549965527295&amp;type=3 效果如下: 再增加limit的值，会发现评论数回到了15条，可见猫眼系统仅支持每次最多获取30条。 构造请求url 方法一根据上面的分析，我们构造请求的url就很明确了： 从offset=0&amp;limit=30开始 通过返回的paging.hasMore来判断是否继续抓取 下一个抓取的url中offset+=limit 只能抓取1000条？！根据上述分析，在返回的json数据中是可以看到总评论数的，但是实际抓取的时候，在offset超过1000之后，返回的数据中hasMore就变成了false。 于是尝试通过浏览器一直下拉刷新，到达offset超过1000的情况，发现页面会不停的发送请求，但也无法获取数据。 那应该就是网站做了控制，不允许offset超过1000。 构造请求URL 方法二那么就要考虑其他构造url的方法来抓取了。先观察下每个请求返回的信息： 发现每个comment里都包含有一个time信息，把time做一下处理： 123456789102019-02-13 13:38:00##感觉韩朵朵这个人设是多余的2019-02-13 13:38:00##真的感动 非常棒2019-02-13 13:38:00##这电影大陆的起航2019-02-13 13:38:00##不怎么样，剧情挺感人，但是有点尴尬2019-02-13 13:37:00##好看。。。。。。。。。。2019-02-13 13:37:00##超级超级超级超级超级超级超级好看2019-02-13 13:37:00##太牛逼了，中国科幻片可有一部能看的了。支持吴京2019-02-13 13:36:00##不错！中国科幻的希望2019-02-13 13:36:00##中国里程碑式的科幻电影。2019-02-13 13:36:00##什么垃圾座位没人管的么乱坐的 可以发现后台是按照时间顺序的，每分钟一个间隔，那么就可以考虑根据每次返回comment中的时间来更新url中的ts即可。 由于不确定每次请求返回的数据中包含了多长的时间段，且返回的第一个评论时间戳与第二个评论是不同的，所以抓取思路如下： 获取请求数据 记录第一个时间戳 记录第二个时间戳 当遇到第三个时间戳时，将ts设置为第二个时间戳，重新构造url 如果单次抓取中每遇到第三个时间戳，则通过修改offset来继续抓取，直到遇到第三个时间戳 实现根据上面思路，实现相对就比较简单了： 生成url 12345def get_url(): global offset url = 'http://m.maoyan.com/review/v2/comments.json?movieId=' + movieId + '&amp;userId=-1&amp;offset=' + str( offset) + '&amp;limit=' + str(limit) + '&amp;ts=' + str(ts) + '&amp;type=3' return url 访问url 123456789101112def open_url(url): global ua try: headers = &#123;'User-Agent': ua.random&#125; response = requests.get(url, headers=headers) if response.status_code == 200: return response.text else: return None except Exception as e: print(e) return None 数据处理：将评论保存并判断是否要继续抓取 1234567891011121314151617181920212223242526272829def parse_json(data): global count global offset global limit global ts ts_duration = ts res = json.loads(data) comments = res['data']['comments'] for comment in comments: comment_time = comment['time'] if ts == 0: ts = comment_time ts_duration = comment_time if comment_time != ts and ts == ts_duration: ts_duration = comment_time if comment_time !=ts_duration: ts = ts_duration offset = 0 return get_url() else: content = comment['content'].strip().replace('\n', '。') print('get comment ' + str(count)) count += 1 write_txt(time.strftime("%Y-%m-%d %H:%M:%S",time.localtime(comment_time/1000)) + '##' + content + '\n') if res['paging']['hasMore']: offset += limit return get_url() else: return None 最后一共抓取评论131106条，足够做各种分析了，完整代码可见GitHub:https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/maoyan 123456789102019-02-13 18:13:10,962 - get_comments.py[line:78] - INFO: get comment 1310982019-02-13 18:13:11,079 - get_comments.py[line:78] - INFO: get comment 1310992019-02-13 18:13:11,189 - get_comments.py[line:78] - INFO: get comment 1311002019-02-13 18:13:11,307 - get_comments.py[line:78] - INFO: get comment 1311012019-02-13 18:13:11,411 - get_comments.py[line:78] - INFO: get comment 1311022019-02-13 18:13:11,527 - get_comments.py[line:78] - INFO: get comment 1311032019-02-13 18:13:11,625 - get_comments.py[line:78] - INFO: get comment 1311042019-02-13 18:13:11,729 - get_comments.py[line:78] - INFO: get comment 1311052019-02-13 18:13:11,827 - get_comments.py[line:78] - INFO: get comment 1311062019-02-13 18:13:15,416 - get_comments.py[line:98] - INFO: end 如果有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask在Windows环境下的部署]]></title>
    <url>%2FFlask%E5%9C%A8Windows%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E9%83%A8%E7%BD%B2.html</url>
    <content type="text"><![CDATA[背景由于目前在用的Flask项目涉及到一部分依赖Windows的处理，还无法迁移到linux平台，那么在windows环境下，要怎么部署呢？ 思路根据Flask官网介绍，由于Flask内置的服务器性能不佳，推荐的主要的部署方式有如下几种： mod_wsgi (Apache) 独立 WSGI 容器 Gunicorn Tornado Gevent uWSGI FastCGI CGI 上述这些部署方式，仅Tornado是支持在windows情况下部署的，配合上Nginx可以达到比较好的效果。可已参考Nginx与tornado框架的并发评测。 但是在实际使用中发现，tornado 的稳定性虽然很高，但是在tornado上部署Flask，并不会有异步的效果。实际上还是单进程阻塞运行的，即使在Flask中配置了threaded = True也无法实现多线程使用。 Flask多线程情况配置启用多线程： 1234# manage.pyfrom flask_script import Serverserver = Server(host="0.0.0.0", threaded=True) 在Flask中配置两条测试路由 123456789import time@main.route('/test')def maintest(): return 'hello world' @main.route('/sleep')def mainsleep(): time.sleep(60) return 'wake up' 先用浏览器访问\sleep： 随即立刻访问\test: 可见两次访问是不同的线程处理的，不会出现堵塞的情况。 tornado + Flask多线程情况使用tornado托管： 12345678from tornado.wsgi import WSGIContainerfrom tornado.httpserver import HTTPServerfrom tornado.ioloop import IOLoopfrom yourapplication import apphttp_server = HTTPServer(WSGIContainer(app))http_server.listen(5000)IOLoop.instance().start() 先用浏览器访问\sleep： 随即立刻访问\test: 可以发现，虽然tornado框架是支持异步的，但是由于实际上后台的处理是同步的，从而无法实现异步的处理的效果。如果想后台的处理也异步，则需要直接使用tornado来开发。 那么为什么使用tornado来托管flask呢？ Tornado 是一个开源的可伸缩的、非阻塞式的 web 服务器和工具集，它驱动了FriendFeed 。因为它使用了 epoll 模型且是非阻塞的，它可以处理数以千计的并发固定连接，这意味着它对实时 web 服务是理想的。把 Flask 集成这个服务是直截了当的 根据官网描述，其实也是为了弥足flask自带服务器不稳定的问题。 Flask高并发下的表现使用tsung进行压测，压力500： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 34.30 msec 31.91 msec 506 / sec 356.60 / sec 33.19 msec 103908 page 0.42 sec 0.29 sec 505 / sec 356.32 / sec 0.39 sec 103782 request 0.42 sec 0.29 sec 505 / sec 356.32 / sec 0.39 sec 103782 session 1mn 24sec 10.64 sec 11.4 / sec 1.21 / sec 14.24 sec 362 Code Highest Rate Mean Rate Total number 200 505 / sec 356.32 / sec 104792 Name Highest Rate Total number error_abort 0.5 / sec 1 error_abort_max_conn_retries 11.7 / sec 362 error_connect_econnrefused 58.6 / sec 1667 可见，在500的并发下，效果不佳，有很多的链接拒绝。 Flask + Nginx在高并发下的表现 使用tsung进行压测，压力500： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 30.95 msec 1810.5 / sec 626.43 / sec 0.11 sec 189853 page 0.68 sec 0.17 sec 1810.1 / sec 625.72 / sec 0.40 sec 189581 request 0.68 sec 0.17 sec 1810.1 / sec 625.72 / sec 0.40 sec 189581 Code Highest Rate Mean Rate Total number 200 906.4 / sec 196.08 / sec 60689 502 1443.9 / sec 430.02 / sec 129006 Name Highest Rate Total number error_abort 0.5 / sec 1 情况差不多，Flask服务器表现还算稳定，那么尝试增加后台Flask服务器数量（通过多端口实现）： 1234python manage.py runserver --port=8001python manage.py runserver --port=8002python manage.py runserver --port=8003python manage.py runserver --port=8004 使用tsung进行压测，压力500，4个Flask服务器： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.18 sec 32.57 msec 3510.1 / sec 639.92 / sec 0.11 sec 195154 page 0.49 sec 85.30 msec 3512.1 / sec 639.07 / sec 0.35 sec 194856 request 0.49 sec 85.30 msec 3512.1 / sec 639.07 / sec 0.35 sec 194856 Code Highest Rate Mean Rate Total number 200 3510.1 / sec 639.50 / sec 194986 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 这个效果妥妥的。 使用tsung进行压测，压力1000，4个Flask服务器： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 32.63 msec 2983.8 / sec 492.94 / sec 98.56 msec 150793 page 0.57 sec 90.00 msec 2976.4 / sec 491.31 / sec 0.40 sec 150275 request 0.57 sec 90.00 msec 2976.4 / sec 491.31 / sec 0.40 sec 150275 Code Highest Rate Mean Rate Total number 200 2981.4 / sec 488.92 / sec 149556 502 92.5 / sec 4.02 / sec 925 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 开始有一些502的超时错误了。 使用tsung进行压测，压力1000，4个tornado服务器： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.18 sec 86.24 msec 2052.1 / sec 693.82 / sec 0.14 sec 208786 page 0.52 sec 0.24 sec 2060.7 / sec 693.34 / sec 0.45 sec 208606 request 0.52 sec 0.24 sec 2060.7 / sec 693.34 / sec 0.45 sec 208606 Code Highest Rate Mean Rate Total number 200 2056.6 / sec 693.67 / sec 208703 在并发1000的情况下，是否使用tornado托管Flask效果差不多。 结论根据上述测试，直接使用Flask服务器的话，由于并发处理较弱，会有各种超时或者连接拒绝的错误。通过搭配Nginx来进行缓冲，通过增加后端服务器数来提供并发处理量。 所以最终选择了Nginx+后台4个Flask服务器的方式。由于目前Flask项目全体用户只有几千，目前并发情况很低，该方式完全满足使用。 如果在更大型项目中，并发上万，建议还是考虑想办法迁移至Liunx环境，通过官方建议的方式部署。]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>nginx</tag>
        <tag>tornado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下安装富士施乐打印机]]></title>
    <url>%2FMac%E4%B8%8B%E5%AE%89%E8%A3%85%E5%AF%8C%E5%A3%AB%E6%96%BD%E4%B9%90%E6%89%93%E5%8D%B0%E6%9C%BA.html</url>
    <content type="text"><![CDATA[背景打印机在windows环境下安装还是很方便的，在mac下，一路默认安装会有点问题，记录一下。 安装 下载打印机驱动：http://onlinesupport.fujixerox.com/setupSupport.do?cid=3&amp;ctry_code=CN&amp;lang_code=zh_CN 安装打印机驱动，一路默认就好了 打开Mac的 系统偏好设置，点击打印机与扫描仪 输入打印机的IP地址，然后点击添加，注意“协议” 要修改为“HP Jetdirect-Socket”。 点击添加即可 注意Mac在安装打印机时，默认的协议”互联网打印协议-IPP”，如下图所示。 按照默认的协议，也会自动识别到打印机型号，但是点击添加之后，会有报错： 如果继续添加，虽然可以添加成功，但实际打印时就会打印机无响应。]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下使用tree命令展示文件树]]></title>
    <url>%2FMac%E4%B8%8B%E4%BD%BF%E7%94%A8tree%E5%91%BD%E4%BB%A4%E5%B1%95%E7%A4%BA%E6%96%87%E4%BB%B6%E6%A0%91.html</url>
    <content type="text"><![CDATA[背景在写代码文档的时候，经常会用到展示项目架构，这时候如果可以有命令直接打印出目录树那就再好不过了，免的截图了。 思路网上找了下，果然是有这种工具的，Mac - tree命令。 Mac默认是没有tree命令的，需要手工安装下： 1brew install tree 安装好之后，看下帮助文档： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364$ tree --helpusage: tree [-acdfghilnpqrstuvxACDFJQNSUX] [-H baseHREF] [-T title ] [-L level [-R]] [-P pattern] [-I pattern] [-o filename] [--version] [--help] [--inodes] [--device] [--noreport] [--nolinks] [--dirsfirst] [--charset charset] [--filelimit[=]#] [--si] [--timefmt[=]&lt;f&gt;] [--sort[=]&lt;name&gt;] [--matchdirs] [--ignore-case] [--fromfile] [--] [&lt;directory list&gt;] ------- Listing options ------- -a All files are listed. -d List directories only. -l Follow symbolic links like directories. -f Print the full path prefix for each file. -x Stay on current filesystem only. -L level Descend only level directories deep. -R Rerun tree when max dir level reached. -P pattern List only those files that match the pattern given. -I pattern Do not list files that match the given pattern. --ignore-case Ignore case when pattern matching. --matchdirs Include directory names in -P pattern matching. --noreport Turn off file/directory count at end of tree listing. --charset X Use charset X for terminal/HTML and indentation line output. --filelimit # Do not descend dirs with more than # files in them. --timefmt &lt;f&gt; Print and format time according to the format &lt;f&gt;. -o filename Output to file instead of stdout. ------- File options ------- -q Print non-printable characters as '?'. -N Print non-printable characters as is. -Q Quote filenames with double quotes. -p Print the protections for each file. -u Displays file owner or UID number. -g Displays file group owner or GID number. -s Print the size in bytes of each file. -h Print the size in a more human readable way. --si Like -h, but use in SI units (powers of 1000). -D Print the date of last modification or (-c) status change. -F Appends '/', '=', '*', '@', '|' or '&gt;' as per ls -F. --inodes Print inode number of each file. --device Print device ID number to which each file belongs. ------- Sorting options ------- -v Sort files alphanumerically by version. -t Sort files by last modification time. -c Sort files by last status change time. -U Leave files unsorted. -r Reverse the order of the sort. --dirsfirst List directories before files (-U disables). --sort X Select sort: name,version,size,mtime,ctime. ------- Graphics options ------- -i Don't print indentation lines. -A Print ANSI lines graphic indentation lines. -S Print with CP437 (console) graphics indentation lines. -n Turn colorization off always (-C overrides). -C Turn colorization on always. ------- XML/HTML/JSON options ------- -X Prints out an XML representation of the tree. -J Prints out an JSON representation of the tree. -H baseHREF Prints out HTML format with baseHREF as top directory. -T string Replace the default HTML title and H1 header with string. --nolinks Turn off hyperlinks in HTML output. ------- Input options ------- --fromfile Reads paths from files (.=stdin) ------- Miscellaneous options ------- --version Print version and exit. --help Print usage and this help message and exit. -- Options processing terminator. 可以添加的参数很多，那么该用那些呢？ 在一个python项目中，先只加文件夹名看下： 1234567891011121314151617181920212223$ tree appapp├── __init__.py├── __pycache__│ └── __init__.cpython-37.pyc├── main│ ├── __init__.py│ ├── __pycache__│ │ ├── __init__.cpython-37.pyc│ │ ├── functions.cpython-37.pyc│ │ └── views.cpython-37.pyc│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-37.pyc │ ├── functions.cpython-37.pyc │ └── views.cpython-37.pyc ├── functions.py └── views.py5 directories, 14 files pyc是编译的临时文件，我们要把删掉，看下说明，可以用-I来： 12345678910111213141516$ tree -I *.pyc appapp├── __init__.py├── __pycache__├── main│ ├── __init__.py│ ├── __pycache__│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── __pycache__ ├── functions.py └── views.py5 directories, 7 files __pycache__也是临时文件，也把删掉： 12345678910111213tree -I *.pyc -I __pycache__ appapp├── __init__.py├── main│ ├── __init__.py│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── functions.py └── views.py2 directories, 7 files 可以看出-I是可以加多个的，每个-I后面加一个pattern。 在上面的例子中，其实所有的.pyc文件都在__pychache__文件夹下，可以直接忽略该文件夹即可： 12345678910111213$ tree -I __pycache__ appapp├── __init__.py├── main│ ├── __init__.py│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── functions.py └── views.py2 directories, 7 files 那么如果只要文件夹的结构呢？-d参数 123456789$ tree -d appapp├── __pycache__├── main│ └── __pycache__└── module └── __pycache__5 directories 忽略__pycache__文件夹： 123456$ tree -d -I __pycache__ appapp├── main└── module2 directories 总结通过brew安装tree工具之后，即可在命令行中使用tree命令展示文件\文件夹目录树： 直接加对应的文件夹来展示某文件夹范围内的文件树 1$ tree app 使用-I参数来忽略不展示的文件或子文件夹，可添加多个-I 1$ tree -I *.pyc -I __pycache__ app 使用-d来仅展示文件夹树 1$ tree -d app 多参数可以混合使用 1$ tree -d -I __pycache__ app 更多的参数使用，可以在有需要的时候参考--help内容 1$ tree --help]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何优雅的在flask中记录log]]></title>
    <url>%2F%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E5%9C%A8flask%E4%B8%AD%E8%AE%B0%E5%BD%95log.html</url>
    <content type="text"><![CDATA[背景记录日志，在任何项目中，都是很重要的。在Flask项目中，即有Flask提供的logger可以用来记录log，也可以通过直接使用Python的logging模块自定义logger来记录。那么这两者是什么关系，又该怎么使用呢？ 思路 Python的logging模块 先看下对于logging模块的官方介绍 Loggers have the following attributes and methods. Note that Loggers are never instantiated directly, but always through the module-level function logging.getLogger(name). Multiple calls to getLogger() with the same name will always return a reference to the same Logger object. The name is potentially a period-separated hierarchical value, like foo.bar.baz (though it could also be just plain foo, for example). Loggers that are further down in the hierarchical list are children of loggers higher up in the list. For example, given a logger with a name of foo, loggers with names of foo.bar, foo.bar.baz, and foo.bam are all descendants of foo. The logger name hierarchy is analogous to the Python package hierarchy, and identical to it if you organise your loggers on a per-module basis using the recommended construction logging.getLogger(__name__). That’s because in a module, __name__ is the module’s name in the Python package namespace. https://docs.python.org/3/library/logging.html#logger-objects 上面主要告诉我们两点， 可以通过logging.getLogger(name)来获取一个logger，相同名字的logger，其实是同一个logger。 logger是通过name进行继承的，比如foo.bar就是foo 的子logger。就可以是实现我们通过配置一个rootLogger，然后直接使用rootLogger.sublogger来记录一下内容，而不需要单独再配置一遍。 当使用logging.getLogger(__name__)时，__name__就是这个模块所在的python package的namespace。 flask提供的logger 再看下flask中的logging模块： Flask uses standard Python logging. All Flask-related messages are logged under the &#39;flask&#39; logger namespace.Flask.logger returns the logger named &#39;flask.app&#39;, and can be used to log messages for your application. Depending on the situation, an extension may choose to log to app.logger or its own named logger. Consult each extension’s documentation for details. http://flask.pocoo.org/docs/1.0/logging/ 我们可以知道flask的logger就是一个标准的Python logging，它的命名是flask。我们既可以使用app.logger，也可以自己定义一个logger。 那么如何使用app.logger呢？ 有两种方式： 直接调用 12logger = logging.getLogger('flask.app')logger.info('flask.app') 使用Flask提供的接口 12from flask import current_appcurrent_app.logger.info('logged by current_app from main') 这里推荐还是使用第二种，current_app是一个单例，可以直接引用到app.logger。 通过修改app.logger的name，可以实现子logger的继承么？ 答案是否定的。 修改app.logger的name： 12# app/__init__.pyapp.logger.name = 'app' 然后在子模块中定义一个app.module的logger来记录： 123456789from flask import current_appimport logginglogger = logging.getLogger('app.module')@module.route('/test', methods=['GET'])def test(): logger.info('logged by app.module') current_app.logger.info('logged by current_app.logger') 输出结果： 12019-02-01 10:56:01,877 - Thread-2 - app - INFO - logged by current_app.logger 只有current_app.logger的输出。 修改app.logger的name是不是无效呢？ 我们把子模块中的logger的name修改为flask.app.module： 123456789from flask import current_appimport logginglogger = logging.getLogger('flask.app.module')@module.route('/test', methods=['GET'])def test(): logger.info('logged by flask.app.module') current_app.logger.info('logged by current_app.logger') 输出结果： 122019-02-01 11:00:10,944 - Thread-2 - flask.app.module - INFO - logged by flask.app.module2019-02-01 11:00:10,946 - Thread-2 - app - INFO - logged by current_app.logger 两个logger均输出了。 可见，通过修改app.logger.name可以在记录的时候显示为我们设置的名称，但实际上这个logger还是flask.app。 __name__的使用 在自定义logger的情况下，为了方便起见，我们可以利用__name__这个参数。 前面说到：当使用logging.getLogger(__name__)时，__name__就是这个模块所在的python package的namespace。 一般Flask的工厂模式结构如下： 12345678910app├── __init__.py├── main│ ├── __init__.py│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── functions.py └── views.py 那么我们在先在app.__init__中定义rootLogger，然后再在app.module.functions.py中定义子Logger，均使用logging.getLogger(__name__): 1234567891011121314# app.__init__.py 初始化rootloggerrootLogger = logging.getLogger(__name__) rootLogger.setLevel(logging.DEBUG) socketHandler = logging.handlers.SocketHandler('localhost',logging.handlers.DEFAULT_TCP_LOGGING_PORT) rootLogger.addHandler(socketHandler) rootLogger.setLevel(logging.DEBUG)# app.module.functions.pyimport logginglogger = logging.getLogger(__name__)def record_from_logging(): logger.info('logged by logging from __name__') 输出： 122019-02-01 12:18:34,743 - MainThread - app - INFO - register root logger by __name__2019-02-01 12:19:24,954 - Thread-4 - app.module.functions - INFO - logged by logging from __name__ 可以发现输出的logger.name就是所在的文件目录，logger之间的继承关系与整个程序包保持一致。 总结根据上面分析，那么怎么优雅的记录logger呢？ 如果没有对模块进行分logger记录要求的话。可以直接使用在程序初始化的时候配置app.logger（可以自行设置logger.name）。在模块中通过import current_app来记录： 123456789101112131415161718# app.__init__.pydef register_logging(app): app.logger.name = 'app' # logstash_handler stashHandler = logstash.LogstashHandler('app.config.get('ELK_HOST')', 'app.config.get('ELK_PORT')') app.logger.addHandler(stashHandler) # socket_handler socketHandler = logging.handlers.SocketHandler('localhost', logging.handlers.DEFAULT_TCP_LOGGING_PORT) app.logger.addHandler(socketHandler) # app.module.function.pyfrom flask import current_app@module.route('/test', methods=['GET'])def test(): current_app.logger.info('logging someting') return 'logged by current_app.logger' 输出效果： 122019-02-01 13:49:28,998 - Thread-2 - app - INFO - logged by current_app from main2019-02-01 13:49:38,346 - Thread-3 - app - INFO - logged by current_app of functions 注意: 对于current_app.logger的引用不能通过如下方式，会有RuntimeError的报错。 123456789101112from flask import current_applogger = current_app.logger## 异常 raise RuntimeError(_app_ctx_err_msg)RuntimeError: Working outside of application context.This typically means that you attempted to use functionality that neededto interface with the current application object in some way. To solvethis, set up an application context with app.app_context(). See thedocumentation for more information. 如果希望按自己的实际需求，对模块进行分logger记录要求的话。那么建议自己设置logger。 12345678910111213141516171819202122# app.__init__.pydef register_logging(): # set own root logger rootLogger = logging.getLogger(__name__) rootLogger.setLevel(logging.DEBUG) # socketHandler socketHandler = logging.handlers.SocketHandler('localhost',logging.handlers.DEFAULT_TCP_LOGGING_PORT) rootLogger.addHandler(socketHandler) # logstash_handler stashHandler = logstash.LogstashHandler('app.config.get('ELK_HOST')', 'app.config.get('ELK_PORT')') rootLogger.addHandler(stashHandler) rootLogger.setLevel(logging.DEBUG)# app.module.function.pyimport logginglogger = logging.getLogger(__name__)@module.route('/test', methods=['GET'])def test(): logger.info('logging someting') return 'logged by logging module' 输出效果： 122019-02-01 13:49:49,297 - Thread-5 - app.module.views - INFO - logged by flask.app.module2019-02-01 13:50:01,013 - Thread-7 - app.module.functions - INFO - logged by logging module of functions 完整代码可参考：https://github.com/keejo125/flask_logging_demo 注意关于python中logging的配置可参考官网： https://docs.python.org/3/library/logging.config.html?highlight=logging 在配置handler时，经常会希望日志可以按时间分割(TimedRotatingFileHandler)或者按大小分割(RotatingFileHandler). 但是在flask项目中，尤其开启多线程之后，在分割日志(doRollover())时会有文件读写的异常: 1WindowsError: [Error 32] 建议使用SocketHandler，将日志发送给单独的LogServer来进行二次处理。 简易的接收socketlog的LogServer可参考：https://github.com/keejo125/flask_logging_demo/blob/master/LogServer.py 或者现在流行的stashHandler，将日志发送给ELK来进行二次处理。]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>logging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL按时间统计数据]]></title>
    <url>%2FMySQL%E6%8C%89%E6%97%B6%E9%97%B4%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE.html</url>
    <content type="text"><![CDATA[背景在做数据库的统计时，经常会需要根据年、月、日来统计数据，然后配合echarts来制作可视化效果。 数据库：MySQL 思路 按照时间维度进行统计的前提是需要数据库中有保留时间信息，建议是使用MySQL自带的datetime类型来记录时间。 1`timestamp` datetime DEFAULT NULL, 在MySQL中对于时间日期的处理的函数主要是DATE_FORMAT(date,format)。可用的参数如下 格式 描述 %a 缩写星期名 %b 缩写月名 %c 月，数值 %D 带有英文前缀的月中的天 %d 月的天，数值(00-31) %e 月的天，数值(0-31) %f 微秒 %H 小时 (00-23) %h 小时 (01-12) %I 小时 (01-12) %i 分钟，数值(00-59) %j 年的天 (001-366) %k 小时 (0-23) %l 小时 (1-12) %M 月名 %m 月，数值(00-12) %p AM 或 PM %r 时间，12-小时（hh:mm:ss AM 或 PM） %S 秒(00-59) %s 秒(00-59) %T 时间, 24-小时 (hh:mm:ss) %U 周 (00-53) 星期日是一周的第一天 %u 周 (00-53) 星期一是一周的第一天 %V 周 (01-53) 星期日是一周的第一天，与 %X 使用 %v 周 (01-53) 星期一是一周的第一天，与 %x 使用 %W 星期名 %w 周的天 （0=星期日, 6=星期六） %X 年，其中的星期日是周的第一天，4 位，与 %V 使用 %x 年，其中的星期一是周的第一天，4 位，与 %v 使用 %Y 年，4 位 %y 年，2 位 注：当涉及到按日统计是，需要使用%j，而如果使用%d, %e, %w的话，那么不同月份/周里的相同值会统计在一起。 涉及到获取当前时间，则可以通过now()或者sysdate()来获取。 12SELECT SYSDATE() FROM DUAL;SELECT NOW() FROM DUAL; 按照实际需求使用group by查询即可。 结论需统计的表结构如下： 123456789CREATE TABLE `apilog` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(64) DEFAULT NULL, `action` varchar(64) DEFAULT NULL, `params` text, `result` text, `timestamp` datetime DEFAULT NULL, PRIMARY KEY (`id`)) 统计时间范围内不同分类action的数量 12345678# 当日SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%j&apos;) = DATE_FORMAT(now(),&apos;%j&apos;) ORDER BY count desc;# 当周SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%u&apos;) = DATE_FORMAT(now(),&apos;%u&apos;) ORDER BY count desc;# 当月SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%m&apos;) = DATE_FORMAT(now(),&apos;%m&apos;) ORDER BY count desc;# 当年SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%Y&apos;) = DATE_FORMAT(now(),&apos;%Y&apos;) ORDER BY count desc; 统计某分类action的时间维度数量 12345678# 按日SELECT action, DATE_FORMAT(`timestamp`,&apos;%j&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%j&apos;)# 按周SELECT action, DATE_FORMAT(`timestamp`,&apos;%u&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%u&apos;)# 按月SELECT action, DATE_FORMAT(`timestamp`,&apos;%m&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%m&apos;)# 按年SELECT action, DATE_FORMAT(`timestamp`,&apos;%Y&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%Y&apos;) 同时按action和时间维度统计 12345678# 按日SELECT action, DATE_FORMAT(`timestamp`,&apos;%j&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%j&apos;)# 按周SELECT action, DATE_FORMAT(`timestamp`,&apos;%u&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%u&apos;)# 按月SELECT action, DATE_FORMAT(`timestamp`,&apos;%m&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%m&apos;)# 按年SELECT action, DATE_FORMAT(`timestamp`,&apos;%Y&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%Y&apos;) 拓展YEARWEEK()函数的使用 查询本周 1SELECT action, COUNT(id) count FROM apilog WHERE YEARWEEK(`timestamp`) = YEARWEEK(now()) GROUP BY action 查询上周 1SELECT action, COUNT(id) count FROM apilog WHERE YEARWEEK(`timestamp`) = YEARWEEK(now()) - 1 GROUP BY action 以上就是比较常用的时间统计了，更多的时间维度，可以参考上面的参数表类似处理即可。]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的KeyError异常处理]]></title>
    <url>%2FPython%E4%B8%AD%E7%9A%84KeyError%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86.html</url>
    <content type="text"><![CDATA[背景在检查web服务器日志的时候，发现有KeyError的异常报错。检查了下出错的代码： 1applyId = session['applyId'] 应该是用户首次访问时，session为空，所以就获取异常了。 思路根据上面的情况，KeyError就是在获取dict中不存在的key值时触发的。那么有解决方案就有两种： 在读取key值的时候，先校验一下是否存在改key： 1234567d = &#123;'a': 1, 'b': 2, 'c': 3&#125;if 'd' in d: print(d['d'])else: print('not exists!') 输出： 1not exists! 在读取dict时，设置default值。这时候需要使用dict内置的get(key[,default])方法： 1234d = &#123;'a': 1, 'b': 2, 'c': 3&#125;print(d.get('d', 'not exits!')) 输出： 1not exists! 结论最后采用了方法二，如果没有该key，就设置默认为空&#39;&#39;。 1applyId = session.get(&apos;applyId&apos;, &apos;&apos;) 后续再涉及到dict的时候都要注意下默认值的处理，避免出现KeyError的方法还有其他几种方法，可以参考如下的链接，会比较麻烦一点： https://blog.csdn.net/u011089523/article/details/72887163/]]></content>
      <categories>
        <category>python</category>
        <category>异常处理</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests发请求时timeout配置及异常捕获]]></title>
    <url>%2Frequests%E5%8F%91%E8%AF%B7%E6%B1%82%E6%97%B6timeout%E9%85%8D%E7%BD%AE%E5%8F%8A%E5%BC%82%E5%B8%B8%E6%8D%95%E8%8E%B7.html</url>
    <content type="text"><![CDATA[背景今天有用户在访问web系统时，出现了Nginx返回的超时报错。经排查是由于某台服务器异常，导致web系统requests请求时，一直等待响应，等待时间超过了Nginx配置的超时时间，所以Nginx就直接返回了。 思路 配置timeout 一般在使用requests库时，是不设置超时时间的，那么请求就会一直等待，直到某一方关闭。所以在发请求的时候手工指定下timeout参数。 requests的timeout配置如下： 12345678def request(method, url, **kwargs): """Constructs and sends a :class:`Request &lt;Request&gt;`.... :param timeout: (optional) How many seconds to wait for the server to send data before giving up, as a float, or a :ref:`(connect timeout, read timeout) &lt;timeouts&gt;` tuple. :type timeout: float or tuple... 官网介绍如下： 如果你制订了一个单一的值作为timeout，如下所示： 1r = requests.get('https://github.com', timeout=5) 这一timeout值将会用作 connect和 read二者的 timeout。如果要分别制定，就传入一个元组： 1r = requests.get('https://github.com', timeout=(3.05, 27)) 如果远端服务器很慢，你可以让 Request 永远等待，传入一个None作为timeout值，然后就冲咖啡去吧。 1r = requests.get('https://github.com', timeout=None) http://docs.python-requests.org/zh_CN/latest/user/advanced.html#timeout 捕获异常 通过try... except...可以捕获异常。获取的异常信息如下： 在老版本的python中，有e.message可以获取str格式的报错信息，但要注意的是，虽然在定义中e.message是str类型的，但是实际上未必。类型错误的话，在后续的处理中就可能会造成其他异常了。 1234567891011121314151617181920# e.message 定义class BaseException(object):... args = property(lambda self: tuple()) """:type: tuple""" message = property(lambda self: '', lambda self, v: None, lambda self: None) """:type: string"""...# 输入try: requests.get('http://122.248.19.3', timeout=0.001)except requests.exceptions.ReadTimeout as e: print(e.message) print(type(e.message))# 输出HTTPConnectionPool(host='122.248.19.3', port=80): Read timed out. (read timeout=0.001)&lt;class 'requests.packages.urllib3.exceptions.ReadTimeoutError'&gt; 而在新版本的python中，e.message已经被淘汰了。 1234567891011121314151617181920212223242526272829303132333435363738394041class BaseException(object): """Superclass representing the base of the exception hierarchy. The __getitem__ method is provided for backwards-compatibility and will be deprecated at some point. The 'message' attribute is also deprecated. """ def __init__(self, *args): self.args = args def __str__(self): return str(self.args[0] if len(self.args) &lt;= 1 else self.args) def __repr__(self): func_args = repr(self.args) if self.args else "()" return self.__class__.__name__ + func_args def __getitem__(self, index): """Index into arguments passed in during instantiation. Provided for backwards-compatibility and will be deprecated. """ return self.args[index] def _get_message(self): """Method for 'message' property.""" warnings.warn("the 'message' attribute has been deprecated " "since Python 2.6") return self.args[0] if len(args) == 1 else '' message = property(_get_message, doc="access the 'message' attribute; " "deprecated and provided only for " "backwards-compatibility") https://www.python.org/dev/peps/pep-0352/#transition-plan 所以可以用e.args来获取异常说明，e.args返回的是一个tuple，直接通过str(e.args)转换成str类型做后续处理。 结论代码如下，对不通原因造成的异常，可以加不同的except。 12345678910import requeststry: r = requests.get('https://github.com', timeout=5)except requests.exceptions.ConnectTimeout as e: msg = str(e.args) # do sth with msgexcept requests.exceptions.ReadTimeout as e: msg = str(e.args) # do sth with msg]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取知乎种某问题的所有答案]]></title>
    <url>%2F%E8%8E%B7%E5%8F%96%E7%9F%A5%E4%B9%8E%E7%A7%8D%E6%9F%90%E9%97%AE%E9%A2%98%E7%9A%84%E6%89%80%E6%9C%89%E7%AD%94%E6%A1%88.html</url>
    <content type="text"><![CDATA[背景知乎是一个比较出名也很有趣的网站，里面很多问题和回答也很有意思。之前看了一些爬虫相关文章经常会以抓取知乎来做一些分析。本次也尝试使用python抓取知乎某问题的全部答案。 思路使用爬虫抓取数据其实主要还是要先弄清楚网页展示的方式，现在大部分网页是基于模板动态生成，具体数据通过json等方式传递，这样的话，其实我们只需要直接通过请求抓取json部分即可，而不需要通过获取整个html然后在分析抓取需要部分。 先随便打开一个知乎首页的问题，比如： 通过在chrome里F12查看网络加载可以发现，有一个answer?相关的请求，在右边的Preview里看下返回的数据，果然答案就在里面。 再看仔细分析下返回的这个json结构： 每个json数据中data字段都含有多个答案体 每个答案体里，具体内容存在于content字段，而该字段是一段html，需要使用BeautifulSoup来解析。 每个json数据中paging字段包含了返回的数据是否是这个问题的起止答案，并且给除了上一批答案的请求地址以及下一批答案的请求地址，和总答案数。 于是可以通过随便获取一个请求地址，然后在data字段中获取答案，并根据paging字段来迭代获取其他的答案。 具体实现根据上述的思路，那么实现起来就比较简单了 先获取手动获取感兴趣的问题的网址 在chrome的F12中找到对应的答案请求地址（开头为https://www.zhihu.com/api/v4/questions/） 通过发起请求获取返回的json数据 在返回的json数据中，遍历data字段，并使用·BeautifulSoup来解析出答案 在返回的json数据中，读取paging字段，获取上一个请求地址或者下一个请求地址 迭代请求，直到结束 具体的实现，可以参考我的github，https://github.com/keejo125/。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式中(pattern)和(?:pattern)的使用]]></title>
    <url>%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B8%AD-pattern-%E5%92%8C-pattern-%E7%9A%84%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[背景在项目中有这样一个需求，在页面提交时，需要验证用户输入的网络端口地址是否符合要求。合法的规则如下： 数字端口：123 udp端口：123udp 范围端口：123-234 udp范围端口：123-234udp 多端口使用;分割：123;123udp;123-234;123-234udp 思路 1、单个端口匹配： 数字端口：\d+ udp端口：\d+udp 范围端口：\d+\-\d+ udp范围端口：\d+\-\d+udp 2、多端口匹配 先使用(pattern)来匹配单个端口，然后(pattern)+就可以来匹配多端口的情况了。由于多端口使用;分割，那么每个pattern的开头有两种情况：字符串起始或者;。写成正则就是(^|;)。 将第一步中4中单端口情况使用|并列起来，就有了如下的正则： 1(^|;)(\d+|\d+udp|\d+\-\d+|\d+\-\d+udp) 多端口： 1((^|;)(\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+ 匹配到结尾： 1((^|;)(\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+$ 3、(?:)的使用 在正则中，通过增加?:，使(pattern)变成(?:pattern)，可以实现匹配效果不变，但是不捕获匹配到的内容，从而提升代码的效率。那么上述的正则就变成了： 1(?:(?:^|;)(?:\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+$ 结论在页面中，使用javascript就是： 123var pattern = /^(?:(?:^|;)(?:\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+$/str = '123;123udp;123-234;123-234udp'console.log(pattern.test(str)) 拓展在正则中(pattern)和(?:pattern)的描述如下表，在仅进行规则匹配而不需要获取匹配到的内容的时候，建议使用(?:pattern)。 字符 描述 (pattern) 匹配 pattern 并获取这一匹配。所获取的匹配可以从产生的 Matches 集合得到，在VBScript 中使用 SubMatches 集合，在JScript 中则使用 0…0…9 属性。 (?:pattern) 匹配 pattern 但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用 “或” 字符 (&#124;) 来组合一个模式的各个部分是很有用。例如， industr(?:y&#124;ies) 就是一个比 ‘industry&#124;industries’ 更简略的表达式。 还有其他更多的符号含义可参考： https://www.cnblogs.com/richiewlq/p/7308005.html]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批量获取AD计算机名信息]]></title>
    <url>%2F%E6%89%B9%E9%87%8F%E8%8E%B7%E5%8F%96AD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%90%8D%E4%BF%A1%E6%81%AF.html</url>
    <content type="text"><![CDATA[背景由于用户加域时需要制定计算机名，为了规范起见，计算机名与AD账号有严格的对应关系。对于一些公共账号来说，就会有很多计算机名。现在需要根据该计算机名的登录时间来筛选出一些废弃计算机名，然后做删除处理，以释放计算机名。 思路在AD管理工具（Active Directory 用户与计算机）中是可以查询计算机名的，并且在计算机的属性中可以查看创建时间和修改时间的。 那么用命令应该就可以批量获取了。AD获取信息的命令为dsget，通过\h获取帮助。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&gt;dsget \h说明: 该工具的命令显示目录中具体对象选择的属性。dsget 命令:dsget computer - 显示目录中计算机的属性。dsget contact - 显示目录中联系人的属性。dsget subnet - 显示目录中子网的属性。dsget group - 显示目录中组的属性。dsget ou - 显示目录中组织单位的属性。dsget server - 显示目录中服务器的属性。dsget site - 显示目录中站点的属性。dsget user - 显示目录中用户的属性。dsget quota - 显示目录中配额的属性。dsget partition - 显示目录中分区的属性。要显示目录中所给对象属性的任意集，请使用 dsquery * 命令 (参见以下示例)。要获取一个具体的命令，请键入 "dsget &lt;ObjectType&gt; /?"，这里的 &lt;ObjectType&gt;是以上显示的一个受支持的对象类型。例如，dsget ou /?。备注:dsget 命令有助于查看目录中特定对象的属性:dsget 的输入是一个对象，输出则是该对象的一系列属性。若要查找满足给定搜索标准的所有对象，请使用 dsquery 命令(dsquery /?)。dsget 命令支持输入管道，以允许您通过管道输入 dsquery 命令的结果，作为 dsget 命令的输入，然后显示 dsquery 命令所找到对象的详细信息。可分辨名称中不用作分隔符的逗号必须用反斜杠("\")字符转义(例如，"CN=Company\, Inc.,CN=Users,DC=microsoft,DC=com")。用在可分辨名称中的反斜杠必须用一个反斜杠转义(例如，"CN=Sales\\ Latin America,OU=Distribution Lists,DC=microsoft,DC=com")。示例:查找姓名以 "John" 开始的所有用户并显示他们的办公室号码: dsquery user -name John* | dsget user -office显示对象的 sAMAccountName、userPrincipalName 和 department 属性，该对象的 DN 是 ou=Test,dc=microsoft,dc=com: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr sAMAccountName userPrincipalName department读取使用 dsquery * 命令的任何对象的所有属性。例如，读取其 DN 为 ou=Test,dc=microsoft,dc=com的对象的所有属性: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr *目录服务命令行工具可帮助:dsadd /? - 帮助添加对象。dsget /? - 帮助显示对象。dsmod /? - 帮助修改对象。dsmove /? - 帮助移动对象。dsquery /? - 帮助查找匹配搜索标准的对象。dsrm /? - 帮助删除对象。dsget 成功 这里我们需要获取计算机名的信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133&gt;dsget computer /?描述: 显示目录中计算机的属性。此命令有两种用法。第一种用法允许您 查看多个计算机的属性。第二种用法允许您查看一个计算机成员身 份的信息。语法: dsget computer &lt;ComputerDN ...&gt; [-dn] [-samid] [-sid] [-desc] [-loc] [-disabled] [&#123;-s &lt;Server&gt; | -d &lt;Domain&gt;&#125;] [-u &lt;UserName&gt;] [-p &#123;&lt;Password&gt; | *&#125;] [-c] [-q] [-l] [&#123;-uc | -uco | -uci&#125;] [-part &lt;PartitionDN&gt; [-qlimit] [-qused]] dsget computer &lt;ComputerDN&gt; [-memberof [-expand]] [&#123;-s &lt;Server&gt; | -d &lt;Domain&gt;&#125;] [-u &lt;UserName&gt;] [-p &#123;&lt;Password&gt; | *&#125;] [-c] [-q] [-l] [&#123;-uc | -uco | -uci&#125;]参数:值 描述&lt;ComputerDN ...&gt; 必需项/stdin。要查看的一台或多台计算机 的可分辨名称(DN)。 如果省略了目标对象，则会从标准 输入(stdin)中读取这些对象，以支持 通过管道将其他命令的输出 用作此命令的输入。 请与以下的 &lt;ComputerDN&gt; 相比。-dn 显示计算机 DN。-samid 显示计算机的 SAM 帐户名。-sid 显示计算机的安全 ID(SID)。-desc 显示计算机的描述。-loc 显示计算机的位置。-disabled 显示计算机帐户是(yes)否(no) 被禁用。&lt;ComputerDN&gt; 必需项。要查看计算机的 可分辨名称(DN)。-memberof 显示计算机所属的组。-expand 显示计算机所属组的循环 扩展列表。此选项采用 计算机直属组成员列表 并递归扩展该列表中 的每个组，以决定其组成员 身份和获得组的完整集。&#123;-s &lt;Server&gt; | -d &lt;Domain&gt;&#125; -s &lt;Server&gt; 用 &lt;Server&gt; 名称连接到 AD DC/LDS 实例。 -d &lt;Domain&gt; 连接到域 &lt;Domain&gt; 中的 AD DC。 默认: 登录域中的 AD DC。-u &lt;UserName&gt; 以 &lt;UserName&gt; 身份连接。默认: 登录的用户。 用户名可以采用: 用户名、域\用户名 或用户主体名称(UPN)。-p &#123;&lt;Password&gt; | *&#125; 用户 &lt;UserName&gt; 的密码。如果是 *， 则会提示您输入密码。-c 连续操作模式: 指定了多个目标对象时，将 报告错误，但继续处理参数列表中的 下一个对象。若无此选项，命令将在遇到 第一个错误时退出。-q 安静模式: 将所有输出抑制到标准输出。-L 以列表格式显示搜索结果集中的项目。 默认: 表格格式。&#123;-uc | -uco | -uci&#125; -uc 指定来字管道的输入或至管道的输出 用 Unicode 格式。 -uco 指定至管道或文件的输出 用 Unicode 格式。 -uci 指定来自管道或文件的输入 用 Unicode 格式。-part &lt;PartitionDN&gt; 用 &lt;PartitionDN&gt; 的可分辨名称 连接到目录分区。-qlimit 显示计算机在指定目录分区中 的有效配额。-qused 显示计算机在指定目录分区中的 已使用配额。备注:如果您在命令提示符处没有提供目标对象，则会从标准输入(stdin)中获取目标对象。可以通过键盘、重定向文件或另一个命令的管道输出接受 Stdin 数据。若要通过键盘或在重定向文件中标记 stdin 数据的结束，请使用 Control+Z 表示文件结束(EOF)。配额规定决定一个给定安全主体在一个特定目录分区中能够拥有的最大目录对象数。dsget 命令帮助您查看目录中某个特定对象的属性: dsget 的输入是一个对象，输出是该对象的属性列表。若要查找满足所给搜索条件的所有对象，请使用 dsquery 命令(dsquery /?)。如果您提供的值包含空格，请在文本两边使用引号(例如，"CN=DC2,OU=Domain Controllers,DC=microsoft,DC=com")。如果您输入了多个值，这些值必须用空格隔开(例如，一个系列可分辨名称)。示例:查找在给定 OU 中名称以 "tst" 开头的所有计算机并显示其说明。 dsquery computer ou=Test,dc=microsoft,dc=com -name tst* | dsget computer -desc显示给定计算机 "MyDBServer" 所属的组的列表(以递归方式展开): dsget computer cn=MyDBServer,cn=computers,dc=microsoft,dc=com -memberof -expand要显示给定计算机 "MyDBServer" 在给定分区 "cn=domain1,dc=microsoft,dc=com" 上的有效配额和已用配额，请键入: dsget computer cn=MyDBServer,cn=computers,dc=microsoft,dc=com -part cn=domain1,dc=microsoft,dc=com -qlimit -qused另请参阅:dsget - 描述适用于所有命令的参数。dsget computer - 显示目录中计算机的属性。dsget contact - 显示目录中联系人的属性。dsget subnet - 显示目录中子网的属性。dsget group - 显示目录中组的属性。dsget ou - 显示目录中组织单位的属性。dsget server - 显示目录中服务器的属性。dsget site - 显示目录中站点的属性。dsget user - 显示目录中用户的属性。dsget quota - 显示目录中配额的属性。dsget partition - 显示目录中分区的属性。目录服务命令行工具帮助:dsadd /? - 有关添加对象的帮助。dsget /? - 有关显示对象的帮助。dsmod /? - 有关修改对象的帮助。dsmove /? - 有关移动对象的帮助。dsquery /? - 有关查找符合搜索条件的对象的帮助。dsrm /? - 有关删除对象的帮助。dsget 成功 可以看到，能显示的计算机信息如下： 12345678910111213141516-dn 显示计算机 DN。-samid 显示计算机的 SAM 帐户名。-sid 显示计算机的安全 ID(SID)。-desc 显示计算机的描述。-loc 显示计算机的位置。-disabled 显示计算机帐户是(yes)否(no) 被禁用。&lt;ComputerDN&gt; 必需项。要查看计算机的 可分辨名称(DN)。-memberof 显示计算机所属的组。-expand 显示计算机所属组的循环 扩展列表。此选项采用 计算机直属组成员列表 并递归扩展该列表中 的每个组，以决定其组成员 身份和获得组的完整集。 那么修改日期的信息就只能通过AD工具来查看了。 再看下dsquery命令会不会有戏： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&gt;dsquery /?描述: 该工具的命令集允许您根据指定的标准查询目录。除 dsquery * 之外 (dsquery * 可以查询任何类型的对象)，以下每一个 dsquery 命令均可查找一个特定对象类型:dsquery computer - 查找目录中的计算机。dsquery contact - 查找目录中的联系人。dsquery subnet - 查找目录中的子网。dsquery group - 查找目录中的组。dsquery ou - 查找目录中的组织单位。dsquery site - 查找目录中的站点。dsquery server - 查找目录中的 AD DC/LDS 实例。dsquery user - 查找目录中的用户。dsquery quota - 查找目录中的配额规定。dsquery partition - 查找目录中的分区。dsquery * - 用通用的 LDAP 查询来查找目录中的任何对象。若要查找特定命令的帮助，请键入 "dsquery &lt;ObjectType&gt; /?"，其中&lt;ObjectType&gt; 是以上所示的受支持对象类型之一。例如，dsquery ou /?。备注:dsquery 命令帮助您查找目录中与指定搜索标准匹配的对象: dsquery 的输入是一个搜索标准，其输出是与该搜索匹配的一系列对象。若要获取特定对象的属性，请使用 dsget 命令(dsget /?)。可以将 dsquery 命令的结果通过管道输出，作为一个其他目录服务命令行工具(如 dsmod、dsget、dsrm 或 dsmove)的输入。可分辨名称中不是用作分隔符的逗号必须用反斜杠("\")字符转义(例如，"CN=Company\, Inc.,CN=Users,DC=microsoft,DC=com")。用在可分辨名称中的反斜杠必须用一个反斜杠转义(例如，"CN=Sales\\ Latin America,OU=Distribution Lists,DC=microsoft,DC=com")。示例:查找过去四个星期内处于非活动状态的计算机并将其从目录中删除: dsquery computer -inactive 4 | dsrm查找组织单位所有的用户 "ou=Marketing,dc=microsoft,dc=com" 并将他们添加到Marketing Staff 组: dsquery user ou=Marketing,dc=microsoft,dc=com | smod group "cn=Marketing Staff,ou=Marketing,dc=microsoft,dc=com" -addmbr查找姓名以 "John" 开始的所有用户并显示他的办公室号码: dsquery user -name John* | dsget user -office要显示目录中所给对象属性的任意集，请使用 dsquery * 命令。例如，要显示对象(该对象的 DN 是 ou=Test，dc=microsoft，dc=com) 的 sAMAccountName，userPrincipalName 和 department 属性: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr sAMAccountName userPrincipalName department要读取对象(该对象的 DN 是 ou=Test，dc=microsoft，dc=com) 的所有属性: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr *目录服务命令行工具帮助:dsadd /? - 添加对象的帮助。dsget /? - 显示对象的帮助。dsmod /? - 修改对象的帮助。dsmove /? - 移动对象的帮助。dsquery /? - 查找与搜索标准匹配对象的帮助。dsrm /? - 删除对象的帮助。 虽然没有直接我们想要的，但是其中一个例子却很有启发： 123查找过去四个星期内处于非活动状态的计算机并将其从目录中删除: dsquery computer -inactive 4 | dsrm 没办法通过命令来回去修改时间，却可以直接根据非活动状态的时间选出计算机名。 那么我们最终想要的效果——获取长时间未活动的计算机名，就可以直接得到了，而不需要另行计算了。 结论根据上面的思路，可以使用dsquery命令 -name test*来筛选包含test 所有计算机名 -inactive 24来筛选过去24周，也就是半年非活动的计算机名 完整命令如下： 1dsquery computer -name TEST* -inactive 24]]></content>
      <categories>
        <category>操作系统</category>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>ad</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery4.2在Python3.7下无法运行的问题]]></title>
    <url>%2FCelery4-2%E5%9C%A8Python3-7%E4%B8%8B%E6%97%A0%E6%B3%95%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景之前使用Flask + Celery + Redis来实现异步队列处理，使用的环境是python3.6，后来由于Mac系统下，使用brew安装的python，直接升级到了python3.7，相同的程序运行就报错了。 问题报错情况提示如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445[2019-01-08 23:15:05,188: CRITICAL/MainProcess] Unrecoverable error: SyntaxError('invalid syntax', ('/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/backends/redis.py', 22, 19, 'from . import async, base\n'))Traceback (most recent call last): File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/kombu/utils/objects.py", line 42, in __get__ return obj.__dict__[self.__name__]KeyError: 'backend'During handling of the above exception, another exception occurred:Traceback (most recent call last): File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/worker/worker.py", line 205, in start self.blueprint.start(self) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/bootsteps.py", line 115, in start self.on_start() File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/apps/worker.py", line 139, in on_start self.emit_banner() File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/apps/worker.py", line 154, in emit_banner ' \n', self.startup_info(artlines=not use_image))), File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/apps/worker.py", line 217, in startup_info results=self.app.backend.as_uri(), File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/kombu/utils/objects.py", line 44, in __get__ value = obj.__dict__[self.__name__] = self.__get(obj) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/base.py", line 1196, in backend return self._get_backend() File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/base.py", line 914, in _get_backend self.loader) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/backends.py", line 70, in by_url return by_name(backend, loader), url File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/backends.py", line 50, in by_name cls = symbol_by_name(backend, aliases) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/kombu/utils/imports.py", line 56, in symbol_by_name module = imp(module_name, package=package, **kwargs) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/importlib/__init__.py", line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) File "&lt;frozen importlib._bootstrap&gt;", line 1006, in _gcd_import File "&lt;frozen importlib._bootstrap&gt;", line 983, in _find_and_load File "&lt;frozen importlib._bootstrap&gt;", line 967, in _find_and_load_unlocked File "&lt;frozen importlib._bootstrap&gt;", line 677, in _load_unlocked File "&lt;frozen importlib._bootstrap_external&gt;", line 724, in exec_module File "&lt;frozen importlib._bootstrap_external&gt;", line 860, in get_code File "&lt;frozen importlib._bootstrap_external&gt;", line 791, in source_to_code File "&lt;frozen importlib._bootstrap&gt;", line 219, in _call_with_frames_removed File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/backends/redis.py", line 22 from . import async, base ^SyntaxError: invalid syntax 结论这个错误有点奇怪，经过一番百度谷歌，终于发现问题的原因。由于celery中有一个文件名命名为async，而在python3.7中，新增两个关键字，其中一个恰好就是async，另一个是await。 https://docs.python.org/3/whatsnew/3.7.html celery的作者也在issue中表示，后续版本会将async这个文件改为asynchronous，但目前版本还未发布（可能会在4.2.2）。现在可以通过github直接安装新版，pipenv环境下安装如下： 1234567pipenv install https://github.com/celery/celery/tarball/masterInstalling https://github.com/celery/celery/tarball/master…✔ Installation Succeeded Pipfile.lock (d4f0f1) out of date, updating to (dccb00)…Locking [dev-packages] dependencies…Locking [packages] dependencies…✔ Success! https://github.com/celery/celery/issues/4849 拓展flask + celery + redis 的单文件和工厂模式demo可参考如下git： https://github.com/keejo125/flask_celery_redis_demo]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[巧用kill重新加载配置并启动进程]]></title>
    <url>%2F%E5%B7%A7%E7%94%A8kill%E9%87%8D%E6%96%B0%E5%8A%A0%E8%BD%BD%E9%85%8D%E7%BD%AE%E5%B9%B6%E5%90%AF%E5%8A%A8%E8%BF%9B%E7%A8%8B.html</url>
    <content type="text"><![CDATA[背景前期在服务器上使用gunicorn托管了一个flask项目，近期修改了配置要重启一下。于是查了下如何优雅的重启进程。 思路修改配置重启进程，最简单的方法就是使用ps -ef|grep xxx命令来找到对应的进程，然后kill -9 pid来结束进程在重新启动。 对于gunicorn来说，稍微有点不同。一般会启动多个worker来跑，比如 1gunicorn -w 4 manager:app 这样的话，使用平常的ps -ef|grep gunicorn就会发现有多个进程，有时候直接kill后还会自动启动。 正确的方法是： 通过pstree来找到gunicorn 的主进程： 1234567$ pstree -ap|grep gunicorn | |-grep,18737 --color=auto gunicorn | `-gunicorn,18205/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | |-gunicorn,17455/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | |-gunicorn,17456/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | |-gunicorn,17457/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | `-gunicorn,17458/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi 这里就会看到，主进程是18737 然后在通过kill命令来结束进程 1kill -9 18737 在启动gunicorn。 这时候发现了有另外一个kill命令，可以直接让进程重新加载配置并启动，这就是 1kill -HUP 18737 重启之后再次使用pstree会发现guncorn的进程号发生了变化，即原进程已经销毁，新建了新的进程。 http://www.chenxm.cc/article/561.html 结论经查询，这其实是liunx中带信号的kill命令起的作用。上述命令其实是对进程发送了一个HUP信号，而很多程序会把HUP信号作为重新读取配置文件的触发条件，当接收到这个信号的时候，不会杀死进程，而是重新读取配置并运行。 1234567891011121314# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX 通过上述命令可以看到SIGHUP是1号信号，也就是说kill -1 pid和kill -HUP pid是等效的。 在logstash中，我们使用kill -1 pid 来实现重新加载配置，其实也是这个道理。]]></content>
      <categories>
        <category>操作系统</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>gunicorn</tag>
        <tag>logstash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript实现网页倒计时并跳转页面]]></title>
    <url>%2FJavaScript%E5%AE%9E%E7%8E%B0%E7%BD%91%E9%A1%B5%E5%80%92%E8%AE%A1%E6%97%B6%E5%B9%B6%E8%B7%B3%E8%BD%AC%E9%A1%B5%E9%9D%A2.html</url>
    <content type="text"><![CDATA[背景之前想自己从头搭建一个个人博客，后来各种原因直接用了hexo并托管在coding上，效果也不错。于是打算在个人服务器上直接建一个倒计时跳转页面，转到hexo， 也不浪费自己买的域名，哈哈哈。本来想直接跳转，感觉还是有一个倒计时提醒比较好一点。 思路 首先先写一个html5的简单的文字页面就好，留出一个div放倒计时的数字就好了。 然后JavaScript中可以用setInterval来实现按周期调用函数功能，也就是倒计时，每过一秒，数字减1。 setInterval()方法是按照指定的周期（毫秒）来调用函数或计算表达式。 12// 每三秒（3000 毫秒）弹出 "Hello" :setInterval(function()&#123; alert("Hello"); &#125;, 3000); http://www.runoob.com/jsref/met-win-setinterval.html 页面跳转就比较简单了location.href=就可以实现跳转了。 最后整个函数需要在页面加载完成之后进行，也就是放在window.onload里。 window.onload事件会在页面或者图像加载完成后立刻发生，通常用于&lt;body&gt;元素。 1234// html中&lt;body onload="SomeJavaScriptCode"&gt;// js中window.onload=function()&#123;SomeJavaScriptCode&#125;; http://www.runoob.com/jsref/event-onload.html 实现根据上面的思路，实现就比较简单了，在页面部分显示倒计时数字的div设置了id=&quot;time&quot;，在onload事件中，每隔1000毫秒调用一次函数使倒计时数字减1，当倒计时结束时跳转页面。 123456789101112131415161718192021222324252627282930313233343536&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;图兰登&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt; &lt;div style="text-align: center;"&gt; &lt;h1&gt;你好，这里是图兰登。&lt;/h1&gt; &lt;h3&gt;欢迎进入我的个人博客！&lt;/h3&gt; &lt;div style="text-align: center"&gt; &lt;div style="display: inline-block"&gt;倒计时：&lt;/div&gt; &lt;div id="time" style="display: inline-block"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;script&gt; window.onload = function () &#123; let oDiv = document.getElementById("time"); let count = 5; oDiv.innerHTML = count; let timer = null; timer = setInterval(function () &#123; if (count &gt; 0) &#123; count = count - 1; oDiv.innerHTML = count; &#125; else &#123; location.href='http://keejo.coding.me' &#125; &#125;, 1000); &#125;&lt;/script&gt;&lt;/html&gt; 最终效果可以见链接：https://www.torandom.com]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>html5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScrip实现Strip()功能]]></title>
    <url>%2FJavaScrip%E5%AE%9E%E7%8E%B0Strip-%E5%8A%9F%E8%83%BD.html</url>
    <content type="text"><![CDATA[背景之前用JS做了一个输入校验的功能，要求输入的必须是一个合法的email地址，结果在自己测试的时候发现输入不通过。检查发现，现在手机输入法（搜狗）联想输入的时候，默认会在输入的词语后面加上一个空格，所以导致校验的正则不通过。 思路Email校验的正则如下： 12let reg = new RegExp("^[a-z0-9]+([._\\-]*[a-z0-9])*@([a-z0-9]+[-a-z0-9]*[a-z0-9]+.)&#123;1,63&#125;[a-z0-9]+$")reg.test(email) 根据上述正则，是不允许字符串开头结尾有空格之类的多余字符的。那么解决这个问题就有两个方案： 修改正则，允许开头、结尾有空格等符号 对输入做处理，删除开头、结尾的空格等符号 这里我选了第二种，因为一直有在写python，其中有一个strip()函数，可以自动的删除字符串开头结尾的空格等多余符号的，所以理所当然认为javascript也有，结果发现竟然是没有的。 那么需要自己写一个了。 对字符串的处理，最简单粗暴的也就是正则了。 在正则中\s可以匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。注意 Unicode 正则表达式会匹配全角空格符。 http://www.runoob.com/regexp/regexp-syntax.html 先找从字符串开头起的\s : ^\s+ 再找字符串结尾的\s: \s+$ 将找到的字符串替换为空&#39;&#39; 结论根据上面的思路，表达式也就出来了： 1let email = email.replace(/^\s+|\s$/g, '') 其中/g是用于全局替换的，如果不加/g，那么当开头和结尾都有空格等符号时，仅会替换开头部分。 http://www.runoob.com/jsref/jsref-regexp-g.html]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序部署EACCES: permission denied问题]]></title>
    <url>%2F%E5%B0%8F%E7%A8%8B%E5%BA%8F%E9%83%A8%E7%BD%B2EACCES-permission-denied%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景开发工具： 微信开发者工具 腾讯云环境： node 问题之前做了一个账本excel导出功能，可见用exceljs实现Json对象导出excel。大体逻辑是在项目根目录生成一个临时的excel文件，然后通过邮件发送给用户之后，删除临时文件。 在本地测试通过之后，部署到腾讯云开发环境，点击导出按钮，就报错了。直接请求failed，看了下报错信息： 思路问题其实比较明确，没有权限。按照功能的实现思路，在生成excel文件时第一次打开，发送邮件时第二次用到这个文件。所以问题应该出在程序无法在根目录写入文件，生成excel文件。 注：后续和腾讯云确认，根目录的确是授权给单独一个用户用于部署程序，所以node运行用户无权限在根目录写入文件。 那么解决方法就只有两个 找一个有权限的目录 给node运行的用户授权 腾讯云的小程序服务器端部署是一套完整的自动化流程，个人无法直接访问服务器确认用户权限，所以在不更换部署服务的情况下，只能选择第一种。 解决由于使用的腾讯官方的wafer2框架，在demo中是有上传文件的案例的。 在wafer-node-sdk的node包中lib\upload\index.js中可以看到源代码： 1234567// 初始化 multiparty const form = new multiparty.Form(&#123; encoding: 'utf8', maxFilesSize: maxSize * 1024 * 1024, autoFiles: true, uploadDir: '/tmp' &#125;) demo中，文件是先上传至/tmp目录下，然后再转保存在cos中。由此可以确定/tmp目录是可以写入文件的。 将代码中临时生成的excel文件路径也放在/tmp目录下，果然问题解决了。 插曲在部署到生产环境时，先上传代码，然后点击“安装依赖”，最后点击“部署代码”。一路都显示成功，但结果却发现node环境挂了，直接用web访问生产环境提示bad getway。 心里一紧，再次点击“部署代码”，这次有报错出来了，提示是某依赖没有找到。 于是又点了一次安装依赖，待提示安装成功之后，再点部署代码，就ok了。 事后咨询了腾讯云的支持，理论上生产环境的部署是不需要手动安装依赖的。系统会自动从package.json中拉取依赖清单并安装。这次就不知道是什么情况，记录一下。]]></content>
      <categories>
        <category>微信小程序</category>
      </categories>
      <tags>
        <tag>微信小程序</tag>
        <tag>node</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用nodemailer实现邮件发送]]></title>
    <url>%2F%E7%94%A8nodemailer%E5%AE%9E%E7%8E%B0%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81.html</url>
    <content type="text"><![CDATA[背景由于随手记账本是基于小程序的，没办法直接通过浏览器下载的方式导出给用户。于是考虑在导出请求时，要求用户提供一个电子邮箱，后台生成导出的excel文件之后直接以附件的形式发送到用户邮箱中。目前是通过SendGrid提供的免费邮箱服务来实现邮箱发送。SendGrid也提供各个版本的webapi支持，不过考虑到后续兼容性，本次就摒弃了SendGrid提供的接口组件，使用nodemail。 思路发送邮件的方式基本大同小异，使用smtp协议的话需要知道： 邮箱服务器地址 端口号 用户名 密码 然后邮件内容上需要明确： 收件人 主题 邮件内容 是否为html格式 附件 发件人（如果通过个人邮箱开通smtp发件，那么可能不支持另外设置发件人。因为发件人就是自己，SendGrid是支持只是发件人的，比如`no-reply@torandom.com`。） 最后按照使用的邮件模块的说明发送即可。 实现nodemailer的文档不是太直观，说明的很详细，但是没有一个完整的简单例子。普通的单次邮件发送主要有这么几个步骤： 设置smtpConfig信息 设置message信息 创建transporter对象 调用transporter对象的sendMail()发送邮件并接受回调 代码如下： 12345678910111213141516171819202122232425262728293031323334353637const nodemailer = require('nodemailer')// 配置邮件服务器信息let smtpConfig = &#123; host: 'smtp.sendgrid.net', port: 587, secure: false, // upgrade later with STARTTLS auth: &#123; user: 'username', pass: 'password' &#125;&#125;// 配置邮件内容信息let message = &#123; from: 'no-reply@torandom.com', to: 'test@torandom.com', subject: '随手记账本 - 导出', text: '随手记账本导出测试', html: '&lt;p&gt;HTML version of the message&lt;/p&gt;', attachments: [ &#123; filename: 'package.json', path: './package.json' &#125; ]&#125;// 创建transporter对象let transporter = nodemailer.createTransport(smtpConfig)// 发送邮件transporter.sendMail(message) .then(info =&gt; &#123; if (info.accepted) &#123; console.log('已发送至' + info.accepted.toString()) &#125; else &#123; console.log('邮件发送失败') console.log(info) &#125; &#125;) 注意 在发送邮件部分。回调的info内容是由你的邮件服务器提供的，不同的邮件服务器提供的内容是不同的需要确认。 如果测试代码的时候，发送到自己的QQ邮箱。发送的数量太多了的话，会有如下的报错： 123456789101112550 Connection frequency limited出错原因：该服务器IP的发信频率超过腾讯邮箱限制。 腾讯邮箱对来自相同IP的外部发信服务器有一定的频率限制： 1、超过每分钟发信量限制，此IP地址被禁止发信若干分钟。 2、超过每小时发信量限制，此IP地址被禁止发信若干小时。 3、超过每日发信量限制，此IP地址本日内禁止再发信。 4、以上频率限制数值属于腾讯邮箱保密数据，恕不公开。 改善建议：如果您是该服务器IP的管理员，请暂停该服务器IP的发信，稍后降低频率重新尝试发信。 如果您是个人邮箱用户，请向您的电子邮件提供商报告此情况。 https://service.mail.qq.com/cgi-bin/help?subtype=1&amp;id=20022&amp;no=1000722 这时候需要把你的发件人加入到接受邮件的白名单中即可。 关于nodemailer的更多高级用法，比如邮件池之列的请参考如下官方文档。 https://nodemailer.com/about/]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>邮件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用exceljs实现Json对象导出excel]]></title>
    <url>%2F%E7%94%A8exceljs%E5%AE%9E%E7%8E%B0Json%E5%AF%B9%E8%B1%A1%E5%AF%BC%E5%87%BAexcel.html</url>
    <content type="text"><![CDATA[背景在做随手记账本项目的时候，很多网友在意见反馈中建议提供导出功能。由于小程序的后台是基于node的，于是在npm里找了下关于excel的包，也参考了百度建议，推荐比较多的是excelexport，但是我最后选了exceljs。主要是一直在持续更新，文档也很全面。 思路由于后台数据库保存的账本数据是采用json格式的，最简单的方法就是通过遍历账本中的所有条目逐行写入excel。但是发现在exceljs这个包中，是提供列定义的，通过定义每列的key，就可以直接把json数据写入了，这个功能非常赞。 这部分的文档说明如下： 123456789101112// Add column headers and define column keys and widths// Note: these column structures are a workbook-building convenience only,// apart from the column width, they will not be fully persisted.worksheet.columns = [ &#123; header: 'Id', key: 'id', width: 10 &#125;, &#123; header: 'Name', key: 'name', width: 32 &#125;, &#123; header: 'D.O.B.', key: 'DOB', width: 10, outlineLevel: 1 &#125;];// Add a couple of Rows by key-value, after the last current row, using the column keysworksheet.addRow(&#123;id: 1, name: 'John Doe', dob: new Date(1970,1,1)&#125;);worksheet.addRow(&#123;id: 2, name: 'Jane Doe', dob: new Date(1965,1,7)&#125;); 需要说明的说，文档里定义的column里的header，会自动写在表格的第一行当做表头，不需要另起一行。 实现主要有这么几个步骤： 新建workbook对象 新建sheet 定义column 写入row 导出 代码如下： 1234567891011121314151617181920212223242526272829303132333435const Excel = require('exceljs')// 测试数据let data = &#123; "subtitle":"偶遇美食节", "comment":"鲷鱼烧，烤鱼，年糕丸子。600+200+350", "cost":1150, "date":"2018-01-20", "time":"14:24", "member":1, "type":"餐饮", "currency":"日元", "location":"Ueno Park (上野恩賜公園)"&#125;// 新建workbook对象let workbook = new Excel.Workbook()// 设置workbook属性，比如作者workbook.creator = 'ToRandom'// 新建sheetlet tempWorksheet = workbook.addWorksheet('东京之旅')// 定义column, 日期比较长，设置为15 可以展示yyyy-mm-ddtempWorksheet.columns = [ &#123;header: '标题', key: 'subtitle'&#125;, &#123;header: '消费类型', key: 'type'&#125;, &#123;header: '评论', key: 'comment'&#125;, &#123;header: '币种', key: 'currency'&#125;, &#123;header: '费用', key: 'cost'&#125;, &#123;header: '日期', key: 'date', width: 15&#125;, &#123;header: '时间', key: 'time'&#125;, &#123;header: '人数', key: 'member'&#125;, &#123;header: '位置信息', key: 'location'&#125; ]// 写入tempWorksheet.addRow(data)// 保存文件，如有需要，前面加await等待执行workbook.xlsx.writeFile("随手记账本.xlsx) 注意 一般node默认是异步执行的，如有后续操作（比如先生成excel文件之后，以附件的形式发送），那么就需要在最后一步保存文件的代码前面加上await。 1await workbook.xlsx.writeFile("随手记账本.xlsx") 如果生成之后，处理完了要删除的话，可以使用fs.unlink来删除。但注意的是，这个unlink也是一个异步函数，删除效果会有延迟。 其他关于exceljs的操作，可以参考原文档，还是相当丰富的。 https://www.npmjs.com/package/exceljs]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>Json</tag>
        <tag>node</tag>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS下部署selenium环境]]></title>
    <url>%2FCentOS%E4%B8%8B%E9%83%A8%E7%BD%B2selenium%E7%8E%AF%E5%A2%83.html</url>
    <content type="text"><![CDATA[背景最近写了一个循环抓取某网站数据的代码，其中涉及到页面登陆，采用了selenium来做。考虑到循环抓取，本机跑容易因系统休眠断网造成爬取失败，于是在自己的服务器上部署一下。 操作系统：CentOS 7 Python版本：Python3.7 问题由于服务器抓取，其实不需要展示浏览器的界面，可以考虑使用PhantomJS来做，结果发现有如下的警告提示： 1UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead 既然后续不支持了，那么就按照官方建议，我选chrome。 不同于PhamtomJS，chromedriver需要和chrome配合使用，也就是如果不安装chrome，直接加载chromedriver那么就会有如下的报错： 123raise exception_class(message, screen, stacktrace)selenium.common.exceptions.WebDriverException: Message: unknown error: cannot find Chrome binary (Driver info: chromedriver=2.45.615279 (12b89733300bd268cff3b78fc76cb8f3a7cc44e5),platform=Linux 3.10.0-693.2.2.el7.x86_64 x86_64) 而且MacOS和CentOS的chromedriver是不同的。如果在linux下直接加载mac版的chromedriver就会如下的报错： 12raise child_exception_type(errno_num, err_msg, err_filename)OSError: [Errno 8] Exec format error: './chromedriver' 部署正确的部署应该是这样的： 安装chromeCentOS服务器是用ssh登陆的，就直接用yum来安装好了，由于存在翻墙的问题，要手动配置下源： 12cd /etc/yum.repos.d/vim google-chrome.repo 然后添加如下语句： 123456[google-chrome]name=google-chromebaseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearchenabled=1gpgcheck=1gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub 再用yum安装： 1yum -y install google-chrome-stable --nogpgcheck 在未翻墙的环境下，一定要加上--nogpgcheck选项，否则会因为检查失败而无法安装成功。 https://blog.csdn.net/u010472499/article/details/72327963 安装chromedriver墙内可以在如下地址下载对应系统的chromedriver: http://npm.taobao.org/mirrors/chromedriver/2.45/ 下载好的chromedriver需要放到PATH目录下，建议是自己使用virtualenv新建一个venv虚拟环境，然后将chromedriver放到虚拟环境的bin目录下（venv/bin）即可。 selenium的使用加上headless的配置即可，关键代码如下： 123456from selenium import webdriveroption = webdriver.ChromeOptions()option.add_argument('headless')driver = webdriver.Chrome(executable_path='./chromedriver', options=option)# do somethingdriver.quit()]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>selenium</tag>
        <tag>爬虫</tag>
        <tag>chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx与tornado框架的并发评测]]></title>
    <url>%2FNginx%E4%B8%8Etornado%E6%A1%86%E6%9E%B6%E7%9A%84%E5%B9%B6%E5%8F%91%E8%AF%84%E6%B5%8B.html</url>
    <content type="text"><![CDATA[背景分别测试在windows平台和linux平台(SuSE)下，tornado框架的并发效果，以及通过配置nginx对并发效果影响。 操作系统： windows: Windows Server 2008 SP2 （8C8G) linux: SuSE12 SP3 （8C8G) 并发测试工具：tsung 测试访问：仅返回”Hello World”字符 评测过程1、直接访问tornado，并发设置为500 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 36.11 msec 32.51 msec 620.3 / sec 138.53 / sec 33.18 msec 44464 page 0.11 sec 33.02 msec 1221.1 / sec 272.69 / sec 76.98 msec 87507 request 0.11 sec 33.02 msec 1221.1 / sec 272.69 / sec 76.98 msec 87507 session 1mn 56sec 8.66 sec 14.8 / sec 1.66 / sec 12.64 sec 498 Code Highest Rate Mean Rate Total number 200 616 / sec 138.70 / sec 44536 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 error_abort_max_conn_retries 4.7 / sec 104 error_abort_max_send_retries 13.9 / sec 394 error_connect_econnrefused 35.8 / sec 581 error_connection_closed 35.7 / sec 1023 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.21 sec 30.57 msec 760 / sec 127.01 / sec 34.44 msec 41788 page 0.21 sec 30.97 msec 1474.9 / sec 247.78 / sec 39.04 msec 81576 request 0.21 sec 30.97 msec 1474.9 / sec 247.78 / sec 39.04 msec 81576 session 2mn 43sec 3.33 sec 11.9 / sec 1.66 / sec 5.48 sec 498 Code Highest Rate Mean Rate Total number 200 762.6 / sec 127.15 / sec 41867 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 error_abort_max_send_retries 11.9 / sec 498 error_connection_closed 38.4 / sec 1500 说明 在500的并发量下，无论在windows还是liunx平台，均有较多的连接错误。但框架系统还是比较稳定的，没有出现崩溃等情况。 2 、通过Nginx代理访问，并发设置为500 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.18 sec 31.78 msec 2287.5 / sec 588.11 / sec 0.11 sec 179083 page 0.58 sec 0.17 sec 2283.4 / sec 587.32 / sec 0.36 sec 178778 request 0.58 sec 0.17 sec 2283.4 / sec 587.32 / sec 0.36 sec 178778 Code Highest Rate Mean Rate Total number 200 1736.4 / sec 307.54 / sec 94844 502 1122.9 / sec 280.16 / sec 84048 Name Highest Rate Total number error_abort 0.5 / sec 1 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.17 sec 32.16 msec 801 / sec 710.33 / sec 73.15 msec 216931 page 0.69 sec 0.16 sec 798 / sec 709.48 / sec 0.58 sec 216633 request 0.69 sec 0.16 sec 798 / sec 709.48 / sec 0.58 sec 216633 Code Highest Rate Mean Rate Total number 200 794.9 / sec 709.79 / sec 216726 504 0.7 / sec 0.04 / sec 11 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 说明 增加Nginx做反向代理之后，有效的提供了一定的缓冲。在windows平台下出现较多的code 502，说明后台没有及时返回，导致Nginx直接返回给压测工具code 502。在linux平台下就比较稳定了。 3、通过Nginx代理访问，后端设置4台服务器，并发设置为1000 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 35.78 msec 4122.2 / sec 508.31 / sec 0.12 sec 156715 page 0.48 sec 74.59 msec 4109.8 / sec 506.99 / sec 0.28 sec 156275 request 0.48 sec 74.59 msec 4109.8 / sec 506.99 / sec 0.28 sec 156275 Code Highest Rate Mean Rate Total number 200 4115.6 / sec 507.68 / sec 156481 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.21 sec 36.22 msec 4786.7 / sec 533.37 / sec 0.12 sec 163256 page 0.50 sec 73.71 msec 4784.9 / sec 532.57 / sec 0.29 sec 162988 request 0.50 sec 73.71 msec 4784.9 / sec 532.57 / sec 0.29 sec 162988 Code Highest Rate Mean Rate Total number 200 4792.2 / sec 533.28 / sec 163199 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 说明 由于后台扩充到了4台服务器，通过Nginx进行轮询访问，分散了压力。在1000的并发下，windows和suse平台表现不相上下，均无错误。 4、通过Nginx代理访问，后端设置4台服务器，并发设置为1500 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.19 sec 32.09 msec 2576 / sec 1075.47 / sec 0.10 sec 327195 page 0.50 sec 71.39 msec 2602.4 / sec 1073.57 / sec 0.31 sec 326577 request 0.50 sec 71.39 msec 2602.4 / sec 1073.57 / sec 0.31 sec 326577 Code Highest Rate Mean Rate Total number 200 2502.1 / sec 1001.18 / sec 304862 502 770.7 / sec 84.82 / sec 22052 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 33.41 msec 3275.7 / sec 990.22 / sec 0.11 sec 299354 page 0.48 sec 68.46 msec 3270.3 / sec 988.43 / sec 0.27 sec 298777 request 0.48 sec 68.46 msec 3270.3 / sec 988.43 / sec 0.27 sec 298777 Code Highest Rate Mean Rate Total number 200 3261.5 / sec 989.09 / sec 298975 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 说明 当并发增加到1500时，windows平台出现code 502，后端服务器出现瓶颈。linux平台表现稳定。 5、通过Nginx代理访问，后端设置4台服务器，并发设置为2000 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 19.30 sec 36.06 msec 2187.66666666667 / sec 611.56 / sec 0.48 sec 166002 page 19.36 sec 0.14 sec 2197 / sec 611.42 / sec 0.57 sec 165828 request 19.36 sec 0.14 sec 2197 / sec 611.42 / sec 0.57 sec 165828 Code Highest Rate Mean Rate Total number 200 2194.66666666667 / sec 611.54 / sec 172473 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 error_connect_etimedout 17.1 / sec 318 error_next_session 0.285714285714286 / sec 2 说明 在2000并发先，linux平台后端依旧稳定返回code 200，但是Nginx会直接返回error，瓶颈出现在Nginx，需要调整相关配置了。 结论这次测试中可以发现，当仅返回字符串Hello World时，无论是windows平台还是liunx平台，在并发500的情况下虽然框架可以稳定输出，但是会出现不同程度的系统处理不过来直接拒绝请求的情况。 通过增加Nginx，可以有效的为后端提供缓冲，同样500的并发下，liunx平台返回给Nginx的错误code 504要明显比windows平台code 502少很多。 通过增加后台服务器，使用Nginx进行轮询，可以增加并发，在后端4台服务器，1000的并发下，linux平台和windows平台表现不相上下。但并发增加到1500之后，windows平台开始出现大量的code 502错误，linux平台依旧稳定。把并发继续增加到2000，Nginx端出现瓶颈，返回连接错误，后端linux保持稳定。可考虑下通过调整Nginx配置或者增加Nginx来继续提升并发效果。 不过，从上述测试情况来看，torando框架还是很稳定的，不至于并高并发弄到崩溃的程度。]]></content>
      <categories>
        <category>中间件</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>SuSE</tag>
        <tag>高并发</tag>
        <tag>torando</tag>
        <tag>Nginx</tag>
        <tag>tsung</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SuSE缺失devel包的问题]]></title>
    <url>%2FSuSE%E7%BC%BA%E5%A4%B1devel%E5%8C%85%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景最近几天计划将原Python项目迁移到Liunx服务器上，操作系统是SuSE 12 SP3。原以为Python项目迁移会比较方面，使用pip安装requirements包就好了，结果遇到不少问题。 问题 安装mysqlclient包时，出现了如下报错： 12345678910creating build/temp.linux-x86_64-2.7gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Dversion_info=(1,3,9,'final',1) -D__version__=1.3.9 -I/usr/include/mysql -I/usr/local/python/include/python2.7 -c _mysql.c -o build/temp.linux-x86_64-2.7/_mysql.o -m64 _mysql.c:29:23: fatal error: my_config.h: No such file or directory #include "my_config.h" ^ compilation terminated. error: command 'gcc' failed with exit status 1 ----------------------------------------Command "/home/sysop/webapp/hzinfo/venv/bin/python -u -c "import setuptools, tokenize;__file__='/tmp/pip-install-4FRt1w/mysqlclient/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))" install --record /tmp/pip-record-AgHBn8/install-record.txt --single-version-externally-managed --compile --install-headers /home/sysop/webapp/hzinfo/venv/include/site/python2.7/mysqlclient" failed with error code 1 in /tmp/pip-install-4FRt1w/mysqlclient/ 安装ldap库时，出现了如下报错： 123456In file included from Modules/LDAPObject.c:9:0:Modules/errors.h:8:18: fatal error: lber.h: No such file or directory #include "lber.h" ^compilation terminated.error: command 'gcc' failed with exit status 1 思路这些错乍看不一样，其实差不多，都是缺失了一些东西，导致安装失败。针对每个问题逐个谷歌百度就会发现，大家会告诉你需要安装对应的开发库及xxx-dev(el)包。 比如上面第一个问题里，安装mysqlclient包报错了，需要安装对应的dev包，在SuSE中是libmysqlclient-devel。 xxx与xxx-dev(el)的关系经过这两天的查询和总结，再Liunx中，一般会把软件拆分为两部分，一部分是直接使用的库即xxx，另一部分就是开发用的库，包含一些头文件之类的，就是xx-dev(el)。 差不多可以这样理解：当你只是使用某个软件的时候，你只要安装xxx即可，但当你需要二次开发或者使用对应的一些插件的时候，很可能你就会需要再安装xxx-dev(el)了。 https://blog.csdn.net/wangeen/article/details/14522227 解决办法知道了问题的原因，那么解决办法就简单了。但是对于SuSE，尤其是没有外网的SuSE就不是了。 SuSE是收费的系统，没办法直接下载到对应的rpm安装包，需要挂载对应系统版本的SDK光盘。而问题是不断暴露和修复的，我们需要的dev包可能在最初获取的SDK光盘里不存在。查看了SuSE官网，有些补丁也是建议通过网络更新的，或者直接下载更新SDK安装盘。 https://www.suse.com/zh-cn/documentation/sles-12/book_sle_deployment/data/sec_add-ons_sdk.html 不过官网下载是相当的慢。。。 zypper的使用zypper是SuSE的当我们有了对应的sdk光盘或者目录之后，可以通过nfs的方式挂载到zypper源中。 添加源 123zypper ar -t yast2 -n 'sles12sp3_sdk1' -fc nfs://122.16.125.112/iso/sles12sp3_sdk1 sles12sp3_sdk1zypper ar -t yast2 -n 'sles12sp3_sdk2' -fc nfs://122.64.29.85/approot1/sles12sp3_sdk sles12sp3_sdk2zypper ar -t yast2 -n 'sles12sp3_server' -fc fs://122.64.29.85/approot1/sles12sp3_server sles12sp3_server 搜索需要的安装包 当遇到上述问题中的安装失败，使用如下的命令搜索下有哪些包可以安装： 1zypper se mysql 显示结果如下： 安装对应的包 这里可以看到libmysqlclient-devel没有安装（前面有i标记的即为已经安装），安装即可。 1~ # zypper install libmysqlclient-devel 搜索安装其他devel包 同理，在第二个问题中，我们搜索ldap，就会发现可以libldapcpp-devel包没有安装。 装好之后继续安装python-ldap还会遇到一个错误： 12345Modules/LDAPObject.c:18:18: fatal error: sasl.h: No such file or directory #include &lt;sasl.h&gt; ^compilation terminated.error: command 'gcc' failed with exit status 1 同理，再搜一下： 发现cyrus-sasl-devel包没装，也把装上，问题就解决了。 注意python也是有python-devel包的，这个不要忘了。]]></content>
      <categories>
        <category>操作系统</category>
        <category>SuSE</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>MySQL</tag>
        <tag>SuSE</tag>
        <tag>zypper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL的安装]]></title>
    <url>%2FMySQL%E7%9A%84%E5%AE%89%E8%A3%85.html</url>
    <content type="text"><![CDATA[背景每次重搭环境都免不了要重新安装数据库，虽然频率不高，也发现竟然在3个平台上都装过了，记录一下。 Windows安装MySQL 下载MySql：http://dev.mysql.com/downloads/mysql/ 解压后放到安装目录 在环境变量中将mysql安装目录定义为%MYSQL_HOME%，并将bin目录加入环境变量（%MYSQL_HOME%\bin） 在mysql-5.6.24-winx64的根目录下，找到my-default.ini文件，改名为my.ini 。打开，添加如下信息： 123456789[mysqld]loose-default-character-set = utf8character-set-server = utf8basedir = D:\mysql-5.6.24-winx64 #写自己的mysql路径哦datadir = D:\mysql-5.6.24-winx64\data #写自己的mysql路径哦！[client]loose-default-character-set = utf8[WinMySQLadmin]Server = D:\mysql-5.6.24-winx64\bin\mysqld.exe # 写自己的mysql路径哟！ 以管理员身份运行cmd，进入到%MYSQL_HOME%到bin目录下运行如下命令： 12mysqld --initialize -insecure # 初始化mysql，创建root用户，密码为空mysqld -install 最后提示：Service successfully in installed! 启动mysql。 1net start mysql 停止mysql 1net stop mysql 首次登陆，无需密码 1mysql -u root 修改root密码 1mysql&gt; set password for root@localhost = password(&apos;root&apos;); 再次登陆 1mysql -u root -p 会提示输入密码，输入后已root用户进入。 其他命令 停止mysql 1net stop mysql 卸载服务 1mysqld -remove MAC下安装Mysql 下载Mac版的DMG Archive包。（不建议用brew安装，后续配置很麻烦） 后双击安装，一路下一步 会出现一个提醒，给了个默认root@localhost账号的密码。比如：root@localhost: wuKgf_mCK38z 安装完成后，在系统偏好设置中找到MySQL图标，点击进入，手动启动MySQL服务。 通过alias绑定命令：在命令行中运行如下命令，绑定mysql 12MacBook-Air:~ icbc$ alias mysql=/usr/local/mysql/bin/mysqlMacBook-Air:~ icbc$ alias mysqladmin=/usr/local/mysql/bin/mysqladmin ​ 注意：这种方式只能在当前命令行中有效。 建议是在环境变量中增加： 123456cd ~touch .bash_profilevi .bash_profile# 加入如下语句export PATH=$&#123;PATH&#125;:/usr/local/mysql/bin 进行root密码重置，比如重置为root，运行如下命令，然后输入临时密码即可。 12MacBook-Air:~ icbc$ mysqladmin -u root -p password rootEnter password: Suse下离线安装MySQL 先下载Suse版本的RPM包，官网有很多，要下载Bundle版，否则得分别下载各个组件。 SUSE Linux Enterprise Server 12 (x86, 64-bit), RPM Bundle 解压安装包 12345HzTomcat:/data # tar -xvf mysql-8.0.13-1.sles12.x86_64.rpm-bundle.tarHzTomcat:/data # cd mysql-8.0.13-1.sles12.x86_64/HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # lsmysql-community-client-8.0.13-1.sles12.x86_64.rpm mysql-community-devel-8.0.13-1.sles12.x86_64.rpm mysql-community-server-8.0.13-1.sles12.x86_64.rpmmysql-community-common-8.0.13-1.sles12.x86_64.rpm mysql-community-libs-8.0.13-1.sles12.x86_64.rpm mysql-community-test-8.0.13-1.sles12.x86_64.rpm 下面的安装要注意按顺序，先安装mysql-community-common-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-common-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-common-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-common-8.0.13-1.s################################# [100%] 再安装mysql-community-libs-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-libs-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-libs-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-libs-8.0.13-1.sle################################# [100%] 再安装mysql-community-client-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-client-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-client-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-client-8.0.13-1.s################################# [100%] 最后安装mysql-community-server-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-server-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-server-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-server-8.0.13-1.s################################# [100%] 启动 1HzTomcat:~ # service mysql start 查看临时root密码 12HzTomcat:~ # grep 'temporary password' /var/log/mysql/mysqld.log 2019-01-09T07:16:31.105387Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: !kwpeD_Pc7/p 修改root密码 12345678910111213141516171819202122HzTomcat:~ # mysql -uroot -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 8Server version: 8.0.13Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; alter user 'root'@'localhost' IDENTIFIED BY 'Password123!';Query OK, 0 rows affected (0.14 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.07 sec)mysql&gt; exitBye 可能出现的问题问题在修改root密码时，忘了flush privileges，然后导致新旧密码都无法登陆的问题。 123HzTomcat:~ # mysql -uroot -pEnter password: ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES) 解决方法 修改mysql的配置文件： 1HzTomcat:~ # vim /etc/my.cnf 增加：skip-grant-tables 然后在登陆直接使用root登陆，重新设置密码，即可。 12345678910111213141516171819202122232425262728293031HzTomcat:/var/lib/mysql # mysql -uroot Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 8.0.13 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; flush privileges;Query OK, 0 rows affected (0.02 sec)mysql&gt; alter user &apos;root&apos;@&apos;localhost&apos; identified by &apos;Password123!&apos;;Query OK, 0 rows affected (0.07 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.07 sec)mysql&gt; exitByeHzTomcat:/var/lib/mysql # vim /etc/my.cnfHzTomcat:/var/lib/mysql # service mysql restartHzTomcat:/var/lib/mysql # mysql -uroot -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 8Server version: 8.0.13 MySQL Community Server - GPL]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask - sqlalchemy.orm.exc.DetachedInstanceError]]></title>
    <url>%2Fflask-sqlalchemy-orm-exc-DetachedInstanceError.html</url>
    <content type="text"><![CDATA[背景在一个基于Flask的项目中，使用到flask-sqlachemy的数据库orm工具。在一次数据插入之后再次查询数据时出现了如下的错误： 1DetachedInstanceError: Instance &lt;User at 0x7f2f54fc8750&gt; is not bound to a Session; attribute refresh operation cannot proceed 原因经查，由于之前开发的时候，参考《Flask Web开发 基于Python的Web应用开发实战》这本书时，作者建议在配置flask-sqlachemy时，加入如下配置： 1SQLALCHEMY_COMMIT_ON_TEARDOWN = Ture 这个配置是用来涉及在db操作时，自动提交的。以下两种情况是等效的。123456# SQLALCHEMY_COMMIT_ON_TEARDOWN = True 时db.add(User)# SQLALCHEMY_COMMIT_ON_TEARDOWN = False 时db.add(User)db.commit() 但其实自动提交时，系统会一并删除当前数据库的session，所以导致了上面出现的问题。目前flask-sqlachemy官方也认为这个设置可能存在问题，已经在文档中移除了。 http://flask-sqlalchemy.pocoo.org/2.3/changelog/ 结论那么建议的方法就是删除这条配置，手动提交了。在每次进行数据库新增、修改、删除时，手动的commit()。]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>sqlachemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Navcat连MySQL提示Autentication错误]]></title>
    <url>%2FNavicat%E8%BF%9EMySQL%E6%8F%90%E7%A4%BAAutentication%E9%94%99%E8%AF%AF.html</url>
    <content type="text"><![CDATA[背景Mysql版本：8.0.13 Navicat版本：Navicat Premium Version 12 问题在MySQL中创建好用户，数据库，并将数据库的权限授权给用户之后 1234567891011mysql&gt; create user 'flask'@'%' identified by 'xxxx';Query OK, 0 rows affected (0.01 sec)mysql&gt; create database FlaskDB default charset utf8 collate utf8_general_ci;Query OK, 1 row affected (0.00 sec)mysql&gt; grant all privileges on FlaskDB.* to "flask"@"%"; Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 然后通过Navicat使用连接数据库，出现了下面的报错： 结论这个问题百度一下有很多帖子，是由于新版的MySQL8更新了认证方式，而其他的数据库软件没有跟上。大部分忒子会推荐我们重新配置MySQL，不启用新的认证方式，甚至有些建议删了重装。 个人还是推荐下面这种方式： 1ALTER USER 'username'@'%' IDENTIFIED WITH mysql_native_password BY 'password'; 不重新配置MySQL，暂时先指定使用旧版本的认证方式来设置密码。待后续软件更新了自然就可以使用新版本功能了。 https://stackoverflow.com/questions/49194719/authentication-plugin-caching-sha2-password-cannot-be-loaded]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
        <tag>navicat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle wm_concat实现字符串分割替换]]></title>
    <url>%2Foracle-wm-concat%E5%AE%9E%E7%8E%B0%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%88%86%E5%89%B2%E6%9B%BF%E6%8D%A2.html</url>
    <content type="text"><![CDATA[背景在数据库中有一个存储有权限用户的字段，通过,分割来存储了多个用户的账号。可以通过用户账号在用户表中查询出用户的姓名，现在需要将这个字段的账号转换成用户姓名在前台展示。 用户表user： id name 0001 小明 0002 小刚 需要处理的字段：0001,0002。 要求的返回结果：小明,小刚。 思路 首先我们需要把user表中id字段存在在处理字段中的值给查出来。 最开始考虑使用instr函数： 1select * from user where instr('0001,0002', id)&gt;0 instr()函数可以判断值在字符串中的位置，如果大于0，也就是存在。但是这是模糊匹配的，会有问题。比如上面场景中，如果有用户id是000那么也会匹配出来。 所以使用regexp_like()，通过正则来判断。 这里的正则表达如为：^(0001|0002)$。使用^确定开通，$确认结尾，|用来分割允许的值。对应到实际的就变成如下： 1select name from user where regexp_like(id, '^(' || replace('0001,0002', ',', '|') || ')$') 注意：很多正则判断工具里^0001|0002$也可以实现效果，但oracle中，必须加上()，否则oracle会认为是^0001 和0002$，0001001这样的也会被识别到。 输出结果： name 小明 小刚 然后可以使用wm_concat函数把连起来即可。 wm_concat是一个未被记录的函数，但是可以实现将列通过,连成一个字段的作用。 1select to_char(wmsys.wm_concat(name)) from user where id = '0001' or id = '0002' https://community.oracle.com/thread/1090158 结论连起来结果如下： 1select to_char(wmsys.wm_concat(name)) name from user where regexp_like(id, '^'|| replace('0001,0002', ',', '|')||'$') 输出结果： name 小明,小刚 注意由于wm_concat未被官方记录，不同版本的oracle有区别，如果输出是个clob类型，那么可以使用to_char()来转换。也可以考虑使用listagg()来实现。]]></content>
      <categories>
        <category>数据库</category>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>oracle</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle regexp_substr函数实现字符串split]]></title>
    <url>%2Foracle-regexp-substr%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E5%AD%97%E7%AC%A6%E4%B8%B2split.html</url>
    <content type="text"><![CDATA[背景数据库中有一个owner字段，里面存了多个有权限用户的账号，通过,分割。现前台返回一个用户账号，需要判断该用户是否有权限。首先要把这个字段进行分割成列，然后就可以判断是否存在。 思路 首先oracle是可以使用regexp_substr实现按照分隔符获取元素的。 1234567REGEXP_SUBSTR(source_char, pattern [, position [, occurrence [, match_parameter ] ] ] ) 其中： source_char是需要分割的字符串。 pattern为正则表达。 position是起始位置，默认为1。 occurrence是表示第几个匹配组，默认为1。 match_parameter是匹配参数： i：大小写不敏感 c：大小写敏感 n：允许(.)匹配换行符 m：oracle将把字符串当做多行字符处理，^和$是起始和结束 x：忽略空格 https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions131.htm 于是我们通过下面的表达式获取分割后的某个值： 1select regexp_substr('1,2,3', '[^,]+', 1, 2) from dual; --获取第二个数 结果如下： 现在我们需要把其中的occurrence变成一个和分割后等长的列即可。 获取分割后元素的个数 使用length()可以获取字符串的个数，而字符串是通过,分割的。使用replece()可以替换删除所有的分割符。分割后的字符串个数即是两者的差加1。 1select length('1,2,3') - length(replace('1,2,3',',')) + 1 from dual; 结果如下： 获取一个空列 可以通过level加上connect by来获取一个空列： 1select level from dual connect by level &lt;= 5; 结果如下： 把上述连起来就可以获取分割后的列了。 结论虽然有点长，但是总体思路还是比较清晰的。代码如下： 1select regexp_substr('1,2,3','[^,]+' , 1, level) from dual connect by level &lt;= (select length('1,2,3') - length(replace('1,2,3',',')) + 1 from dual); 结果如下： 最后我们需要判断某个人是否有权限，即某人的账号是否在我们获取的分割后的列里面。直接用count(1)是否大于0即可。 12345select count(1) from (select regexp_substr('1,2,3','[^,]+' , 1, level) as owner from dual connect by level &lt;= (select length('1,2,3') - length(replace('1,2,3',',')) + 1 from dual) twhere t.owner = i_owner]]></content>
      <categories>
        <category>数据库</category>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac常用装机及开发软件分享]]></title>
    <url>%2FMac%E5%B8%B8%E7%94%A8%E8%A3%85%E6%9C%BA%E5%8F%8A%E5%BC%80%E5%8F%91%E8%BD%AF%E4%BB%B6%E5%88%86%E4%BA%AB.html</url>
    <content type="text"><![CDATA[其实软件也不多，主要几个专业软件和开发软件要收费，经常会有一些失效的破解和假的破解版。亲测可用，在这里记录和分享，以备万一。 办公软件 Office2019 + 激活：链接:https://pan.baidu.com/s/1YgGS6AI-XdI5vCcyfPA8OQ 密码:f2dl 专业软件 Photoshop CC2018 + 激活：链接:https://pan.baidu.com/s/1ZdPU1GkkOmYYfgvtLOwpBA 密码:zv6u Auto CAD 2018 + 激活：链接:https://pan.baidu.com/s/1IYWly6SJuGvh8l0LI33hcg 密码:2tpg typora（markdown编辑器）：https://www.typora.io/ 开发软件 PyCharm：http://www.jetbrains.com/pycharm/download/#section=mac idea：https://www.jetbrains.com/idea/download/#section=mac WebStrom：https://www.jetbrains.com/webstorm/download/download-thanks.html Navcat Premium：链接:https://pan.baidu.com/s/1egvUoYnoi21980vo38vQUw 密码:7zn6 SubLime：http://www.sublimetext.com/3 其他 Parallel Desktop（虚拟机）+ 激活：链接:https://pan.baidu.com/s/1dPfTfCDVpzXMrivbt9OUqg 密码:ztns Win10官方镜像：链接:https://pan.baidu.com/s/11iOX061WNDlt9oSpNJh6lw 密码:xn7r Win10 激活工具：链接:https://pan.baidu.com/s/1FNbLuXKG5TPW9y_hvr5YmA 密码:jn9b Keka118（压缩软件）：链接:https://pan.baidu.com/s/16Yjo44uRRPk0aZY6ugZ4qg 密码:cli2]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用node-schedule实现node后台定时任务]]></title>
    <url>%2F%E7%94%A8node-schedule%E5%AE%9E%E7%8E%B0node%E5%90%8E%E5%8F%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.html</url>
    <content type="text"><![CDATA[背景最近在自己的小程序中增加了多币种支持涉及到汇率的更新，于是需要在后台服务端设置一个定时任务来自动通过响应的接口更新最新的汇率。获取汇率的接口目前通过聚合数据提供的免费接口实现（调到需要实名注册）。 node-schedule由于小程序后端服务器是基于node，查一下，果然是有对应的npm包的——node-schedule。 node-schedule可以使用多种方式定义定时任务的，一般使用类似liunx的cron方式就可以满足绝大部分需求了。cron的定义方式参考如下表： 123456789* * * * * *┬ ┬ ┬ ┬ ┬ ┬│ │ │ │ │ ││ │ │ │ │ └ 星期 day of week (0 - 7) (0 or 7 is Sun)│ │ │ │ └───── 月 month (1 - 12)│ │ │ └────────── 日期 day of month (1 - 31)│ │ └─────────────── 小时 hour (0 - 23)│ └──────────────────── 分钟 minute (0 - 59)└───────────────────────── 秒 second (0 - 59, OPTIONAL) 如果你熟悉linux系统的crontab定时任务的话，那就相当简单了。 它还支持基于日期的定时任务以及基于rule的定时任务。详细可以见一下官方说明。 https://www.npmjs.com/package/node-schedule 实现方法首先需要安装node-schedule包，并保存在项目的package.json中。 1cnpm install node-schedule --save 我的需求是每三十分钟自动更新一次数据，那么通过cron的方式就可以定义为如下规则： 10 */30 * * * * 其中*/30是指可以被30整除的，也就是0分和30分的时候。规则确定了，那么代码就很简单了： 12345678910const schedule = require('node-schedule')function scheduleCron () &#123; schedule.scheduleJob('0 */30 * * * *', function () &#123; console.log('scheduleCron ' + new Date()) // do something &#125;)&#125;scheduleCron() 输出结果： 12scheduleCron Wed Jan 02 2019 00:00:00 GMT+0800 (CST)scheduleCron Wed Jan 02 2019 00:30:00 GMT+0800 (CST) 注意！根据官方说明node-schedule是基本上支持所有的cron表达式，除了一下几个： W：最近的工作日，放在日期(day of month)字段，比如15W指到本月15日最近的工作日。 L：表示最后，放在星期(day of month)或者星期(day of week)字段。 #：表示每月的周几，放在星期(day of week)字段。 cron拓展配置规则时几个常用的符号： *：表示匹配任意值。 /：x/y表示等步长序列，可以理解为从x开始，每y个单位执行一次。其中*/5与0/5是等效的，都是指每5分钟执行一次。 ,：表示序列的分割，比如3,4指在3和4的时候执行。 -：表示一个范围，比如3-5指在3到5的时候执行。 ?：仅用在日期和星期字段，表示任意的值，相当于占位。]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序腾讯云环境安装依赖错误]]></title>
    <url>%2F%E5%B0%8F%E7%A8%8B%E5%BA%8F%E8%85%BE%E8%AE%AF%E4%BA%91%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E9%94%99%E8%AF%AF.html</url>
    <content type="text"><![CDATA[背景开发工具：微信开发者工具 操作系统：Mac 腾讯云环境：Node 问题最近使用node-sechedule开发了一个后台定时任务，这个是新增的node模块，所以需要安装依赖，即在package.json中增加对应的依赖： 1234"dependencies": &#123; ... "node-schedule": "^1.3.1" &#125;, 在本地测试通过之后，点击开发工具上面的“腾讯云”-“上传测试代码” 并勾选安装依赖。问题就出现了！！！ 排查及原因 感觉这是权限问题啊，于是切换了小程序的管理员用户，点击上传，情况一样。 无奈只能先恢复开发环境了，然后再次上传。根据文档推荐，首次上传最好使用模块上传，全部勾选，结果情况还是一样。 看了node_modules文件夹中，的确有node-schedule文件夹啊，不过有一点奇怪的是，安装每个安装的npm包都有一个_开头的文件夹： 看来得找专家了，于是在腾讯云控制台上提交了一个工单，过了半个小时的样子就有工程师电话来了。他首先在自己的环境里试了一下我的package.json包，竟然没有问题。。。 工程师登录我的后台服务器看了下： 感觉可能是因为上面这种奇怪的包文件夹造成的。于是乎，再恢复一次环境，然后我再次上传，这次不再勾选“上传node_modules代码”。 这样也即是让后台服务器自动安装所有的依赖。最后，成功了！ 这时候再看一下后台服务器上目录： 没有了那种_开头的目录，就成功了。 插曲有了上面的经验，本以为上传到生产环境就妥妥的了。结果点击“上传正式代码”之后，坑爹了。 正式代码上传是没有上面的这种选项的，本机上的那些_开头的node_modules文件也上传了，再一次入坑。 于是乎，只能再次联系腾讯云的工程师，删除了我工程的node_modules文件夹，然后点击控制台上的安装依赖。待依赖安装完成后，再次部署代码。 这时候后端启动已经没有问题，但是小程序竟然无法直接访问，最后发现是由于小程序解决方案里本来是自带ssl证书的，这个证书最近到期了，需要自己购买新的证书（选免费的）联系后台更新。 至此问题彻底解决。 结论可以看出，主要问题还是在于node_modules里文件的问题。而这个的罪魁祸首是——cnpm。 可以参考如下的issue，cnpm安装包时采用link的方式，与npm不一样。 https://github.com/cnpm/cnpmjs.org/issues/1000 在小程序上传之后自动安装包，则用的是npm安装，从而导致问题出现。以后在没弄明白之前还是先用原生的吧，在别人给你便利的时候，也要明白其中有什么道理。 另外，建议自己在本地开发的时候，server目录下还是把node_modules给删除了，以免后续一不小心又上传了什么问题文件。]]></content>
      <categories>
        <category>微信小程序</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>小程序</tag>
        <tag>javascript</tag>
        <tag>腾讯云</tag>
        <tag>微信开发者工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Gulp实现Hexo网页压缩优化]]></title>
    <url>%2F%E7%94%A8Gulp%E5%AE%9E%E7%8E%B0Hexo%E7%BD%91%E9%A1%B5%E5%8E%8B%E7%BC%A9%E4%BC%98%E5%8C%96.html</url>
    <content type="text"><![CDATA[背景经过一段时间的折腾，也算是把这个Hexo的个人博客搭建起来了，换主题，加插件，文章里加图片是什么的，就发现网站有时候会有点慢，于是开始考虑做SEO以及一些优化工作，于是乎发现了Gulp这个神器。 Gulp是什么Hexo生成的静态网页其实是可读性比较好的，会有大量的空格、换行什么的，而实际浏览器解析式完全不需要的。如果把这些空格、换行全部删掉，就会节省很多空间出来，于是网站的响应速度也就变快了。 而Gulp是一种基于node的自动化构建工具，至于自动化构建这个我们目前不需要纠结，我们只要知道它有一些插件可以帮助我们自动化的对hexo生成的各种文件进行压缩。 Gulp怎么用 首先我们要在全局安装下gulp和我们要用到的插件 12npm install gulp -gnpm install gulp gulp-uglify gulp-minify-css gulp-imagemin gulp-htmlmin gulp-htmlclean gulp-concat --save 在hexo的根目录创建一个gulpfile.js文件 先放代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162var gulp = require('gulp'), uglify = require('gulp-uglify'), cssmin = require('gulp-minify-css'), imagemin = require('gulp-imagemin'), htmlmin = require('gulp-htmlmin'), htmlclean = require('gulp-htmlclean'); concat = require('gulp-concat');//JS压缩gulp.task('uglify', function() &#123; return gulp.src(['./public/js/**/.js','!./public/js/**/*min.js'])//只是排除min.js文件还是不严谨，一般不会有问题，根据自己博客的修改我的修改为return gulp.src(['./public/**/*.js','!./public/zuoxi/**/*.js',,'!./public/radio/**/*.js']) .pipe(uglify()) .pipe(gulp.dest('./public/js'));//对应修改为./public即可&#125;);//public-fancybox-js压缩gulp.task('fancybox:js', function() &#123; return gulp.src('./public/vendors/fancybox/source/jquery.fancybox.js') .pipe(uglify()) .pipe(gulp.dest('./public/vendors/fancybox/source/'));&#125;);// 合并 JSgulp.task('jsall', function () &#123; return gulp.src('./public/**/*.js') // 压缩后重命名 .pipe(concat('app.js')) .pipe(gulp.dest('./public'));&#125;);//public-fancybox-css压缩gulp.task('fancybox:css', function() &#123; return gulp.src('./public/vendors/fancybox/source/jquery.fancybox.css') .pipe(cssmin()) .pipe(gulp.dest('./public/vendors/fancybox/source/'));&#125;);//CSS压缩gulp.task('cssmin', function() &#123; return gulp.src(['./public/css/main.css','!./public/css/*min.css']) .pipe(cssmin()) .pipe(gulp.dest('./public/css/'));&#125;);//图片压缩gulp.task('images', function() &#123; gulp.src('./public/uploads/*.*') .pipe(imagemin(&#123; progressive: false &#125;)) .pipe(gulp.dest('./public/uploads/'));&#125;);// 压缩 public 目录 html文件 public/**/*.hmtl 表示public下所有文件夹中html，包括当前目录gulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// gulp.task('default', gulp.series('uglify', 'cssmin', 'fancybox:js', 'fancybox:css', 'jsall','images'));gulp.task('default', gulp.series('uglify', 'cssmin', 'jsall', 'minify-html'));//, 'minify-html' 这里要注意的是，默认安装的是gulp 4.0.0，而网上很多例子是基于gulp 3的，所以运行起来 会有如下的报错： 123456789101112131415assert.js:351 throw err; ^AssertionError [ERR_ASSERTION]: Task function must be specified at Gulp.set [as _setTask] (/Users/zhengk/Desktop/hexo/blog/node_modules/_undertaker@1.2.0@undertaker/lib/set-task.js:10:3) at Gulp.task (/Users/zhengk/Desktop/hexo/blog/node_modules/_undertaker@1.2.0@undertaker/lib/task.js:13:8) at Object.&lt;anonymous&gt; (/Users/zhengk/Desktop/hexo/blog/gulpfile.js:59:6) at Module._compile (internal/modules/cjs/loader.js:721:30) at Object.Module._extensions..js (internal/modules/cjs/loader.js:732:10) at Module.load (internal/modules/cjs/loader.js:620:32) at tryModuleLoad (internal/modules/cjs/loader.js:560:12) at Function.Module._load (internal/modules/cjs/loader.js:552:3) at Module.require (internal/modules/cjs/loader.js:657:17) at require (internal/modules/cjs/helpers.js:22:18) 这是由于gulp4中需要使用gulp.series 和 gulp.parallel来指定运行的任务。具体见上面gulpfile.js中的最后一行。 1gulp.task('default', gulp.series('uglify', 'cssmin', 'jsall', 'minify-html')); https://blog.csdn.net/qq_31975963/article/details/83034450 这一行是写明gulp需要执行的任务，然后需要注意的是，当其中某个任务失败或者没有东西需要压缩的时候，比如你没有用到fancybox却要执行fancybox:js任务，就会有如下的报错： 1234567891011121314151617181920[23:59:25] Using gulpfile ~/Desktop/hexo/blog/gulpfile.js[23:59:25] Starting 'default'...[23:59:25] Starting 'uglify'...[23:59:25] Finished 'uglify' after 24 ms[23:59:25] Starting 'cssmin'...[23:59:26] Finished 'cssmin' after 215 ms[23:59:26] Starting 'fancybox:js'...[23:59:26] 'fancybox:js' errored after 2.96 ms[23:59:26] Error: File not found with singular glob: /Users/zhengk/Desktop/hexo/blog/public/vendors/fancybox/source/jquery.fancybox.js (if this was purposeful, use `allowEmpty` option) at Glob.&lt;anonymous&gt; (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob-stream@6.1.0@glob-stream/readable.js:84:17) at Object.onceWrapper (events.js:277:13) at Glob.emit (events.js:189:13) at Glob.EventEmitter.emit (domain.js:441:20) at Glob._finish (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:197:8) at done (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:182:14) at Glob._processSimple2 (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:688:12) at /Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:676:10 at Glob._stat2 (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:772:12) at lstatcb_ (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:764:12)[23:59:26] 'default' errored after 246 ms 只要把对应的任务删掉就好了。 运行gult 先执行hexo g来生成静态网页，然后我们看下public文件夹下的静态文件大小： 1234$ls -lh...-rw-r--r-- 1 zhengk staff 57K 12 30 00:07 index.html... 然后我们再执行下gulp（执行时默认执行default任务，所以前面gulpfile.js中设置任务为default）对比下效果： 1234567891011121314151617$gulp[00:53:29] Working directory changed to ~/Desktop/hexo/blog[00:53:30] Using gulpfile ~/Desktop/hexo/blog/gulpfile.js[00:53:30] Starting 'default'...[00:53:30] Starting 'uglify'...[00:53:30] Finished 'uglify' after 23 ms[00:53:30] Starting 'cssmin'...[00:53:30] Finished 'cssmin' after 216 ms[00:53:30] Starting 'jsall'...[00:53:30] Finished 'jsall' after 56 ms[00:53:30] Starting 'minify-html'...[00:53:31] Finished 'minify-html' after 1.16 s[00:53:31] Finished 'default' after 1.46 s$ls -lh...-rw-r--r-- 1 zhengk staff 27K 12 30 00:07 hello-world.html... 可以发现足足小了30k，压缩了近一半大小。]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>hexo</tag>
        <tag>gulp</tag>
        <tag>seo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript中的Json、Map、Set]]></title>
    <url>%2FJavaScript%E4%B8%AD%E7%9A%84Json%E3%80%81Map%E3%80%81Set.html</url>
    <content type="text"><![CDATA[问题之前在一个项目中，需要根据申请的部门来获取对应的邮箱地址，想当然的使用了Map对象，结果在调试中完全没有问题，却在实际使用上失效了，查看了下后台log，提示获取到的邮箱地址是undefined。 排查及原因经过百度之后发现原来JS的Map对象的浏览器支持不好，虽然很多地方写IE11开始支持，但其实IE11是不支持new Map()这种方式新建的。 详见如下链接：https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Map#浏览器兼容 于是我把换成了用Json对象，果然就解决了。 所以尽量还是用Json吧。 总结Map对象JS的Map存放的是键值对，key值不允许重复。特别要注意的是在一些旧的浏览器中并不支持Map对象。 基本使用 12345678910111213141516171819202122// 新建Map对象&gt; m = new Map()Map &#123;&#125;// 新增Map键值对&gt; m.set('key1','value1')Map &#123; 'key1' =&gt; 'value1' &#125;// 获取Map中的key值&gt; m.get('key1')'value1'// 判断是否存在某key&gt; m.has('key1')true&gt; m.set('key2','value2')Map &#123; 'key1' =&gt; 'value1', 'key2' =&gt; 'value2' &#125;// 删除某键值对&gt; m.delete('key1')true&gt; mMap &#123; 'key2' =&gt; 'value2' &#125;// 清空Map对象&gt; m.clear()undefined 遍历 12345678910&gt; m.set('key1','value1')Map &#123; 'key1' =&gt; 'value1' &#125;&gt; m.set('key2','value2')Map &#123; 'key1' =&gt; 'value1', 'key2' =&gt; 'value2' &#125;&gt; m.forEach(function (value, key, map) &#123;... console.log(key + ":" + value)... &#125;)key1:value1key2:value2undefined Set对象Set对象可以理解为没有值的Map对象，一般用于存放一个不允许重复的列表 基本使用 123456789101112131415161718// 新建Set对象&gt; s = new Set()Set &#123;&#125;// 新增Set值&gt; s.add('key1')Set &#123; 'key1' &#125;&gt; s.add('key2')Set &#123; 'key1', 'key2' &#125;// 删除某值&gt; s.delete('key1')true&gt; sSet &#123; 'key2' &#125;// 清空Set对象&gt; s.clear()undefined&gt; sSet &#123;&#125; 遍历 Set对象的礼遍历和Map基本一样，但是由于Set对象没有value值，所以遍历的时候key和value是一样的。 12345678910&gt; s.add('key1')Set &#123; 'key1' &#125;&gt; s.add('key2')Set &#123; 'key1', 'key2' &#125;&gt; s.forEach(function (value, key, set) &#123;... console.log(key + ":" + value)... &#125;)key1:key1key2:key2undefined Json对象 基本使用 123456\\ 新建Json对象&gt; let currencyItems = &#123; '人民币': 1, '港币': 0.88, '澳门元': 0.86, '新台币': 0.2241, '美元': 6.905, '日元': 0.06, '英镑': 8.69, '欧元': 7.8, '韩元': 0.006, '泰铢': 0.21, '新西兰元': 4.69, '澳大利亚元': 4.96, '菲律宾比索': 0.13, '加拿大元': 5.16, '瑞士法郎': 6.92, '瑞典克朗': 0.76, '丹麦克朗': 1.05, '挪威克朗': 0.8 &#125;undefined\\ 获取Json对象某值&gt; currencyItems['人民币']1 获取Json对象的所有Key值 12345678910111213141516171819&gt; Object.keys(currencyItems)[ '人民币', '港币', '澳门元', '新台币', '美元', '日元', '英镑', '欧元', '韩元', '泰铢', '新西兰元', '澳大利亚元', '菲律宾比索', '加拿大元', '瑞士法郎', '瑞典克朗', '丹麦克朗', '挪威克朗' ] 获取Json对象的长度 12&gt; Object.keys(currencyItems).length18 遍历 12345678910111213141516171819202122&gt; for ( var i in currencyItems) &#123;... console.log(i + ":" + currencyItems[i])... &#125;人民币:1港币:0.88澳门元:0.86新台币:0.2241美元:6.905日元:0.06英镑:8.69欧元:7.8韩元:0.006泰铢:0.21新西兰元:4.69澳大利亚元:4.96菲律宾比索:0.13加拿大元:5.16瑞士法郎:6.92瑞典克朗:0.76丹麦克朗:1.05挪威克朗:0.8undefined]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>Json</tag>
        <tag>Map</tag>
        <tag>Set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows Server 2018 TCP 连接数限制问题]]></title>
    <url>%2FWindows-Server-2018-TCP-%E8%BF%9E%E6%8E%A5%E6%95%B0%E9%99%90%E5%88%B6%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景最近在查一个并发问题，在压测的时候，Nginx的error.log显示connect() failed(111: Connection refused)。而后端应用并未手工设置过拒绝连接。于是怀疑是在高并发的情况下，windows服务器可能存在自行拒绝连接的情况。 排查过程首先打开windows服务器上的 任务管理器 - 性能 - 资源监控器。TCP连接这儿显示总数为100。 然后开启压测，TCP连接开始飙升，然后问题出现了。 TCP连接满了，怎么就变成10了！不过瓶颈应该就是这儿了！ 结论经过各种百度，谷歌，发现我好像被误导了。 微软官方说从Windows Vista，Window server 2008 SP2 起，不在限制half-open TCP connections，也就是理论上不再有连接数的限制。 官方说明见这个地址：https://support.microsoft.com/zh-cn/help/969710/how-to-enable-the-half-open-tcp-connections-limit-in-windows-vista-wit 然后根据国外有个问答网站的结论，这个“10”，“100”这个显示应该是个Bug，并不是一共就10个或者100个。 可参考如下这个解释： https://serverfault.com/questions/448589/increasing-of-max-more-than-10-tcp-connections 那么怎么看确定的连接数呢？ 在 开始 - 运行 中输入 perfmon.exe打开性能监视器，然后添加TCPv4的计数器。 这里就可以看到当前的实际连接数了，图里当前最新连接数是“824“，远超前面显示的”10“或者”100“。 看来一不小心又碰到坑了。 那么最开始要查的Connection refused到底是什么原因呢，还得继续努力了。。。]]></content>
      <categories>
        <category>操作系统</category>
        <category>windows</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取当当网的书籍分类目录]]></title>
    <url>%2F%E8%8E%B7%E5%8F%96%E5%BD%93%E5%BD%93%E7%BD%91%E7%9A%84%E4%B9%A6%E7%B1%8D%E5%88%86%E7%B1%BB%E7%9B%AE%E5%BD%95.html</url>
    <content type="text"><![CDATA[背景之前单位新建了一个小图书馆，然后就有了这么一个需求，需要设置一下图书的分类与目录。要怎么定义呢，当然是百度咯。然后想到了卖书发家的当当网，打算把当当网上的所有图书分类全部抓下来提供给行政来作参考。 思路打开当当网的图书页面http://book.dangdang.com/，图书分类就在网页的左边，开启F12看源代码。 多看看就看出来规律了，关注红框部分。所有的分类其实都在&lt;a&gt;标签里，其中的href属性里的网址很有规律，去掉前面的域名之后，都以cp + 数字来命名，其中数字与数字之间用.来分割，代表一级目录和二级目录。 所以大体思路就是通过正则表达式先抓取href属性中含有cp开头的元素，然后找出所有第一节数字不同的元素，获取其text属性来当一级目录，然后把域名+cp+一级目录序号当做固定前缀来找对应的二级目录。 要注意就是去重还有一些删除一些网址不符合这个过滤的，以及所有的text记得用strip()来删除一下多余的空格和换行符号。 具体实现按上面的思路，主要用requests bs4就差不多了，详细代码就参考github吧，https://github.com/keejo125/ 有更好的方法的也欢迎分享。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开启又关闭icloud云盘，我的桌面文件去哪里了！]]></title>
    <url>%2F%E5%BC%80%E5%90%AF%E5%8F%88%E5%85%B3%E9%97%ADicloud%E4%BA%91%E7%9B%98%EF%BC%8C%E6%88%91%E7%9A%84%E6%A1%8C%E9%9D%A2%E6%96%87%E4%BB%B6%E5%8E%BB%E5%93%AA%E9%87%8C%E4%BA%86%EF%BC%81.html</url>
    <content type="text"><![CDATA[最近更换笔记本，又不想直接通过时间胶囊设置新mac，于是乎在整理好旧资料之后，准备拷贝到新电脑时发现了iCloud云盘这个东西，可以自动备份桌面和文稿的内容到iCloud，那么在新电脑中再通过iCloud下载就好了，完美！ 结果，高估了iCloud的效果，开启之后，mac会上传桌面和文稿，速度超级慢，然后mac风扇呼呼的转，果断放弃，关闭了iCloud云盘。 关闭也很慢，卡了一会儿，提示 慢的不行，反正也没上传多少东西，于是就点了“停止更新并关闭” 然后就问题出现了！！！ 桌面空空如也，我的东西呢！！！ 在翻翻iCloud云盘，只有已经上传了的那一丢丢！！！ 急中生智，赶紧百度，翻了好结果贴，结论如下： 其实前面已经提示了，文件都能被放在了一个叫做 “iCloud云盘（归档）”中了。路径如下：/User/xxx/中。 真是虚惊一场，看到网上好多碰到一样问题的，好多人以为就没有了。 特地记录一下。]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>iCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于这个”博客“]]></title>
    <url>%2F%E5%85%B3%E4%BA%8E%E8%BF%99%E4%B8%AA%E2%80%9D%E5%8D%9A%E5%AE%A2%E2%80%9C.html</url>
    <content type="text"><![CDATA[最初是在腾讯的云开发者平台上知道的hexo，好像很方便的样子，又很Geek的样子，于是打算尝试一下。 后来发现hexo作为博客的话，缺少了很大一部分功能——评论。 虽然可以使用第三方插件，但总觉得有点怪怪的。 于是乎，我打算把这里作为记录自己日常知识积累，或是感悟的地方。所以在标题中用了有引号的”博客“。 把每次百度或是谷歌出来的答案都记下来，希望不会再搜第二次。 感觉上好像有点跌跌撞撞，但方向是往前不就好了么，是吧]]></content>
      <categories>
        <category>生活随笔</category>
        <category>感悟</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world.html</url>
    <content type="text"><![CDATA[这是一个新的开始。 马东说：我的底色的悲凉的。 蔡康永说：只有底色悲凉的乐观，才是真的乐观啊。 乐观起来，哪怕人间不值得。]]></content>
      <categories>
        <category>生活随笔</category>
        <category>感悟</category>
      </categories>
  </entry>
</search>
