<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[获取猫眼电影的评论]]></title>
    <url>%2F%E8%8E%B7%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E7%9A%84%E8%AF%84%E8%AE%BA.html</url>
    <content type="text"><![CDATA[背景最近几年猫眼电影越来越热门了，都差不多和豆瓣并驾齐驱了。今年的《流浪地球》这么火，看看猫眼电影上网友对该片的评价如何。 思路找到评论网页地址先打开猫眼官网找到《流浪地球》的介绍页面：https://maoyan.com/films/248906 虽然显示有112.4万人评分，但是页面只有热门短评，其他评论都去哪里了，手机明明是有的。 那么我们用chrome切换到手机页面： 打开开发者工具 开启手机浏览功能 访问手机版地址：http://m.maoyan.com/movie/248906?_v_=yes&amp;channelId=4&amp;$from=canary# 这时候我们就看到了所有的评论。 获取评论请求地址在点击打开“查看全部330613条讨论”后，发现评论分为最热和最新两部分，最热数量有限，而最新则是未经过处理的，也正是我们需要的。通过search来查看下对应的请求： 发现，在chrome 的网络展示中发现只有一个类型为document的请求包含了所需的信息。那么这部分的评论获取就需要解析网页了，我们再把屏幕上的评论往下拉，发现会自动加载更多的评论，对应的chrome网络请求多出来了两个comments.json的请求： 果然这才是我们需要的！把初始页面的url和这两个json请求的url复制到一起比较一下： 123http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=0&amp;limit=15&amp;ts=0&amp;type=3http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=15&amp;limit=15&amp;ts=1549965527295&amp;type=3http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=30&amp;limit=15&amp;ts=1549965527295&amp;type=3 我们可以发现规律： 初始页面的ts值为0，随后会有ts值，且保持不变。这里的ts是当前的时间戳，可以通过转换工具查看： offset是请求评论开始的序号，limit为请求的条数 再看返回的json结果： data.comments中是评论的具体内容 paging中通过hasMore来告诉我们是否还有更多（判断是否继续抓取） 我们再尝试下将offset设置为0，也加上ts参数： 1http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=0&amp;limit=15&amp;ts=1549965527295&amp;type=3 发现也是可以获取数据的： 那么推测ts应该是一个时间参数，通过offset和limit来控制每次请求获取的数量。 我们还可以通过加大limit参数来尝试，是否可以一次性获取更多的评论: 1http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=0&amp;limit=30&amp;ts=1549965527295&amp;type=3 效果如下: 再增加limit的值，会发现评论数回到了15条，可见猫眼系统仅支持每次最多获取30条。 构造请求url 方法一根据上面的分析，我们构造请求的url就很明确了： 从offset=0&amp;limit=30开始 通过返回的paging.hasMore来判断是否继续抓取 下一个抓取的url中offset+=limit 只能抓取1000条？！根据上述分析，在返回的json数据中是可以看到总评论数的，但是实际抓取的时候，在offset超过1000之后，返回的数据中hasMore就变成了false。 于是尝试通过浏览器一直下拉刷新，到达offset超过1000的情况，发现页面会不停的发送请求，但也无法获取数据。 那应该就是网站做了控制，不允许offset超过1000。 构造请求URL 方法二那么就要考虑其他构造url的方法来抓取了。先观察下每个请求返回的信息： 发现每个comment里都包含有一个time信息，把time做一下处理： 123456789102019-02-13 13:38:00##感觉韩朵朵这个人设是多余的2019-02-13 13:38:00##真的感动 非常棒2019-02-13 13:38:00##这电影大陆的起航2019-02-13 13:38:00##不怎么样，剧情挺感人，但是有点尴尬2019-02-13 13:37:00##好看。。。。。。。。。。2019-02-13 13:37:00##超级超级超级超级超级超级超级好看2019-02-13 13:37:00##太牛逼了，中国科幻片可有一部能看的了。支持吴京2019-02-13 13:36:00##不错！中国科幻的希望2019-02-13 13:36:00##中国里程碑式的科幻电影。2019-02-13 13:36:00##什么垃圾座位没人管的么乱坐的 可以发现后台是按照时间顺序的，每分钟一个间隔，那么就可以考虑根据每次返回comment中的时间来更新url中的ts即可。 由于不确定每次请求返回的数据中包含了多长的时间段，且返回的第一个评论时间戳与第二个评论是不同的，所以抓取思路如下： 获取请求数据 记录第一个时间戳 记录第二个时间戳 当遇到第三个时间戳时，将ts设置为第二个时间戳，重新构造url 如果单次抓取中每遇到第三个时间戳，则通过修改offset来继续抓取，直到遇到第三个时间戳 实现根据上面思路，实现相对就比较简单了： 生成url 12345def get_url(): global offset url = 'http://m.maoyan.com/review/v2/comments.json?movieId=' + movieId + '&amp;userId=-1&amp;offset=' + str( offset) + '&amp;limit=' + str(limit) + '&amp;ts=' + str(ts) + '&amp;type=3' return url 访问url 123456789101112def open_url(url): global ua try: headers = &#123;'User-Agent': ua.random&#125; response = requests.get(url, headers=headers) if response.status_code == 200: return response.text else: return None except Exception as e: print(e) return None 数据处理：将评论保存并判断是否要继续抓取 1234567891011121314151617181920212223242526272829def parse_json(data): global count global offset global limit global ts ts_duration = ts res = json.loads(data) comments = res['data']['comments'] for comment in comments: comment_time = comment['time'] if ts == 0: ts = comment_time ts_duration = comment_time if comment_time != ts and ts == ts_duration: ts_duration = comment_time if comment_time !=ts_duration: ts = ts_duration offset = 0 return get_url() else: content = comment['content'].strip().replace('\n', '。') print('get comment ' + str(count)) count += 1 write_txt(time.strftime("%Y-%m-%d %H:%M:%S",time.localtime(comment_time/1000)) + '##' + content + '\n') if res['paging']['hasMore']: offset += limit return get_url() else: return None 完整代码可见GitHub:https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/maoyan 如果有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask在Windows环境下的部署]]></title>
    <url>%2FFlask%E5%9C%A8Windows%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E9%83%A8%E7%BD%B2.html</url>
    <content type="text"><![CDATA[背景由于目前在用的Flask项目涉及到一部分依赖Windows的处理，还无法迁移到linux平台，那么在windows环境下，要怎么部署呢？ 思路根据Flask官网介绍，由于Flask内置的服务器性能不佳，推荐的主要的部署方式有如下几种： mod_wsgi (Apache) 独立 WSGI 容器 Gunicorn Tornado Gevent uWSGI FastCGI CGI 上述这些部署方式，仅Tornado是支持在windows情况下部署的，配合上Nginx可以达到比较好的效果。可已参考Nginx与tornado框架的并发评测。 但是在实际使用中发现，tornado 的稳定性虽然很高，但是在tornado上部署Flask，并不会有异步的效果。实际上还是单进程阻塞运行的，即使在Flask中配置了threaded = True也无法实现多线程使用。 Flask多线程情况配置启用多线程： 1234# manage.pyfrom flask_script import Serverserver = Server(host="0.0.0.0", threaded=True) 在Flask中配置两条测试路由 123456789import time@main.route('/test')def maintest(): return 'hello world' @main.route('/sleep')def mainsleep(): time.sleep(60) return 'wake up' 先用浏览器访问\sleep： 随即立刻访问\test: 可见两次访问是不同的线程处理的，不会出现堵塞的情况。 tornado + Flask多线程情况使用tornado托管： 12345678from tornado.wsgi import WSGIContainerfrom tornado.httpserver import HTTPServerfrom tornado.ioloop import IOLoopfrom yourapplication import apphttp_server = HTTPServer(WSGIContainer(app))http_server.listen(5000)IOLoop.instance().start() 先用浏览器访问\sleep： 随即立刻访问\test: 可以发现，虽然tornado框架是支持异步的，但是由于实际上后台的处理是同步的，从而无法实现异步的处理的效果。如果想后台的处理也异步，则需要直接使用tornado来开发。 那么为什么使用tornado来托管flask呢？ Tornado 是一个开源的可伸缩的、非阻塞式的 web 服务器和工具集，它驱动了FriendFeed 。因为它使用了 epoll 模型且是非阻塞的，它可以处理数以千计的并发固定连接，这意味着它对实时 web 服务是理想的。把 Flask 集成这个服务是直截了当的 根据官网描述，其实也是为了弥足flask自带服务器不稳定的问题。 Flask高并发下的表现使用tsung进行压测，压力500： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 34.30 msec 31.91 msec 506 / sec 356.60 / sec 33.19 msec 103908 page 0.42 sec 0.29 sec 505 / sec 356.32 / sec 0.39 sec 103782 request 0.42 sec 0.29 sec 505 / sec 356.32 / sec 0.39 sec 103782 session 1mn 24sec 10.64 sec 11.4 / sec 1.21 / sec 14.24 sec 362 Code Highest Rate Mean Rate Total number 200 505 / sec 356.32 / sec 104792 Name Highest Rate Total number error_abort 0.5 / sec 1 error_abort_max_conn_retries 11.7 / sec 362 error_connect_econnrefused 58.6 / sec 1667 可见，在500的并发下，效果不佳，有很多的链接拒绝。 Flask + Nginx在高并发下的表现 使用tsung进行压测，压力500： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 30.95 msec 1810.5 / sec 626.43 / sec 0.11 sec 189853 page 0.68 sec 0.17 sec 1810.1 / sec 625.72 / sec 0.40 sec 189581 request 0.68 sec 0.17 sec 1810.1 / sec 625.72 / sec 0.40 sec 189581 Code Highest Rate Mean Rate Total number 200 906.4 / sec 196.08 / sec 60689 502 1443.9 / sec 430.02 / sec 129006 Name Highest Rate Total number error_abort 0.5 / sec 1 情况差不多，Flask服务器表现还算稳定，那么尝试增加后台Flask服务器数量（通过多端口实现）： 1234python manage.py runserver --port=8001python manage.py runserver --port=8002python manage.py runserver --port=8003python manage.py runserver --port=8004 使用tsung进行压测，压力500，4个Flask服务器： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.18 sec 32.57 msec 3510.1 / sec 639.92 / sec 0.11 sec 195154 page 0.49 sec 85.30 msec 3512.1 / sec 639.07 / sec 0.35 sec 194856 request 0.49 sec 85.30 msec 3512.1 / sec 639.07 / sec 0.35 sec 194856 Code Highest Rate Mean Rate Total number 200 3510.1 / sec 639.50 / sec 194986 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 这个效果妥妥的。 使用tsung进行压测，压力1000，4个Flask服务器： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 32.63 msec 2983.8 / sec 492.94 / sec 98.56 msec 150793 page 0.57 sec 90.00 msec 2976.4 / sec 491.31 / sec 0.40 sec 150275 request 0.57 sec 90.00 msec 2976.4 / sec 491.31 / sec 0.40 sec 150275 Code Highest Rate Mean Rate Total number 200 2981.4 / sec 488.92 / sec 149556 502 92.5 / sec 4.02 / sec 925 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 开始有一些502的超时错误了。 使用tsung进行压测，压力1000，4个tornado服务器： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.18 sec 86.24 msec 2052.1 / sec 693.82 / sec 0.14 sec 208786 page 0.52 sec 0.24 sec 2060.7 / sec 693.34 / sec 0.45 sec 208606 request 0.52 sec 0.24 sec 2060.7 / sec 693.34 / sec 0.45 sec 208606 Code Highest Rate Mean Rate Total number 200 2056.6 / sec 693.67 / sec 208703 在并发1000的情况下，是否使用tornado托管Flask效果差不多。 结论根据上述测试，直接使用Flask服务器的话，由于并发处理较弱，会有各种超时或者连接拒绝的错误。通过搭配Nginx来进行缓冲，通过增加后端服务器数来提供并发处理量。 所以最终选择了Nginx+后台4个Flask服务器的方式。由于目前Flask项目全体用户只有几千，目前并发情况很低，该方式完全满足使用。 如果在更大型项目中，并发上万，建议还是考虑想办法迁移至Liunx环境，通过官方建议的方式部署。]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>nginx</tag>
        <tag>tornado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下安装富士施乐打印机]]></title>
    <url>%2FMac%E4%B8%8B%E5%AE%89%E8%A3%85%E5%AF%8C%E5%A3%AB%E6%96%BD%E4%B9%90%E6%89%93%E5%8D%B0%E6%9C%BA.html</url>
    <content type="text"><![CDATA[背景打印机在windows环境下安装还是很方便的，在mac下，一路默认安装会有点问题，记录一下。 安装 下载打印机驱动：http://onlinesupport.fujixerox.com/setupSupport.do?cid=3&amp;ctry_code=CN&amp;lang_code=zh_CN 安装打印机驱动，一路默认就好了 打开Mac的 系统偏好设置，点击打印机与扫描仪 输入打印机的IP地址，然后点击添加，注意“协议” 要修改为“HP Jetdirect-Socket”。 点击添加即可 注意Mac在安装打印机时，默认的协议”互联网打印协议-IPP”，如下图所示。 按照默认的协议，也会自动识别到打印机型号，但是点击添加之后，会有报错： 如果继续添加，虽然可以添加成功，但实际打印时就会打印机无响应。]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下使用tree命令展示文件树]]></title>
    <url>%2FMac%E4%B8%8B%E4%BD%BF%E7%94%A8tree%E5%91%BD%E4%BB%A4%E5%B1%95%E7%A4%BA%E6%96%87%E4%BB%B6%E6%A0%91.html</url>
    <content type="text"><![CDATA[背景在写代码文档的时候，经常会用到展示项目架构，这时候如果可以有命令直接打印出目录树那就再好不过了，免的截图了。 思路网上找了下，果然是有这种工具的，Mac - tree命令。 Mac默认是没有tree命令的，需要手工安装下： 1brew install tree 安装好之后，看下帮助文档： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364$ tree --helpusage: tree [-acdfghilnpqrstuvxACDFJQNSUX] [-H baseHREF] [-T title ] [-L level [-R]] [-P pattern] [-I pattern] [-o filename] [--version] [--help] [--inodes] [--device] [--noreport] [--nolinks] [--dirsfirst] [--charset charset] [--filelimit[=]#] [--si] [--timefmt[=]&lt;f&gt;] [--sort[=]&lt;name&gt;] [--matchdirs] [--ignore-case] [--fromfile] [--] [&lt;directory list&gt;] ------- Listing options ------- -a All files are listed. -d List directories only. -l Follow symbolic links like directories. -f Print the full path prefix for each file. -x Stay on current filesystem only. -L level Descend only level directories deep. -R Rerun tree when max dir level reached. -P pattern List only those files that match the pattern given. -I pattern Do not list files that match the given pattern. --ignore-case Ignore case when pattern matching. --matchdirs Include directory names in -P pattern matching. --noreport Turn off file/directory count at end of tree listing. --charset X Use charset X for terminal/HTML and indentation line output. --filelimit # Do not descend dirs with more than # files in them. --timefmt &lt;f&gt; Print and format time according to the format &lt;f&gt;. -o filename Output to file instead of stdout. ------- File options ------- -q Print non-printable characters as '?'. -N Print non-printable characters as is. -Q Quote filenames with double quotes. -p Print the protections for each file. -u Displays file owner or UID number. -g Displays file group owner or GID number. -s Print the size in bytes of each file. -h Print the size in a more human readable way. --si Like -h, but use in SI units (powers of 1000). -D Print the date of last modification or (-c) status change. -F Appends '/', '=', '*', '@', '|' or '&gt;' as per ls -F. --inodes Print inode number of each file. --device Print device ID number to which each file belongs. ------- Sorting options ------- -v Sort files alphanumerically by version. -t Sort files by last modification time. -c Sort files by last status change time. -U Leave files unsorted. -r Reverse the order of the sort. --dirsfirst List directories before files (-U disables). --sort X Select sort: name,version,size,mtime,ctime. ------- Graphics options ------- -i Don't print indentation lines. -A Print ANSI lines graphic indentation lines. -S Print with CP437 (console) graphics indentation lines. -n Turn colorization off always (-C overrides). -C Turn colorization on always. ------- XML/HTML/JSON options ------- -X Prints out an XML representation of the tree. -J Prints out an JSON representation of the tree. -H baseHREF Prints out HTML format with baseHREF as top directory. -T string Replace the default HTML title and H1 header with string. --nolinks Turn off hyperlinks in HTML output. ------- Input options ------- --fromfile Reads paths from files (.=stdin) ------- Miscellaneous options ------- --version Print version and exit. --help Print usage and this help message and exit. -- Options processing terminator. 可以添加的参数很多，那么该用那些呢？ 在一个python项目中，先只加文件夹名看下： 1234567891011121314151617181920212223$ tree appapp├── __init__.py├── __pycache__│ └── __init__.cpython-37.pyc├── main│ ├── __init__.py│ ├── __pycache__│ │ ├── __init__.cpython-37.pyc│ │ ├── functions.cpython-37.pyc│ │ └── views.cpython-37.pyc│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-37.pyc │ ├── functions.cpython-37.pyc │ └── views.cpython-37.pyc ├── functions.py └── views.py5 directories, 14 files pyc是编译的临时文件，我们要把删掉，看下说明，可以用-I来： 12345678910111213141516$ tree -I *.pyc appapp├── __init__.py├── __pycache__├── main│ ├── __init__.py│ ├── __pycache__│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── __pycache__ ├── functions.py └── views.py5 directories, 7 files __pycache__也是临时文件，也把删掉： 12345678910111213tree -I *.pyc -I __pycache__ appapp├── __init__.py├── main│ ├── __init__.py│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── functions.py └── views.py2 directories, 7 files 可以看出-I是可以加多个的，每个-I后面加一个pattern。 在上面的例子中，其实所有的.pyc文件都在__pychache__文件夹下，可以直接忽略该文件夹即可： 12345678910111213$ tree -I __pycache__ appapp├── __init__.py├── main│ ├── __init__.py│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── functions.py └── views.py2 directories, 7 files 那么如果只要文件夹的结构呢？-d参数 123456789$ tree -d appapp├── __pycache__├── main│ └── __pycache__└── module └── __pycache__5 directories 忽略__pycache__文件夹： 123456$ tree -d -I __pycache__ appapp├── main└── module2 directories 总结通过brew安装tree工具之后，即可在命令行中使用tree命令展示文件\文件夹目录树： 直接加对应的文件夹来展示某文件夹范围内的文件树 1$ tree app 使用-I参数来忽略不展示的文件或子文件夹，可添加多个-I 1$ tree -I *.pyc -I __pycache__ app 使用-d来仅展示文件夹树 1$ tree -d app 多参数可以混合使用 1$ tree -d -I __pycache__ app 更多的参数使用，可以在有需要的时候参考--help内容 1$ tree --help]]></content>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何优雅的在flask中记录log]]></title>
    <url>%2F%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E5%9C%A8flask%E4%B8%AD%E8%AE%B0%E5%BD%95log.html</url>
    <content type="text"><![CDATA[背景记录日志，在任何项目中，都是很重要的。在Flask项目中，即有Flask提供的logger可以用来记录log，也可以通过直接使用Python的logging模块自定义logger来记录。那么这两者是什么关系，又该怎么使用呢？ 思路 Python的logging模块 先看下对于logging模块的官方介绍 Loggers have the following attributes and methods. Note that Loggers are never instantiated directly, but always through the module-level function logging.getLogger(name). Multiple calls to getLogger() with the same name will always return a reference to the same Logger object. The name is potentially a period-separated hierarchical value, like foo.bar.baz (though it could also be just plain foo, for example). Loggers that are further down in the hierarchical list are children of loggers higher up in the list. For example, given a logger with a name of foo, loggers with names of foo.bar, foo.bar.baz, and foo.bam are all descendants of foo. The logger name hierarchy is analogous to the Python package hierarchy, and identical to it if you organise your loggers on a per-module basis using the recommended construction logging.getLogger(__name__). That’s because in a module, __name__ is the module’s name in the Python package namespace. https://docs.python.org/3/library/logging.html#logger-objects 上面主要告诉我们两点， 可以通过logging.getLogger(name)来获取一个logger，相同名字的logger，其实是同一个logger。 logger是通过name进行继承的，比如foo.bar就是foo 的子logger。就可以是实现我们通过配置一个rootLogger，然后直接使用rootLogger.sublogger来记录一下内容，而不需要单独再配置一遍。 当使用logging.getLogger(__name__)时，__name__就是这个模块所在的python package的namespace。 flask提供的logger 再看下flask中的logging模块： Flask uses standard Python logging. All Flask-related messages are logged under the &#39;flask&#39; logger namespace.Flask.logger returns the logger named &#39;flask.app&#39;, and can be used to log messages for your application. Depending on the situation, an extension may choose to log to app.logger or its own named logger. Consult each extension’s documentation for details. http://flask.pocoo.org/docs/1.0/logging/ 我们可以知道flask的logger就是一个标准的Python logging，它的命名是flask。我们既可以使用app.logger，也可以自己定义一个logger。 那么如何使用app.logger呢？ 有两种方式： 直接调用 12logger = logging.getLogger('flask.app')logger.info('flask.app') 使用Flask提供的接口 12from flask import current_appcurrent_app.logger.info('logged by current_app from main') 这里推荐还是使用第二种，current_app是一个单例，可以直接引用到app.logger。 通过修改app.logger的name，可以实现子logger的继承么？ 答案是否定的。 修改app.logger的name： 12# app/__init__.pyapp.logger.name = 'app' 然后在子模块中定义一个app.module的logger来记录： 123456789from flask import current_appimport logginglogger = logging.getLogger('app.module')@module.route('/test', methods=['GET'])def test(): logger.info('logged by app.module') current_app.logger.info('logged by current_app.logger') 输出结果： 12019-02-01 10:56:01,877 - Thread-2 - app - INFO - logged by current_app.logger 只有current_app.logger的输出。 修改app.logger的name是不是无效呢？ 我们把子模块中的logger的name修改为flask.app.module： 123456789from flask import current_appimport logginglogger = logging.getLogger('flask.app.module')@module.route('/test', methods=['GET'])def test(): logger.info('logged by flask.app.module') current_app.logger.info('logged by current_app.logger') 输出结果： 122019-02-01 11:00:10,944 - Thread-2 - flask.app.module - INFO - logged by flask.app.module2019-02-01 11:00:10,946 - Thread-2 - app - INFO - logged by current_app.logger 两个logger均输出了。 可见，通过修改app.logger.name可以在记录的时候显示为我们设置的名称，但实际上这个logger还是flask.app。 __name__的使用 在自定义logger的情况下，为了方便起见，我们可以利用__name__这个参数。 前面说到：当使用logging.getLogger(__name__)时，__name__就是这个模块所在的python package的namespace。 一般Flask的工厂模式结构如下： 12345678910app├── __init__.py├── main│ ├── __init__.py│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── functions.py └── views.py 那么我们在先在app.__init__中定义rootLogger，然后再在app.module.functions.py中定义子Logger，均使用logging.getLogger(__name__): 1234567891011121314# app.__init__.py 初始化rootloggerrootLogger = logging.getLogger(__name__) rootLogger.setLevel(logging.DEBUG) socketHandler = logging.handlers.SocketHandler('localhost',logging.handlers.DEFAULT_TCP_LOGGING_PORT) rootLogger.addHandler(socketHandler) rootLogger.setLevel(logging.DEBUG)# app.module.functions.pyimport logginglogger = logging.getLogger(__name__)def record_from_logging(): logger.info('logged by logging from __name__') 输出： 122019-02-01 12:18:34,743 - MainThread - app - INFO - register root logger by __name__2019-02-01 12:19:24,954 - Thread-4 - app.module.functions - INFO - logged by logging from __name__ 可以发现输出的logger.name就是所在的文件目录，logger之间的继承关系与整个程序包保持一致。 总结根据上面分析，那么怎么优雅的记录logger呢？ 如果没有对模块进行分logger记录要求的话。可以直接使用在程序初始化的时候配置app.logger（可以自行设置logger.name）。在模块中通过import current_app来记录： 123456789101112131415161718# app.__init__.pydef register_logging(app): app.logger.name = 'app' # logstash_handler stashHandler = logstash.LogstashHandler('app.config.get('ELK_HOST')', 'app.config.get('ELK_PORT')') app.logger.addHandler(stashHandler) # socket_handler socketHandler = logging.handlers.SocketHandler('localhost', logging.handlers.DEFAULT_TCP_LOGGING_PORT) app.logger.addHandler(socketHandler) # app.module.function.pyfrom flask import current_app@module.route('/test', methods=['GET'])def test(): current_app.logger.info('logging someting') return 'logged by current_app.logger' 输出效果： 122019-02-01 13:49:28,998 - Thread-2 - app - INFO - logged by current_app from main2019-02-01 13:49:38,346 - Thread-3 - app - INFO - logged by current_app of functions 注意: 对于current_app.logger的引用不能通过如下方式，会有RuntimeError的报错。 123456789101112from flask import current_applogger = current_app.logger## 异常 raise RuntimeError(_app_ctx_err_msg)RuntimeError: Working outside of application context.This typically means that you attempted to use functionality that neededto interface with the current application object in some way. To solvethis, set up an application context with app.app_context(). See thedocumentation for more information. 如果希望按自己的实际需求，对模块进行分logger记录要求的话。那么建议自己设置logger。 12345678910111213141516171819202122# app.__init__.pydef register_logging(): # set own root logger rootLogger = logging.getLogger(__name__) rootLogger.setLevel(logging.DEBUG) # socketHandler socketHandler = logging.handlers.SocketHandler('localhost',logging.handlers.DEFAULT_TCP_LOGGING_PORT) rootLogger.addHandler(socketHandler) # logstash_handler stashHandler = logstash.LogstashHandler('app.config.get('ELK_HOST')', 'app.config.get('ELK_PORT')') rootLogger.addHandler(stashHandler) rootLogger.setLevel(logging.DEBUG)# app.module.function.pyimport logginglogger = logging.getLogger(__name__)@module.route('/test', methods=['GET'])def test(): logger.info('logging someting') return 'logged by logging module' 输出效果： 122019-02-01 13:49:49,297 - Thread-5 - app.module.views - INFO - logged by flask.app.module2019-02-01 13:50:01,013 - Thread-7 - app.module.functions - INFO - logged by logging module of functions 完整代码可参考：https://github.com/keejo125/flask_logging_demo 注意关于python中logging的配置可参考官网： https://docs.python.org/3/library/logging.config.html?highlight=logging 在配置handler时，经常会希望日志可以按时间分割(TimedRotatingFileHandler)或者按大小分割(RotatingFileHandler). 但是在flask项目中，尤其开启多线程之后，在分割日志(doRollover())时会有文件读写的异常: 1WindowsError: [Error 32] 建议使用SocketHandler，将日志发送给单独的LogServer来进行二次处理。 简易的接收socketlog的LogServer可参考：https://github.com/keejo125/flask_logging_demo/blob/master/LogServer.py 或者现在流行的stashHandler，将日志发送给ELK来进行二次处理。]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>logging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL按时间统计数据]]></title>
    <url>%2FMySQL%E6%8C%89%E6%97%B6%E9%97%B4%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE.html</url>
    <content type="text"><![CDATA[背景在做数据库的统计时，经常会需要根据年、月、日来统计数据，然后配合echarts来制作可视化效果。 数据库：MySQL 思路 按照时间维度进行统计的前提是需要数据库中有保留时间信息，建议是使用MySQL自带的datetime类型来记录时间。 1`timestamp` datetime DEFAULT NULL, 在MySQL中对于时间日期的处理的函数主要是DATE_FORMAT(date,format)。可用的参数如下 格式 描述 %a 缩写星期名 %b 缩写月名 %c 月，数值 %D 带有英文前缀的月中的天 %d 月的天，数值(00-31) %e 月的天，数值(0-31) %f 微秒 %H 小时 (00-23) %h 小时 (01-12) %I 小时 (01-12) %i 分钟，数值(00-59) %j 年的天 (001-366) %k 小时 (0-23) %l 小时 (1-12) %M 月名 %m 月，数值(00-12) %p AM 或 PM %r 时间，12-小时（hh:mm:ss AM 或 PM） %S 秒(00-59) %s 秒(00-59) %T 时间, 24-小时 (hh:mm:ss) %U 周 (00-53) 星期日是一周的第一天 %u 周 (00-53) 星期一是一周的第一天 %V 周 (01-53) 星期日是一周的第一天，与 %X 使用 %v 周 (01-53) 星期一是一周的第一天，与 %x 使用 %W 星期名 %w 周的天 （0=星期日, 6=星期六） %X 年，其中的星期日是周的第一天，4 位，与 %V 使用 %x 年，其中的星期一是周的第一天，4 位，与 %v 使用 %Y 年，4 位 %y 年，2 位 注：当涉及到按日统计是，需要使用%j，而如果使用%d, %e, %w的话，那么不同月份/周里的相同值会统计在一起。 涉及到获取当前时间，则可以通过now()或者sysdate()来获取。 12SELECT SYSDATE() FROM DUAL;SELECT NOW() FROM DUAL; 按照实际需求使用group by查询即可。 结论需统计的表结构如下： 123456789CREATE TABLE `apilog` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(64) DEFAULT NULL, `action` varchar(64) DEFAULT NULL, `params` text, `result` text, `timestamp` datetime DEFAULT NULL, PRIMARY KEY (`id`)) 统计时间范围内不同分类action的数量 12345678# 当日SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%j&apos;) = DATE_FORMAT(now(),&apos;%j&apos;) ORDER BY count desc;# 当周SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%u&apos;) = DATE_FORMAT(now(),&apos;%u&apos;) ORDER BY count desc;# 当月SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%m&apos;) = DATE_FORMAT(now(),&apos;%m&apos;) ORDER BY count desc;# 当年SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%Y&apos;) = DATE_FORMAT(now(),&apos;%Y&apos;) ORDER BY count desc; 统计某分类action的时间维度数量 12345678# 按日SELECT action, DATE_FORMAT(`timestamp`,&apos;%j&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%j&apos;)# 按周SELECT action, DATE_FORMAT(`timestamp`,&apos;%u&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%u&apos;)# 按月SELECT action, DATE_FORMAT(`timestamp`,&apos;%m&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%m&apos;)# 按年SELECT action, DATE_FORMAT(`timestamp`,&apos;%Y&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%Y&apos;) 同时按action和时间维度统计 12345678# 按日SELECT action, DATE_FORMAT(`timestamp`,&apos;%j&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%j&apos;)# 按周SELECT action, DATE_FORMAT(`timestamp`,&apos;%u&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%u&apos;)# 按月SELECT action, DATE_FORMAT(`timestamp`,&apos;%m&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%m&apos;)# 按年SELECT action, DATE_FORMAT(`timestamp`,&apos;%Y&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%Y&apos;) 以上就是比较常用的时间统计了，更多的时间维度，可以参考上面的参数表类似处理即可。]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的KeyError异常处理]]></title>
    <url>%2FPython%E4%B8%AD%E7%9A%84KeyError%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86.html</url>
    <content type="text"><![CDATA[背景在检查web服务器日志的时候，发现有KeyError的异常报错。检查了下出错的代码： 1applyId = session['applyId'] 应该是用户首次访问时，session为空，所以就获取异常了。 思路根据上面的情况，KeyError就是在获取dict中不存在的key值时触发的。那么有解决方案就有两种： 在读取key值的时候，先校验一下是否存在改key： 1234567d = &#123;'a': 1, 'b': 2, 'c': 3&#125;if 'd' in d: print(d['d'])else: print('not exists!') 输出： 1not exists! 在读取dict时，设置default值。这时候需要使用dict内置的get(key[,default])方法： 1234d = &#123;'a': 1, 'b': 2, 'c': 3&#125;print(d.get('d', 'not exits!')) 输出： 1not exists! 结论最后采用了方法二，如果没有该key，就设置默认为空&#39;&#39;。 1applyId = session.get(&apos;applyId&apos;, &apos;&apos;) 后续再涉及到dict的时候都要注意下默认值的处理，避免出现KeyError的方法还有其他几种方法，可以参考如下的链接，会比较麻烦一点： https://blog.csdn.net/u011089523/article/details/72887163/]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests发请求时timeout配置及异常捕获]]></title>
    <url>%2Frequests%E5%8F%91%E8%AF%B7%E6%B1%82%E6%97%B6timeout%E9%85%8D%E7%BD%AE%E5%8F%8A%E5%BC%82%E5%B8%B8%E6%8D%95%E8%8E%B7.html</url>
    <content type="text"><![CDATA[背景今天有用户在访问web系统时，出现了Nginx返回的超时报错。经排查是由于某台服务器异常，导致web系统requests请求时，一直等待响应，等待时间超过了Nginx配置的超时时间，所以Nginx就直接返回了。 思路 配置timeout 一般在使用requests库时，是不设置超时时间的，那么请求就会一直等待，直到某一方关闭。所以在发请求的时候手工指定下timeout参数。 requests的timeout配置如下： 12345678def request(method, url, **kwargs): """Constructs and sends a :class:`Request &lt;Request&gt;`.... :param timeout: (optional) How many seconds to wait for the server to send data before giving up, as a float, or a :ref:`(connect timeout, read timeout) &lt;timeouts&gt;` tuple. :type timeout: float or tuple... 官网介绍如下： 如果你制订了一个单一的值作为timeout，如下所示： 1r = requests.get('https://github.com', timeout=5) 这一timeout值将会用作 connect和 read二者的 timeout。如果要分别制定，就传入一个元组： 1r = requests.get('https://github.com', timeout=(3.05, 27)) 如果远端服务器很慢，你可以让 Request 永远等待，传入一个None作为timeout值，然后就冲咖啡去吧。 1r = requests.get('https://github.com', timeout=None) http://docs.python-requests.org/zh_CN/latest/user/advanced.html#timeout 捕获异常 通过try... except...可以捕获异常。获取的异常信息如下： 在老版本的python中，有e.message可以获取str格式的报错信息，但要注意的是，虽然在定义中e.message是str类型的，但是实际上未必。类型错误的话，在后续的处理中就可能会造成其他异常了。 1234567891011121314151617181920# e.message 定义class BaseException(object):... args = property(lambda self: tuple()) """:type: tuple""" message = property(lambda self: '', lambda self, v: None, lambda self: None) """:type: string"""...# 输入try: requests.get('http://122.248.19.3', timeout=0.001)except requests.exceptions.ReadTimeout as e: print(e.message) print(type(e.message))# 输出HTTPConnectionPool(host='122.248.19.3', port=80): Read timed out. (read timeout=0.001)&lt;class 'requests.packages.urllib3.exceptions.ReadTimeoutError'&gt; 而在新版本的python中，e.message已经被淘汰了。 1234567891011121314151617181920212223242526272829303132333435363738394041class BaseException(object): """Superclass representing the base of the exception hierarchy. The __getitem__ method is provided for backwards-compatibility and will be deprecated at some point. The 'message' attribute is also deprecated. """ def __init__(self, *args): self.args = args def __str__(self): return str(self.args[0] if len(self.args) &lt;= 1 else self.args) def __repr__(self): func_args = repr(self.args) if self.args else "()" return self.__class__.__name__ + func_args def __getitem__(self, index): """Index into arguments passed in during instantiation. Provided for backwards-compatibility and will be deprecated. """ return self.args[index] def _get_message(self): """Method for 'message' property.""" warnings.warn("the 'message' attribute has been deprecated " "since Python 2.6") return self.args[0] if len(args) == 1 else '' message = property(_get_message, doc="access the 'message' attribute; " "deprecated and provided only for " "backwards-compatibility") https://www.python.org/dev/peps/pep-0352/#transition-plan 所以可以用e.args来获取异常说明，e.args返回的是一个tuple，直接通过str(e.args)转换成str类型做后续处理。 结论代码如下，对不通原因造成的异常，可以加不同的except。 12345678910import requeststry: r = requests.get('https://github.com', timeout=5)except requests.exceptions.ConnectTimeout as e: msg = str(e.args) # do sth with msgexcept requests.exceptions.ReadTimeout as e: msg = str(e.args) # do sth with msg]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取知乎种某问题的所有答案]]></title>
    <url>%2F%E8%8E%B7%E5%8F%96%E7%9F%A5%E4%B9%8E%E7%A7%8D%E6%9F%90%E9%97%AE%E9%A2%98%E7%9A%84%E6%89%80%E6%9C%89%E7%AD%94%E6%A1%88.html</url>
    <content type="text"><![CDATA[背景知乎是一个比较出名也很有趣的网站，里面很多问题和回答也很有意思。之前看了一些爬虫相关文章经常会以抓取知乎来做一些分析。本次也尝试使用python抓取知乎某问题的全部答案。 思路使用爬虫抓取数据其实主要还是要先弄清楚网页展示的方式，现在大部分网页是基于模板动态生成，具体数据通过json等方式传递，这样的话，其实我们只需要直接通过请求抓取json部分即可，而不需要通过获取整个html然后在分析抓取需要部分。 先随便打开一个知乎首页的问题，比如： 通过在chrome里F12查看网络加载可以发现，有一个answer?相关的请求，在右边的Preview里看下返回的数据，果然答案就在里面。 再看仔细分析下返回的这个json结构： 每个json数据中data字段都含有多个答案体 每个答案体里，具体内容存在于content字段，而该字段是一段html，需要使用BeautifulSoup来解析。 每个json数据中paging字段包含了返回的数据是否是这个问题的起止答案，并且给除了上一批答案的请求地址以及下一批答案的请求地址，和总答案数。 于是可以通过随便获取一个请求地址，然后在data字段中获取答案，并根据paging字段来迭代获取其他的答案。 具体实现根据上述的思路，那么实现起来就比较简单了 先获取手动获取感兴趣的问题的网址 在chrome的F12中找到对应的答案请求地址（开头为https://www.zhihu.com/api/v4/questions/） 通过发起请求获取返回的json数据 在返回的json数据中，遍历data字段，并使用·BeautifulSoup来解析出答案 在返回的json数据中，读取paging字段，获取上一个请求地址或者下一个请求地址 迭代请求，直到结束 具体的实现，可以参考我的github，https://github.com/keejo125/。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式中(pattern)和(?:pattern)的使用]]></title>
    <url>%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B8%AD-pattern-%E5%92%8C-pattern-%E7%9A%84%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[背景在项目中有这样一个需求，在页面提交时，需要验证用户输入的网络端口地址是否符合要求。合法的规则如下： 数字端口：123 udp端口：123udp 范围端口：123-234 udp范围端口：123-234udp 多端口使用;分割：123;123udp;123-234;123-234udp 思路 1、单个端口匹配： 数字端口：\d+ udp端口：\d+udp 范围端口：\d+\-\d+ udp范围端口：\d+\-\d+udp 2、多端口匹配 先使用(pattern)来匹配单个端口，然后(pattern)+就可以来匹配多端口的情况了。由于多端口使用;分割，那么每个pattern的开头有两种情况：字符串起始或者;。写成正则就是(^|;)。 将第一步中4中单端口情况使用|并列起来，就有了如下的正则： 1(^|;)(\d+|\d+udp|\d+\-\d+|\d+\-\d+udp) 多端口： 1((^|;)(\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+ 匹配到结尾： 1((^|;)(\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+$ 3、(?:)的使用 在正则中，通过增加?:，使(pattern)变成(?:pattern)，可以实现匹配效果不变，但是不捕获匹配到的内容，从而提升代码的效率。那么上述的正则就变成了： 1(?:(?:^|;)(?:\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+$ 结论在页面中，使用javascript就是： 123var pattern = /^(?:(?:^|;)(?:\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+$/str = '123;123udp;123-234;123-234udp'console.log(pattern.test(str)) 拓展在正则中(pattern)和(?:pattern)的描述如下表，在仅进行规则匹配而不需要获取匹配到的内容的时候，建议使用(?:pattern)。 字符 描述 (pattern) 匹配 pattern 并获取这一匹配。所获取的匹配可以从产生的 Matches 集合得到，在VBScript 中使用 SubMatches 集合，在JScript 中则使用 0…0…9 属性。 (?:pattern) 匹配 pattern 但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用 “或” 字符 (&#124;) 来组合一个模式的各个部分是很有用。例如， industr(?:y&#124;ies) 就是一个比 ‘industry&#124;industries’ 更简略的表达式。 还有其他更多的符号含义可参考： https://www.cnblogs.com/richiewlq/p/7308005.html]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批量获取AD计算机名信息]]></title>
    <url>%2F%E6%89%B9%E9%87%8F%E8%8E%B7%E5%8F%96AD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%90%8D%E4%BF%A1%E6%81%AF.html</url>
    <content type="text"><![CDATA[背景由于用户加域时需要制定计算机名，为了规范起见，计算机名与AD账号有严格的对应关系。对于一些公共账号来说，就会有很多计算机名。现在需要根据该计算机名的登录时间来筛选出一些废弃计算机名，然后做删除处理，以释放计算机名。 思路在AD管理工具（Active Directory 用户与计算机）中是可以查询计算机名的，并且在计算机的属性中可以查看创建时间和修改时间的。 那么用命令应该就可以批量获取了。AD获取信息的命令为dsget，通过\h获取帮助。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&gt;dsget \h说明: 该工具的命令显示目录中具体对象选择的属性。dsget 命令:dsget computer - 显示目录中计算机的属性。dsget contact - 显示目录中联系人的属性。dsget subnet - 显示目录中子网的属性。dsget group - 显示目录中组的属性。dsget ou - 显示目录中组织单位的属性。dsget server - 显示目录中服务器的属性。dsget site - 显示目录中站点的属性。dsget user - 显示目录中用户的属性。dsget quota - 显示目录中配额的属性。dsget partition - 显示目录中分区的属性。要显示目录中所给对象属性的任意集，请使用 dsquery * 命令 (参见以下示例)。要获取一个具体的命令，请键入 "dsget &lt;ObjectType&gt; /?"，这里的 &lt;ObjectType&gt;是以上显示的一个受支持的对象类型。例如，dsget ou /?。备注:dsget 命令有助于查看目录中特定对象的属性:dsget 的输入是一个对象，输出则是该对象的一系列属性。若要查找满足给定搜索标准的所有对象，请使用 dsquery 命令(dsquery /?)。dsget 命令支持输入管道，以允许您通过管道输入 dsquery 命令的结果，作为 dsget 命令的输入，然后显示 dsquery 命令所找到对象的详细信息。可分辨名称中不用作分隔符的逗号必须用反斜杠("\")字符转义(例如，"CN=Company\, Inc.,CN=Users,DC=microsoft,DC=com")。用在可分辨名称中的反斜杠必须用一个反斜杠转义(例如，"CN=Sales\\ Latin America,OU=Distribution Lists,DC=microsoft,DC=com")。示例:查找姓名以 "John" 开始的所有用户并显示他们的办公室号码: dsquery user -name John* | dsget user -office显示对象的 sAMAccountName、userPrincipalName 和 department 属性，该对象的 DN 是 ou=Test,dc=microsoft,dc=com: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr sAMAccountName userPrincipalName department读取使用 dsquery * 命令的任何对象的所有属性。例如，读取其 DN 为 ou=Test,dc=microsoft,dc=com的对象的所有属性: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr *目录服务命令行工具可帮助:dsadd /? - 帮助添加对象。dsget /? - 帮助显示对象。dsmod /? - 帮助修改对象。dsmove /? - 帮助移动对象。dsquery /? - 帮助查找匹配搜索标准的对象。dsrm /? - 帮助删除对象。dsget 成功 这里我们需要获取计算机名的信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133&gt;dsget computer /?描述: 显示目录中计算机的属性。此命令有两种用法。第一种用法允许您 查看多个计算机的属性。第二种用法允许您查看一个计算机成员身 份的信息。语法: dsget computer &lt;ComputerDN ...&gt; [-dn] [-samid] [-sid] [-desc] [-loc] [-disabled] [&#123;-s &lt;Server&gt; | -d &lt;Domain&gt;&#125;] [-u &lt;UserName&gt;] [-p &#123;&lt;Password&gt; | *&#125;] [-c] [-q] [-l] [&#123;-uc | -uco | -uci&#125;] [-part &lt;PartitionDN&gt; [-qlimit] [-qused]] dsget computer &lt;ComputerDN&gt; [-memberof [-expand]] [&#123;-s &lt;Server&gt; | -d &lt;Domain&gt;&#125;] [-u &lt;UserName&gt;] [-p &#123;&lt;Password&gt; | *&#125;] [-c] [-q] [-l] [&#123;-uc | -uco | -uci&#125;]参数:值 描述&lt;ComputerDN ...&gt; 必需项/stdin。要查看的一台或多台计算机 的可分辨名称(DN)。 如果省略了目标对象，则会从标准 输入(stdin)中读取这些对象，以支持 通过管道将其他命令的输出 用作此命令的输入。 请与以下的 &lt;ComputerDN&gt; 相比。-dn 显示计算机 DN。-samid 显示计算机的 SAM 帐户名。-sid 显示计算机的安全 ID(SID)。-desc 显示计算机的描述。-loc 显示计算机的位置。-disabled 显示计算机帐户是(yes)否(no) 被禁用。&lt;ComputerDN&gt; 必需项。要查看计算机的 可分辨名称(DN)。-memberof 显示计算机所属的组。-expand 显示计算机所属组的循环 扩展列表。此选项采用 计算机直属组成员列表 并递归扩展该列表中 的每个组，以决定其组成员 身份和获得组的完整集。&#123;-s &lt;Server&gt; | -d &lt;Domain&gt;&#125; -s &lt;Server&gt; 用 &lt;Server&gt; 名称连接到 AD DC/LDS 实例。 -d &lt;Domain&gt; 连接到域 &lt;Domain&gt; 中的 AD DC。 默认: 登录域中的 AD DC。-u &lt;UserName&gt; 以 &lt;UserName&gt; 身份连接。默认: 登录的用户。 用户名可以采用: 用户名、域\用户名 或用户主体名称(UPN)。-p &#123;&lt;Password&gt; | *&#125; 用户 &lt;UserName&gt; 的密码。如果是 *， 则会提示您输入密码。-c 连续操作模式: 指定了多个目标对象时，将 报告错误，但继续处理参数列表中的 下一个对象。若无此选项，命令将在遇到 第一个错误时退出。-q 安静模式: 将所有输出抑制到标准输出。-L 以列表格式显示搜索结果集中的项目。 默认: 表格格式。&#123;-uc | -uco | -uci&#125; -uc 指定来字管道的输入或至管道的输出 用 Unicode 格式。 -uco 指定至管道或文件的输出 用 Unicode 格式。 -uci 指定来自管道或文件的输入 用 Unicode 格式。-part &lt;PartitionDN&gt; 用 &lt;PartitionDN&gt; 的可分辨名称 连接到目录分区。-qlimit 显示计算机在指定目录分区中 的有效配额。-qused 显示计算机在指定目录分区中的 已使用配额。备注:如果您在命令提示符处没有提供目标对象，则会从标准输入(stdin)中获取目标对象。可以通过键盘、重定向文件或另一个命令的管道输出接受 Stdin 数据。若要通过键盘或在重定向文件中标记 stdin 数据的结束，请使用 Control+Z 表示文件结束(EOF)。配额规定决定一个给定安全主体在一个特定目录分区中能够拥有的最大目录对象数。dsget 命令帮助您查看目录中某个特定对象的属性: dsget 的输入是一个对象，输出是该对象的属性列表。若要查找满足所给搜索条件的所有对象，请使用 dsquery 命令(dsquery /?)。如果您提供的值包含空格，请在文本两边使用引号(例如，"CN=DC2,OU=Domain Controllers,DC=microsoft,DC=com")。如果您输入了多个值，这些值必须用空格隔开(例如，一个系列可分辨名称)。示例:查找在给定 OU 中名称以 "tst" 开头的所有计算机并显示其说明。 dsquery computer ou=Test,dc=microsoft,dc=com -name tst* | dsget computer -desc显示给定计算机 "MyDBServer" 所属的组的列表(以递归方式展开): dsget computer cn=MyDBServer,cn=computers,dc=microsoft,dc=com -memberof -expand要显示给定计算机 "MyDBServer" 在给定分区 "cn=domain1,dc=microsoft,dc=com" 上的有效配额和已用配额，请键入: dsget computer cn=MyDBServer,cn=computers,dc=microsoft,dc=com -part cn=domain1,dc=microsoft,dc=com -qlimit -qused另请参阅:dsget - 描述适用于所有命令的参数。dsget computer - 显示目录中计算机的属性。dsget contact - 显示目录中联系人的属性。dsget subnet - 显示目录中子网的属性。dsget group - 显示目录中组的属性。dsget ou - 显示目录中组织单位的属性。dsget server - 显示目录中服务器的属性。dsget site - 显示目录中站点的属性。dsget user - 显示目录中用户的属性。dsget quota - 显示目录中配额的属性。dsget partition - 显示目录中分区的属性。目录服务命令行工具帮助:dsadd /? - 有关添加对象的帮助。dsget /? - 有关显示对象的帮助。dsmod /? - 有关修改对象的帮助。dsmove /? - 有关移动对象的帮助。dsquery /? - 有关查找符合搜索条件的对象的帮助。dsrm /? - 有关删除对象的帮助。dsget 成功 可以看到，能显示的计算机信息如下： 12345678910111213141516-dn 显示计算机 DN。-samid 显示计算机的 SAM 帐户名。-sid 显示计算机的安全 ID(SID)。-desc 显示计算机的描述。-loc 显示计算机的位置。-disabled 显示计算机帐户是(yes)否(no) 被禁用。&lt;ComputerDN&gt; 必需项。要查看计算机的 可分辨名称(DN)。-memberof 显示计算机所属的组。-expand 显示计算机所属组的循环 扩展列表。此选项采用 计算机直属组成员列表 并递归扩展该列表中 的每个组，以决定其组成员 身份和获得组的完整集。 那么修改日期的信息就只能通过AD工具来查看了。 再看下dsquery命令会不会有戏： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&gt;dsquery /?描述: 该工具的命令集允许您根据指定的标准查询目录。除 dsquery * 之外 (dsquery * 可以查询任何类型的对象)，以下每一个 dsquery 命令均可查找一个特定对象类型:dsquery computer - 查找目录中的计算机。dsquery contact - 查找目录中的联系人。dsquery subnet - 查找目录中的子网。dsquery group - 查找目录中的组。dsquery ou - 查找目录中的组织单位。dsquery site - 查找目录中的站点。dsquery server - 查找目录中的 AD DC/LDS 实例。dsquery user - 查找目录中的用户。dsquery quota - 查找目录中的配额规定。dsquery partition - 查找目录中的分区。dsquery * - 用通用的 LDAP 查询来查找目录中的任何对象。若要查找特定命令的帮助，请键入 "dsquery &lt;ObjectType&gt; /?"，其中&lt;ObjectType&gt; 是以上所示的受支持对象类型之一。例如，dsquery ou /?。备注:dsquery 命令帮助您查找目录中与指定搜索标准匹配的对象: dsquery 的输入是一个搜索标准，其输出是与该搜索匹配的一系列对象。若要获取特定对象的属性，请使用 dsget 命令(dsget /?)。可以将 dsquery 命令的结果通过管道输出，作为一个其他目录服务命令行工具(如 dsmod、dsget、dsrm 或 dsmove)的输入。可分辨名称中不是用作分隔符的逗号必须用反斜杠("\")字符转义(例如，"CN=Company\, Inc.,CN=Users,DC=microsoft,DC=com")。用在可分辨名称中的反斜杠必须用一个反斜杠转义(例如，"CN=Sales\\ Latin America,OU=Distribution Lists,DC=microsoft,DC=com")。示例:查找过去四个星期内处于非活动状态的计算机并将其从目录中删除: dsquery computer -inactive 4 | dsrm查找组织单位所有的用户 "ou=Marketing,dc=microsoft,dc=com" 并将他们添加到Marketing Staff 组: dsquery user ou=Marketing,dc=microsoft,dc=com | smod group "cn=Marketing Staff,ou=Marketing,dc=microsoft,dc=com" -addmbr查找姓名以 "John" 开始的所有用户并显示他的办公室号码: dsquery user -name John* | dsget user -office要显示目录中所给对象属性的任意集，请使用 dsquery * 命令。例如，要显示对象(该对象的 DN 是 ou=Test，dc=microsoft，dc=com) 的 sAMAccountName，userPrincipalName 和 department 属性: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr sAMAccountName userPrincipalName department要读取对象(该对象的 DN 是 ou=Test，dc=microsoft，dc=com) 的所有属性: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr *目录服务命令行工具帮助:dsadd /? - 添加对象的帮助。dsget /? - 显示对象的帮助。dsmod /? - 修改对象的帮助。dsmove /? - 移动对象的帮助。dsquery /? - 查找与搜索标准匹配对象的帮助。dsrm /? - 删除对象的帮助。 虽然没有直接我们想要的，但是其中一个例子却很有启发： 123查找过去四个星期内处于非活动状态的计算机并将其从目录中删除: dsquery computer -inactive 4 | dsrm 没办法通过命令来回去修改时间，却可以直接根据非活动状态的时间选出计算机名。 那么我们最终想要的效果——获取长时间未活动的计算机名，就可以直接得到了，而不需要另行计算了。 结论根据上面的思路，可以使用dsquery命令 -name test*来筛选包含test 所有计算机名 -inactive 24来筛选过去24周，也就是半年非活动的计算机名 完整命令如下： 1dsquery computer -name TEST* -inactive 24]]></content>
      <categories>
        <category>操作系统</category>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>ad</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery4.2在Python3.7下无法运行的问题]]></title>
    <url>%2FCelery4-2%E5%9C%A8Python3-7%E4%B8%8B%E6%97%A0%E6%B3%95%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景之前使用Flask + Celery + Redis来实现异步队列处理，使用的环境是python3.6，后来由于Mac系统下，使用brew安装的python，直接升级到了python3.7，相同的程序运行就报错了。 问题报错情况提示如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445[2019-01-08 23:15:05,188: CRITICAL/MainProcess] Unrecoverable error: SyntaxError('invalid syntax', ('/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/backends/redis.py', 22, 19, 'from . import async, base\n'))Traceback (most recent call last): File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/kombu/utils/objects.py", line 42, in __get__ return obj.__dict__[self.__name__]KeyError: 'backend'During handling of the above exception, another exception occurred:Traceback (most recent call last): File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/worker/worker.py", line 205, in start self.blueprint.start(self) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/bootsteps.py", line 115, in start self.on_start() File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/apps/worker.py", line 139, in on_start self.emit_banner() File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/apps/worker.py", line 154, in emit_banner ' \n', self.startup_info(artlines=not use_image))), File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/apps/worker.py", line 217, in startup_info results=self.app.backend.as_uri(), File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/kombu/utils/objects.py", line 44, in __get__ value = obj.__dict__[self.__name__] = self.__get(obj) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/base.py", line 1196, in backend return self._get_backend() File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/base.py", line 914, in _get_backend self.loader) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/backends.py", line 70, in by_url return by_name(backend, loader), url File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/backends.py", line 50, in by_name cls = symbol_by_name(backend, aliases) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/kombu/utils/imports.py", line 56, in symbol_by_name module = imp(module_name, package=package, **kwargs) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/importlib/__init__.py", line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) File "&lt;frozen importlib._bootstrap&gt;", line 1006, in _gcd_import File "&lt;frozen importlib._bootstrap&gt;", line 983, in _find_and_load File "&lt;frozen importlib._bootstrap&gt;", line 967, in _find_and_load_unlocked File "&lt;frozen importlib._bootstrap&gt;", line 677, in _load_unlocked File "&lt;frozen importlib._bootstrap_external&gt;", line 724, in exec_module File "&lt;frozen importlib._bootstrap_external&gt;", line 860, in get_code File "&lt;frozen importlib._bootstrap_external&gt;", line 791, in source_to_code File "&lt;frozen importlib._bootstrap&gt;", line 219, in _call_with_frames_removed File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/backends/redis.py", line 22 from . import async, base ^SyntaxError: invalid syntax 结论这个错误有点奇怪，经过一番百度谷歌，终于发现问题的原因。由于celery中有一个文件名命名为async，而在python3.7中，新增两个关键字，其中一个恰好就是async，另一个是await。 https://docs.python.org/3/whatsnew/3.7.html celery的作者也在issue中表示，后续版本会将async这个文件改为asynchronous，但目前版本还未发布（可能会在4.2.2）。现在可以通过github直接安装新版，pipenv环境下安装如下： 1234567pipenv install https://github.com/celery/celery/tarball/masterInstalling https://github.com/celery/celery/tarball/master…✔ Installation Succeeded Pipfile.lock (d4f0f1) out of date, updating to (dccb00)…Locking [dev-packages] dependencies…Locking [packages] dependencies…✔ Success! https://github.com/celery/celery/issues/4849 拓展flask + celery + redis 的单文件和工厂模式demo可参考如下git： https://github.com/keejo125/flask_celery_redis_demo]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[巧用kill重新加载配置并启动进程]]></title>
    <url>%2F%E5%B7%A7%E7%94%A8kill%E9%87%8D%E6%96%B0%E5%8A%A0%E8%BD%BD%E9%85%8D%E7%BD%AE%E5%B9%B6%E5%90%AF%E5%8A%A8%E8%BF%9B%E7%A8%8B.html</url>
    <content type="text"><![CDATA[背景前期在服务器上使用gunicorn托管了一个flask项目，近期修改了配置要重启一下。于是查了下如何优雅的重启进程。 思路修改配置重启进程，最简单的方法就是使用ps -ef|grep xxx命令来找到对应的进程，然后kill -9 pid来结束进程在重新启动。 对于gunicorn来说，稍微有点不同。一般会启动多个worker来跑，比如 1gunicorn -w 4 manager:app 这样的话，使用平常的ps -ef|grep gunicorn就会发现有多个进程，有时候直接kill后还会自动启动。 正确的方法是： 通过pstree来找到gunicorn 的主进程： 1234567$ pstree -ap|grep gunicorn | |-grep,18737 --color=auto gunicorn | `-gunicorn,18205/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | |-gunicorn,17455/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | |-gunicorn,17456/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | |-gunicorn,17457/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | `-gunicorn,17458/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi 这里就会看到，主进程是18737 然后在通过kill命令来结束进程 1kill -9 18737 在启动gunicorn。 这时候发现了有另外一个kill命令，可以直接让进程重新加载配置并启动，这就是 1kill -HUP 18737 重启之后再次使用pstree会发现guncorn的进程号发生了变化，即原进程已经销毁，新建了新的进程。 http://www.chenxm.cc/article/561.html 结论经查询，这其实是liunx中带信号的kill命令起的作用。上述命令其实是对进程发送了一个HUP信号，而很多程序会把HUP信号作为重新读取配置文件的触发条件，当接收到这个信号的时候，不会杀死进程，而是重新读取配置并运行。 1234567891011121314# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX 通过上述命令可以看到SIGHUP是1号信号，也就是说kill -1 pid和kill -HUP pid是等效的。 在logstash中，我们使用kill -1 pid 来实现重新加载配置，其实也是这个道理。]]></content>
      <categories>
        <category>操作系统</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>gunicorn</tag>
        <tag>logstash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript实现网页倒计时并跳转页面]]></title>
    <url>%2FJavaScript%E5%AE%9E%E7%8E%B0%E7%BD%91%E9%A1%B5%E5%80%92%E8%AE%A1%E6%97%B6%E5%B9%B6%E8%B7%B3%E8%BD%AC%E9%A1%B5%E9%9D%A2.html</url>
    <content type="text"><![CDATA[背景之前想自己从头搭建一个个人博客，后来各种原因直接用了hexo并托管在coding上，效果也不错。于是打算在个人服务器上直接建一个倒计时跳转页面，转到hexo， 也不浪费自己买的域名，哈哈哈。本来想直接跳转，感觉还是有一个倒计时提醒比较好一点。 思路 首先先写一个html5的简单的文字页面就好，留出一个div放倒计时的数字就好了。 然后JavaScript中可以用setInterval来实现按周期调用函数功能，也就是倒计时，每过一秒，数字减1。 setInterval()方法是按照指定的周期（毫秒）来调用函数或计算表达式。 12// 每三秒（3000 毫秒）弹出 "Hello" :setInterval(function()&#123; alert("Hello"); &#125;, 3000); http://www.runoob.com/jsref/met-win-setinterval.html 页面跳转就比较简单了location.href=就可以实现跳转了。 最后整个函数需要在页面加载完成之后进行，也就是放在window.onload里。 window.onload事件会在页面或者图像加载完成后立刻发生，通常用于&lt;body&gt;元素。 1234// html中&lt;body onload="SomeJavaScriptCode"&gt;// js中window.onload=function()&#123;SomeJavaScriptCode&#125;; http://www.runoob.com/jsref/event-onload.html 实现根据上面的思路，实现就比较简单了，在页面部分显示倒计时数字的div设置了id=&quot;time&quot;，在onload事件中，每隔1000毫秒调用一次函数使倒计时数字减1，当倒计时结束时跳转页面。 123456789101112131415161718192021222324252627282930313233343536&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;图兰登&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt; &lt;div style="text-align: center;"&gt; &lt;h1&gt;你好，这里是图兰登。&lt;/h1&gt; &lt;h3&gt;欢迎进入我的个人博客！&lt;/h3&gt; &lt;div style="text-align: center"&gt; &lt;div style="display: inline-block"&gt;倒计时：&lt;/div&gt; &lt;div id="time" style="display: inline-block"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;script&gt; window.onload = function () &#123; let oDiv = document.getElementById("time"); let count = 5; oDiv.innerHTML = count; let timer = null; timer = setInterval(function () &#123; if (count &gt; 0) &#123; count = count - 1; oDiv.innerHTML = count; &#125; else &#123; location.href='http://keejo.coding.me' &#125; &#125;, 1000); &#125;&lt;/script&gt;&lt;/html&gt; 最终效果可以见链接：https://www.torandom.com]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>html5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScrip实现Strip()功能]]></title>
    <url>%2FJavaScrip%E5%AE%9E%E7%8E%B0Strip-%E5%8A%9F%E8%83%BD.html</url>
    <content type="text"><![CDATA[背景之前用JS做了一个输入校验的功能，要求输入的必须是一个合法的email地址，结果在自己测试的时候发现输入不通过。检查发现，现在手机输入法（搜狗）联想输入的时候，默认会在输入的词语后面加上一个空格，所以导致校验的正则不通过。 思路Email校验的正则如下： 12let reg = new RegExp("^[a-z0-9]+([._\\-]*[a-z0-9])*@([a-z0-9]+[-a-z0-9]*[a-z0-9]+.)&#123;1,63&#125;[a-z0-9]+$")reg.test(email) 根据上述正则，是不允许字符串开头结尾有空格之类的多余字符的。那么解决这个问题就有两个方案： 修改正则，允许开头、结尾有空格等符号 对输入做处理，删除开头、结尾的空格等符号 这里我选了第二种，因为一直有在写python，其中有一个strip()函数，可以自动的删除字符串开头结尾的空格等多余符号的，所以理所当然认为javascript也有，结果发现竟然是没有的。 那么需要自己写一个了。 对字符串的处理，最简单粗暴的也就是正则了。 在正则中\s可以匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。注意 Unicode 正则表达式会匹配全角空格符。 http://www.runoob.com/regexp/regexp-syntax.html 先找从字符串开头起的\s : ^\s+ 再找字符串结尾的\s: \s+$ 将找到的字符串替换为空&#39;&#39; 结论根据上面的思路，表达式也就出来了： 1let email = email.replace(/^\s+|\s$/g, '') 其中/g是用于全局替换的，如果不加/g，那么当开头和结尾都有空格等符号时，仅会替换开头部分。 http://www.runoob.com/jsref/jsref-regexp-g.html]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序部署EACCES: permission denied问题]]></title>
    <url>%2F%E5%B0%8F%E7%A8%8B%E5%BA%8F%E9%83%A8%E7%BD%B2EACCES-permission-denied%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景开发工具： 微信开发者工具 腾讯云环境： node 问题之前做了一个账本excel导出功能，可见用exceljs实现Json对象导出excel。大体逻辑是在项目根目录生成一个临时的excel文件，然后通过邮件发送给用户之后，删除临时文件。 在本地测试通过之后，部署到腾讯云开发环境，点击导出按钮，就报错了。直接请求failed，看了下报错信息： 思路问题其实比较明确，没有权限。按照功能的实现思路，在生成excel文件时第一次打开，发送邮件时第二次用到这个文件。所以问题应该出在程序无法在根目录写入文件，生成excel文件。 注：后续和腾讯云确认，根目录的确是授权给单独一个用户用于部署程序，所以node运行用户无权限在根目录写入文件。 那么解决方法就只有两个 找一个有权限的目录 给node运行的用户授权 腾讯云的小程序服务器端部署是一套完整的自动化流程，个人无法直接访问服务器确认用户权限，所以在不更换部署服务的情况下，只能选择第一种。 解决由于使用的腾讯官方的wafer2框架，在demo中是有上传文件的案例的。 在wafer-node-sdk的node包中lib\upload\index.js中可以看到源代码： 1234567// 初始化 multiparty const form = new multiparty.Form(&#123; encoding: 'utf8', maxFilesSize: maxSize * 1024 * 1024, autoFiles: true, uploadDir: '/tmp' &#125;) demo中，文件是先上传至/tmp目录下，然后再转保存在cos中。由此可以确定/tmp目录是可以写入文件的。 将代码中临时生成的excel文件路径也放在/tmp目录下，果然问题解决了。 插曲在部署到生产环境时，先上传代码，然后点击“安装依赖”，最后点击“部署代码”。一路都显示成功，但结果却发现node环境挂了，直接用web访问生产环境提示bad getway。 心里一紧，再次点击“部署代码”，这次有报错出来了，提示是某依赖没有找到。 于是又点了一次安装依赖，待提示安装成功之后，再点部署代码，就ok了。 事后咨询了腾讯云的支持，理论上生产环境的部署是不需要手动安装依赖的。系统会自动从package.json中拉取依赖清单并安装。这次就不知道是什么情况，记录一下。]]></content>
      <categories>
        <category>微信小程序</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>微信小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用nodemailer实现邮件发送]]></title>
    <url>%2F%E7%94%A8nodemailer%E5%AE%9E%E7%8E%B0%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81.html</url>
    <content type="text"><![CDATA[背景由于随手记账本是基于小程序的，没办法直接通过浏览器下载的方式导出给用户。于是考虑在导出请求时，要求用户提供一个电子邮箱，后台生成导出的excel文件之后直接以附件的形式发送到用户邮箱中。目前是通过SendGrid提供的免费邮箱服务来实现邮箱发送。SendGrid也提供各个版本的webapi支持，不过考虑到后续兼容性，本次就摒弃了SendGrid提供的接口组件，使用nodemail。 思路发送邮件的方式基本大同小异，使用smtp协议的话需要知道： 邮箱服务器地址 端口号 用户名 密码 然后邮件内容上需要明确： 收件人 主题 邮件内容 是否为html格式 附件 发件人（如果通过个人邮箱开通smtp发件，那么可能不支持另外设置发件人。因为发件人就是自己，SendGrid是支持只是发件人的，比如`no-reply@torandom.com`。） 最后按照使用的邮件模块的说明发送即可。 实现nodemailer的文档不是太直观，说明的很详细，但是没有一个完整的简单例子。普通的单次邮件发送主要有这么几个步骤： 设置smtpConfig信息 设置message信息 创建transporter对象 调用transporter对象的sendMail()发送邮件并接受回调 代码如下： 12345678910111213141516171819202122232425262728293031323334353637const nodemailer = require('nodemailer')// 配置邮件服务器信息let smtpConfig = &#123; host: 'smtp.sendgrid.net', port: 587, secure: false, // upgrade later with STARTTLS auth: &#123; user: 'username', pass: 'password' &#125;&#125;// 配置邮件内容信息let message = &#123; from: 'no-reply@torandom.com', to: 'test@torandom.com', subject: '随手记账本 - 导出', text: '随手记账本导出测试', html: '&lt;p&gt;HTML version of the message&lt;/p&gt;', attachments: [ &#123; filename: 'package.json', path: './package.json' &#125; ]&#125;// 创建transporter对象let transporter = nodemailer.createTransport(smtpConfig)// 发送邮件transporter.sendMail(message) .then(info =&gt; &#123; if (info.accepted) &#123; console.log('已发送至' + info.accepted.toString()) &#125; else &#123; console.log('邮件发送失败') console.log(info) &#125; &#125;) 注意 在发送邮件部分。回调的info内容是由你的邮件服务器提供的，不同的邮件服务器提供的内容是不同的需要确认。 如果测试代码的时候，发送到自己的QQ邮箱。发送的数量太多了的话，会有如下的报错： 123456789101112550 Connection frequency limited出错原因：该服务器IP的发信频率超过腾讯邮箱限制。 腾讯邮箱对来自相同IP的外部发信服务器有一定的频率限制： 1、超过每分钟发信量限制，此IP地址被禁止发信若干分钟。 2、超过每小时发信量限制，此IP地址被禁止发信若干小时。 3、超过每日发信量限制，此IP地址本日内禁止再发信。 4、以上频率限制数值属于腾讯邮箱保密数据，恕不公开。 改善建议：如果您是该服务器IP的管理员，请暂停该服务器IP的发信，稍后降低频率重新尝试发信。 如果您是个人邮箱用户，请向您的电子邮件提供商报告此情况。 https://service.mail.qq.com/cgi-bin/help?subtype=1&amp;id=20022&amp;no=1000722 这时候需要把你的发件人加入到接受邮件的白名单中即可。 关于nodemailer的更多高级用法，比如邮件池之列的请参考如下官方文档。 https://nodemailer.com/about/]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>邮件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用exceljs实现Json对象导出excel]]></title>
    <url>%2F%E7%94%A8exceljs%E5%AE%9E%E7%8E%B0Json%E5%AF%B9%E8%B1%A1%E5%AF%BC%E5%87%BAexcel.html</url>
    <content type="text"><![CDATA[背景在做随手记账本项目的时候，很多网友在意见反馈中建议提供导出功能。由于小程序的后台是基于node的，于是在npm里找了下关于excel的包，也参考了百度建议，推荐比较多的是excelexport，但是我最后选了exceljs。主要是一直在持续更新，文档也很全面。 思路由于后台数据库保存的账本数据是采用json格式的，最简单的方法就是通过遍历账本中的所有条目逐行写入excel。但是发现在exceljs这个包中，是提供列定义的，通过定义每列的key，就可以直接把json数据写入了，这个功能非常赞。 这部分的文档说明如下： 123456789101112// Add column headers and define column keys and widths// Note: these column structures are a workbook-building convenience only,// apart from the column width, they will not be fully persisted.worksheet.columns = [ &#123; header: 'Id', key: 'id', width: 10 &#125;, &#123; header: 'Name', key: 'name', width: 32 &#125;, &#123; header: 'D.O.B.', key: 'DOB', width: 10, outlineLevel: 1 &#125;];// Add a couple of Rows by key-value, after the last current row, using the column keysworksheet.addRow(&#123;id: 1, name: 'John Doe', dob: new Date(1970,1,1)&#125;);worksheet.addRow(&#123;id: 2, name: 'Jane Doe', dob: new Date(1965,1,7)&#125;); 需要说明的说，文档里定义的column里的header，会自动写在表格的第一行当做表头，不需要另起一行。 实现主要有这么几个步骤： 新建workbook对象 新建sheet 定义column 写入row 导出 代码如下： 1234567891011121314151617181920212223242526272829303132333435const Excel = require('exceljs')// 测试数据let data = &#123; "subtitle":"偶遇美食节", "comment":"鲷鱼烧，烤鱼，年糕丸子。600+200+350", "cost":1150, "date":"2018-01-20", "time":"14:24", "member":1, "type":"餐饮", "currency":"日元", "location":"Ueno Park (上野恩賜公園)"&#125;// 新建workbook对象let workbook = new Excel.Workbook()// 设置workbook属性，比如作者workbook.creator = 'ToRandom'// 新建sheetlet tempWorksheet = workbook.addWorksheet('东京之旅')// 定义column, 日期比较长，设置为15 可以展示yyyy-mm-ddtempWorksheet.columns = [ &#123;header: '标题', key: 'subtitle'&#125;, &#123;header: '消费类型', key: 'type'&#125;, &#123;header: '评论', key: 'comment'&#125;, &#123;header: '币种', key: 'currency'&#125;, &#123;header: '费用', key: 'cost'&#125;, &#123;header: '日期', key: 'date', width: 15&#125;, &#123;header: '时间', key: 'time'&#125;, &#123;header: '人数', key: 'member'&#125;, &#123;header: '位置信息', key: 'location'&#125; ]// 写入tempWorksheet.addRow(data)// 保存文件，如有需要，前面加await等待执行workbook.xlsx.writeFile("随手记账本.xlsx) 注意 一般node默认是异步执行的，如有后续操作（比如先生成excel文件之后，以附件的形式发送），那么就需要在最后一步保存文件的代码前面加上await。 1await workbook.xlsx.writeFile("随手记账本.xlsx") 如果生成之后，处理完了要删除的话，可以使用fs.unlink来删除。但注意的是，这个unlink也是一个异步函数，删除效果会有延迟。 其他关于exceljs的操作，可以参考原文档，还是相当丰富的。 https://www.npmjs.com/package/exceljs]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>Json</tag>
        <tag>node</tag>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS下部署selenium环境]]></title>
    <url>%2FCentOS%E4%B8%8B%E9%83%A8%E7%BD%B2selenium%E7%8E%AF%E5%A2%83.html</url>
    <content type="text"><![CDATA[背景最近写了一个循环抓取某网站数据的代码，其中涉及到页面登陆，采用了selenium来做。考虑到循环抓取，本机跑容易因系统休眠断网造成爬取失败，于是在自己的服务器上部署一下。 操作系统：CentOS 7 Python版本：Python3.7 问题由于服务器抓取，其实不需要展示浏览器的界面，可以考虑使用PhantomJS来做，结果发现有如下的警告提示： 1UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead 既然后续不支持了，那么就按照官方建议，我选chrome。 不同于PhamtomJS，chromedriver需要和chrome配合使用，也就是如果不安装chrome，直接加载chromedriver那么就会有如下的报错： 123raise exception_class(message, screen, stacktrace)selenium.common.exceptions.WebDriverException: Message: unknown error: cannot find Chrome binary (Driver info: chromedriver=2.45.615279 (12b89733300bd268cff3b78fc76cb8f3a7cc44e5),platform=Linux 3.10.0-693.2.2.el7.x86_64 x86_64) 而且MacOS和CentOS的chromedriver是不同的。如果在linux下直接加载mac版的chromedriver就会如下的报错： 12raise child_exception_type(errno_num, err_msg, err_filename)OSError: [Errno 8] Exec format error: './chromedriver' 部署正确的部署应该是这样的： 安装chromeCentOS服务器是用ssh登陆的，就直接用yum来安装好了，由于存在翻墙的问题，要手动配置下源： 12cd /etc/yum.repos.d/vim google-chrome.repo 然后添加如下语句： 123456[google-chrome]name=google-chromebaseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearchenabled=1gpgcheck=1gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub 再用yum安装： 1yum -y install google-chrome-stable --nogpgcheck 在未翻墙的环境下，一定要加上--nogpgcheck选项，否则会因为检查失败而无法安装成功。 https://blog.csdn.net/u010472499/article/details/72327963 安装chromedriver墙内可以在如下地址下载对应系统的chromedriver: http://npm.taobao.org/mirrors/chromedriver/2.45/ 下载好的chromedriver需要放到PATH目录下，建议是自己使用virtualenv新建一个venv虚拟环境，然后将chromedriver放到虚拟环境的bin目录下（venv/bin）即可。 selenium的使用加上headless的配置即可，关键代码如下： 123456from selenium import webdriveroption = webdriver.ChromeOptions()option.add_argument('headless')driver = webdriver.Chrome(executable_path='./chromedriver', options=option)# do somethingdriver.quit()]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>selenium</tag>
        <tag>爬虫</tag>
        <tag>chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx与tornado框架的并发评测]]></title>
    <url>%2FNginx%E4%B8%8Etornado%E6%A1%86%E6%9E%B6%E7%9A%84%E5%B9%B6%E5%8F%91%E8%AF%84%E6%B5%8B.html</url>
    <content type="text"><![CDATA[背景分别测试在windows平台和linux平台(SuSE)下，tornado框架的并发效果，以及通过配置nginx对并发效果影响。 操作系统： windows: Windows Server 2008 SP2 （8C8G) linux: SuSE12 SP3 （8C8G) 并发测试工具：tsung 测试访问：仅返回”Hello World”字符 评测过程1、直接访问tornado，并发设置为500 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 36.11 msec 32.51 msec 620.3 / sec 138.53 / sec 33.18 msec 44464 page 0.11 sec 33.02 msec 1221.1 / sec 272.69 / sec 76.98 msec 87507 request 0.11 sec 33.02 msec 1221.1 / sec 272.69 / sec 76.98 msec 87507 session 1mn 56sec 8.66 sec 14.8 / sec 1.66 / sec 12.64 sec 498 Code Highest Rate Mean Rate Total number 200 616 / sec 138.70 / sec 44536 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 error_abort_max_conn_retries 4.7 / sec 104 error_abort_max_send_retries 13.9 / sec 394 error_connect_econnrefused 35.8 / sec 581 error_connection_closed 35.7 / sec 1023 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.21 sec 30.57 msec 760 / sec 127.01 / sec 34.44 msec 41788 page 0.21 sec 30.97 msec 1474.9 / sec 247.78 / sec 39.04 msec 81576 request 0.21 sec 30.97 msec 1474.9 / sec 247.78 / sec 39.04 msec 81576 session 2mn 43sec 3.33 sec 11.9 / sec 1.66 / sec 5.48 sec 498 Code Highest Rate Mean Rate Total number 200 762.6 / sec 127.15 / sec 41867 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 error_abort_max_send_retries 11.9 / sec 498 error_connection_closed 38.4 / sec 1500 说明 在500的并发量下，无论在windows还是liunx平台，均有较多的连接错误。但框架系统还是比较稳定的，没有出现崩溃等情况。 2 、通过Nginx代理访问，并发设置为500 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.18 sec 31.78 msec 2287.5 / sec 588.11 / sec 0.11 sec 179083 page 0.58 sec 0.17 sec 2283.4 / sec 587.32 / sec 0.36 sec 178778 request 0.58 sec 0.17 sec 2283.4 / sec 587.32 / sec 0.36 sec 178778 Code Highest Rate Mean Rate Total number 200 1736.4 / sec 307.54 / sec 94844 502 1122.9 / sec 280.16 / sec 84048 Name Highest Rate Total number error_abort 0.5 / sec 1 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.17 sec 32.16 msec 801 / sec 710.33 / sec 73.15 msec 216931 page 0.69 sec 0.16 sec 798 / sec 709.48 / sec 0.58 sec 216633 request 0.69 sec 0.16 sec 798 / sec 709.48 / sec 0.58 sec 216633 Code Highest Rate Mean Rate Total number 200 794.9 / sec 709.79 / sec 216726 504 0.7 / sec 0.04 / sec 11 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 说明 增加Nginx做反向代理之后，有效的提供了一定的缓冲。在windows平台下出现较多的code 502，说明后台没有及时返回，导致Nginx直接返回给压测工具code 502。在linux平台下就比较稳定了。 3、通过Nginx代理访问，后端设置4台服务器，并发设置为1000 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 35.78 msec 4122.2 / sec 508.31 / sec 0.12 sec 156715 page 0.48 sec 74.59 msec 4109.8 / sec 506.99 / sec 0.28 sec 156275 request 0.48 sec 74.59 msec 4109.8 / sec 506.99 / sec 0.28 sec 156275 Code Highest Rate Mean Rate Total number 200 4115.6 / sec 507.68 / sec 156481 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.21 sec 36.22 msec 4786.7 / sec 533.37 / sec 0.12 sec 163256 page 0.50 sec 73.71 msec 4784.9 / sec 532.57 / sec 0.29 sec 162988 request 0.50 sec 73.71 msec 4784.9 / sec 532.57 / sec 0.29 sec 162988 Code Highest Rate Mean Rate Total number 200 4792.2 / sec 533.28 / sec 163199 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 说明 由于后台扩充到了4台服务器，通过Nginx进行轮询访问，分散了压力。在1000的并发下，windows和suse平台表现不相上下，均无错误。 4、通过Nginx代理访问，后端设置4台服务器，并发设置为1500 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.19 sec 32.09 msec 2576 / sec 1075.47 / sec 0.10 sec 327195 page 0.50 sec 71.39 msec 2602.4 / sec 1073.57 / sec 0.31 sec 326577 request 0.50 sec 71.39 msec 2602.4 / sec 1073.57 / sec 0.31 sec 326577 Code Highest Rate Mean Rate Total number 200 2502.1 / sec 1001.18 / sec 304862 502 770.7 / sec 84.82 / sec 22052 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 33.41 msec 3275.7 / sec 990.22 / sec 0.11 sec 299354 page 0.48 sec 68.46 msec 3270.3 / sec 988.43 / sec 0.27 sec 298777 request 0.48 sec 68.46 msec 3270.3 / sec 988.43 / sec 0.27 sec 298777 Code Highest Rate Mean Rate Total number 200 3261.5 / sec 989.09 / sec 298975 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 说明 当并发增加到1500时，windows平台出现code 502，后端服务器出现瓶颈。linux平台表现稳定。 5、通过Nginx代理访问，后端设置4台服务器，并发设置为2000 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 19.30 sec 36.06 msec 2187.66666666667 / sec 611.56 / sec 0.48 sec 166002 page 19.36 sec 0.14 sec 2197 / sec 611.42 / sec 0.57 sec 165828 request 19.36 sec 0.14 sec 2197 / sec 611.42 / sec 0.57 sec 165828 Code Highest Rate Mean Rate Total number 200 2194.66666666667 / sec 611.54 / sec 172473 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 error_connect_etimedout 17.1 / sec 318 error_next_session 0.285714285714286 / sec 2 说明 在2000并发先，linux平台后端依旧稳定返回code 200，但是Nginx会直接返回error，瓶颈出现在Nginx，需要调整相关配置了。 结论这次测试中可以发现，当仅返回字符串Hello World时，无论是windows平台还是liunx平台，在并发500的情况下虽然框架可以稳定输出，但是会出现不同程度的系统处理不过来直接拒绝请求的情况。 通过增加Nginx，可以有效的为后端提供缓冲，同样500的并发下，liunx平台返回给Nginx的错误code 504要明显比windows平台code 502少很多。 通过增加后台服务器，使用Nginx进行轮询，可以增加并发，在后端4台服务器，1000的并发下，linux平台和windows平台表现不相上下。但并发增加到1500之后，windows平台开始出现大量的code 502错误，linux平台依旧稳定。把并发继续增加到2000，Nginx端出现瓶颈，返回连接错误，后端linux保持稳定。可考虑下通过调整Nginx配置或者增加Nginx来继续提升并发效果。 不过，从上述测试情况来看，torando框架还是很稳定的，不至于并高并发弄到崩溃的程度。]]></content>
      <categories>
        <category>中间件</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>SuSE</tag>
        <tag>windows</tag>
        <tag>高并发</tag>
        <tag>torando</tag>
        <tag>Nginx</tag>
        <tag>tsung</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SuSE缺失devel包的问题]]></title>
    <url>%2FSuSE%E7%BC%BA%E5%A4%B1devel%E5%8C%85%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景最近几天计划将原Python项目迁移到Liunx服务器上，操作系统是SuSE 12 SP3。原以为Python项目迁移会比较方面，使用pip安装requirements包就好了，结果遇到不少问题。 问题 安装mysqlclient包时，出现了如下报错： 12345678910creating build/temp.linux-x86_64-2.7gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Dversion_info=(1,3,9,'final',1) -D__version__=1.3.9 -I/usr/include/mysql -I/usr/local/python/include/python2.7 -c _mysql.c -o build/temp.linux-x86_64-2.7/_mysql.o -m64 _mysql.c:29:23: fatal error: my_config.h: No such file or directory #include "my_config.h" ^ compilation terminated. error: command 'gcc' failed with exit status 1 ----------------------------------------Command "/home/sysop/webapp/hzinfo/venv/bin/python -u -c "import setuptools, tokenize;__file__='/tmp/pip-install-4FRt1w/mysqlclient/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))" install --record /tmp/pip-record-AgHBn8/install-record.txt --single-version-externally-managed --compile --install-headers /home/sysop/webapp/hzinfo/venv/include/site/python2.7/mysqlclient" failed with error code 1 in /tmp/pip-install-4FRt1w/mysqlclient/ 安装ldap库时，出现了如下报错： 123456In file included from Modules/LDAPObject.c:9:0:Modules/errors.h:8:18: fatal error: lber.h: No such file or directory #include "lber.h" ^compilation terminated.error: command 'gcc' failed with exit status 1 思路这些错乍看不一样，其实差不多，都是缺失了一些东西，导致安装失败。针对每个问题逐个谷歌百度就会发现，大家会告诉你需要安装对应的开发库及xxx-dev(el)包。 比如上面第一个问题里，安装mysqlclient包报错了，需要安装对应的dev包，在SuSE中是libmysqlclient-devel。 xxx与xxx-dev(el)的关系经过这两天的查询和总结，再Liunx中，一般会把软件拆分为两部分，一部分是直接使用的库即xxx，另一部分就是开发用的库，包含一些头文件之类的，就是xx-dev(el)。 差不多可以这样理解：当你只是使用某个软件的时候，你只要安装xxx即可，但当你需要二次开发或者使用对应的一些插件的时候，很可能你就会需要再安装xxx-dev(el)了。 https://blog.csdn.net/wangeen/article/details/14522227 解决办法知道了问题的原因，那么解决办法就简单了。但是对于SuSE，尤其是没有外网的SuSE就不是了。 SuSE是收费的系统，没办法直接下载到对应的rpm安装包，需要挂载对应系统版本的SDK光盘。而问题是不断暴露和修复的，我们需要的dev包可能在最初获取的SDK光盘里不存在。查看了SuSE官网，有些补丁也是建议通过网络更新的，或者直接下载更新SDK安装盘。 https://www.suse.com/zh-cn/documentation/sles-12/book_sle_deployment/data/sec_add-ons_sdk.html 不过官网下载是相当的慢。。。 zypper的使用zypper是SuSE的当我们有了对应的sdk光盘或者目录之后，可以通过nfs的方式挂载到zypper源中。 添加源 123zypper ar -t yast2 -n 'sles12sp3_sdk1' -fc nfs://122.16.125.112/iso/sles12sp3_sdk1 sles12sp3_sdk1zypper ar -t yast2 -n 'sles12sp3_sdk2' -fc nfs://122.64.29.85/approot1/sles12sp3_sdk sles12sp3_sdk2zypper ar -t yast2 -n 'sles12sp3_server' -fc fs://122.64.29.85/approot1/sles12sp3_server sles12sp3_server 搜索需要的安装包 当遇到上述问题中的安装失败，使用如下的命令搜索下有哪些包可以安装： 1zypper se mysql 显示结果如下： 安装对应的包 这里可以看到libmysqlclient-devel没有安装（前面有i标记的即为已经安装），安装即可。 1~ # zypper install libmysqlclient-devel 搜索安装其他devel包 同理，在第二个问题中，我们搜索ldap，就会发现可以libldapcpp-devel包没有安装。 装好之后继续安装python-ldap还会遇到一个错误： 12345Modules/LDAPObject.c:18:18: fatal error: sasl.h: No such file or directory #include &lt;sasl.h&gt; ^compilation terminated.error: command 'gcc' failed with exit status 1 同理，再搜一下： 发现cyrus-sasl-devel包没装，也把装上，问题就解决了。 注意python也是有python-devel包的，这个不要忘了。]]></content>
      <categories>
        <category>操作系统</category>
        <category>SuSE</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>MySQL</tag>
        <tag>SuSE</tag>
        <tag>zypper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL的安装]]></title>
    <url>%2FMySQL%E7%9A%84%E5%AE%89%E8%A3%85.html</url>
    <content type="text"><![CDATA[背景每次重搭环境都免不了要重新安装数据库，虽然频率不高，也发现竟然在3个平台上都装过了，记录一下。 Windows安装MySQL 下载MySql：http://dev.mysql.com/downloads/mysql/ 解压后放到安装目录 在环境变量中将mysql安装目录定义为%MYSQL_HOME%，并将bin目录加入环境变量（%MYSQL_HOME%\bin） 在mysql-5.6.24-winx64的根目录下，找到my-default.ini文件，改名为my.ini 。打开，添加如下信息： 123456789[mysqld]loose-default-character-set = utf8character-set-server = utf8basedir = D:\mysql-5.6.24-winx64 #写自己的mysql路径哦datadir = D:\mysql-5.6.24-winx64\data #写自己的mysql路径哦！[client]loose-default-character-set = utf8[WinMySQLadmin]Server = D:\mysql-5.6.24-winx64\bin\mysqld.exe # 写自己的mysql路径哟！ 以管理员身份运行cmd，进入到%MYSQL_HOME%到bin目录下运行如下命令： 12mysqld --initialize -insecure # 初始化mysql，创建root用户，密码为空mysqld -install 最后提示：Service successfully in installed! 启动mysql。 1net start mysql 停止mysql 1net stop mysql 首次登陆，无需密码 1mysql -u root 修改root密码 1mysql&gt; set password for root@localhost = password(&apos;root&apos;); 再次登陆 1mysql -u root -p 会提示输入密码，输入后已root用户进入。 其他命令 停止mysql 1net stop mysql 卸载服务 1mysqld -remove MAC下安装Mysql 下载Mac版的DMG Archive包。（不建议用brew安装，后续配置很麻烦） 后双击安装，一路下一步 会出现一个提醒，给了个默认root@localhost账号的密码。比如：root@localhost: wuKgf_mCK38z 安装完成后，在系统偏好设置中找到MySQL图标，点击进入，手动启动MySQL服务。 通过alias绑定命令：在命令行中运行如下命令，绑定mysql 12MacBook-Air:~ icbc$ alias mysql=/usr/local/mysql/bin/mysqlMacBook-Air:~ icbc$ alias mysqladmin=/usr/local/mysql/bin/mysqladmin ​ 注意：这种方式只能在当前命令行中有效。 建议是在环境变量中增加： 123456cd ~touch .bash_profilevi .bash_profile# 加入如下语句export PATH=$&#123;PATH&#125;:/usr/local/mysql/bin 进行root密码重置，比如重置为root，运行如下命令，然后输入临时密码即可。 12MacBook-Air:~ icbc$ mysqladmin -u root -p password rootEnter password: Suse下离线安装MySQL 先下载Suse版本的RPM包，官网有很多，要下载Bundle版，否则得分别下载各个组件。 SUSE Linux Enterprise Server 12 (x86, 64-bit), RPM Bundle 解压安装包 12345HzTomcat:/data # tar -xvf mysql-8.0.13-1.sles12.x86_64.rpm-bundle.tarHzTomcat:/data # cd mysql-8.0.13-1.sles12.x86_64/HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # lsmysql-community-client-8.0.13-1.sles12.x86_64.rpm mysql-community-devel-8.0.13-1.sles12.x86_64.rpm mysql-community-server-8.0.13-1.sles12.x86_64.rpmmysql-community-common-8.0.13-1.sles12.x86_64.rpm mysql-community-libs-8.0.13-1.sles12.x86_64.rpm mysql-community-test-8.0.13-1.sles12.x86_64.rpm 下面的安装要注意按顺序，先安装mysql-community-common-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-common-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-common-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-common-8.0.13-1.s################################# [100%] 再安装mysql-community-libs-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-libs-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-libs-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-libs-8.0.13-1.sle################################# [100%] 再安装mysql-community-client-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-client-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-client-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-client-8.0.13-1.s################################# [100%] 最后安装mysql-community-server-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-server-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-server-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-server-8.0.13-1.s################################# [100%] 启动 1HzTomcat:~ # service mysql start 查看临时root密码 12HzTomcat:~ # grep 'temporary password' /var/log/mysql/mysqld.log 2019-01-09T07:16:31.105387Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: !kwpeD_Pc7/p 修改root密码 12345678910111213141516171819202122HzTomcat:~ # mysql -uroot -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 8Server version: 8.0.13Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; alter user 'root'@'localhost' IDENTIFIED BY 'Password123!';Query OK, 0 rows affected (0.14 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.07 sec)mysql&gt; exitBye 可能出现的问题问题在修改root密码时，忘了flush privileges，然后导致新旧密码都无法登陆的问题。 123HzTomcat:~ # mysql -uroot -pEnter password: ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES) 解决方法 修改mysql的配置文件： 1HzTomcat:~ # vim /etc/my.cnf 增加：skip-grant-tables 然后在登陆直接使用root登陆，重新设置密码，即可。 12345678910111213141516171819202122232425262728293031HzTomcat:/var/lib/mysql # mysql -uroot Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 8.0.13 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; flush privileges;Query OK, 0 rows affected (0.02 sec)mysql&gt; alter user &apos;root&apos;@&apos;localhost&apos; identified by &apos;Password123!&apos;;Query OK, 0 rows affected (0.07 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.07 sec)mysql&gt; exitByeHzTomcat:/var/lib/mysql # vim /etc/my.cnfHzTomcat:/var/lib/mysql # service mysql restartHzTomcat:/var/lib/mysql # mysql -uroot -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 8Server version: 8.0.13 MySQL Community Server - GPL]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask - sqlalchemy.orm.exc.DetachedInstanceError]]></title>
    <url>%2Fflask-sqlalchemy-orm-exc-DetachedInstanceError.html</url>
    <content type="text"><![CDATA[背景在一个基于Flask的项目中，使用到flask-sqlachemy的数据库orm工具。在一次数据插入之后再次查询数据时出现了如下的错误： 1DetachedInstanceError: Instance &lt;User at 0x7f2f54fc8750&gt; is not bound to a Session; attribute refresh operation cannot proceed 原因经查，由于之前开发的时候，参考《Flask Web开发 基于Python的Web应用开发实战》这本书时，作者建议在配置flask-sqlachemy时，加入如下配置： 1SQLALCHEMY_COMMIT_ON_TEARDOWN = Ture 这个配置是用来涉及在db操作时，自动提交的。以下两种情况是等效的。123456# SQLALCHEMY_COMMIT_ON_TEARDOWN = True 时db.add(User)# SQLALCHEMY_COMMIT_ON_TEARDOWN = False 时db.add(User)db.commit() 但其实自动提交时，系统会一并删除当前数据库的session，所以导致了上面出现的问题。目前flask-sqlachemy官方也认为这个设置可能存在问题，已经在文档中移除了。 http://flask-sqlalchemy.pocoo.org/2.3/changelog/ 结论那么建议的方法就是删除这条配置，手动提交了。在每次进行数据库新增、修改、删除时，手动的commit()。]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>sqlachemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Navcat连MySQL提示Autentication错误]]></title>
    <url>%2FNavicat%E8%BF%9EMySQL%E6%8F%90%E7%A4%BAAutentication%E9%94%99%E8%AF%AF.html</url>
    <content type="text"><![CDATA[背景Mysql版本：8.0.13 Navicat版本：Navicat Premium Version 12 问题在MySQL中创建好用户，数据库，并将数据库的权限授权给用户之后 1234567891011mysql&gt; create user 'flask'@'%' identified by 'xxxx';Query OK, 0 rows affected (0.01 sec)mysql&gt; create database FlaskDB default charset utf8 collate utf8_general_ci;Query OK, 1 row affected (0.00 sec)mysql&gt; grant all privileges on FlaskDB.* to "flask"@"%"; Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 然后通过Navicat使用连接数据库，出现了下面的报错： 结论这个问题百度一下有很多帖子，是由于新版的MySQL8更新了认证方式，而其他的数据库软件没有跟上。大部分忒子会推荐我们重新配置MySQL，不启用新的认证方式，甚至有些建议删了重装。 个人还是推荐下面这种方式： 1ALTER USER 'username'@'%' IDENTIFIED WITH mysql_native_password BY 'password'; 不重新配置MySQL，暂时先指定使用旧版本的认证方式来设置密码。待后续软件更新了自然就可以使用新版本功能了。 https://stackoverflow.com/questions/49194719/authentication-plugin-caching-sha2-password-cannot-be-loaded]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
        <tag>navicat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle wm_concat实现字符串分割替换]]></title>
    <url>%2Foracle-wm-concat%E5%AE%9E%E7%8E%B0%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%88%86%E5%89%B2%E6%9B%BF%E6%8D%A2.html</url>
    <content type="text"><![CDATA[背景在数据库中有一个存储有权限用户的字段，通过,分割来存储了多个用户的账号。可以通过用户账号在用户表中查询出用户的姓名，现在需要将这个字段的账号转换成用户姓名在前台展示。 用户表user： id name 0001 小明 0002 小刚 需要处理的字段：0001,0002。 要求的返回结果：小明,小刚。 思路 首先我们需要把user表中id字段存在在处理字段中的值给查出来。 最开始考虑使用instr函数： 1select * from user where instr('0001,0002', id)&gt;0 instr()函数可以判断值在字符串中的位置，如果大于0，也就是存在。但是这是模糊匹配的，会有问题。比如上面场景中，如果有用户id是000那么也会匹配出来。 所以使用regexp_like()，通过正则来判断。 这里的正则表达如为：^(0001|0002)$。使用^确定开通，$确认结尾，|用来分割允许的值。对应到实际的就变成如下： 1select name from user where regexp_like(id, '^(' || replace('0001,0002', ',', '|') || ')$') 注意：很多正则判断工具里^0001|0002$也可以实现效果，但oracle中，必须加上()，否则oracle会认为是^0001 和0002$，0001001这样的也会被识别到。 输出结果： name 小明 小刚 然后可以使用wm_concat函数把连起来即可。 wm_concat是一个未被记录的函数，但是可以实现将列通过,连成一个字段的作用。 1select to_char(wmsys.wm_concat(name)) from user where id = '0001' or id = '0002' https://community.oracle.com/thread/1090158 结论连起来结果如下： 1select to_char(wmsys.wm_concat(name)) name from user where regexp_like(id, '^'|| replace('0001,0002', ',', '|')||'$') 输出结果： name 小明,小刚 注意由于wm_concat未被官方记录，不同版本的oracle有区别，如果输出是个clob类型，那么可以使用to_char()来转换。也可以考虑使用listagg()来实现。]]></content>
      <categories>
        <category>数据库</category>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>oracle</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle regexp_substr函数实现字符串split]]></title>
    <url>%2Foracle-regexp-substr%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E5%AD%97%E7%AC%A6%E4%B8%B2split.html</url>
    <content type="text"><![CDATA[背景数据库中有一个owner字段，里面存了多个有权限用户的账号，通过,分割。现前台返回一个用户账号，需要判断该用户是否有权限。首先要把这个字段进行分割成列，然后就可以判断是否存在。 思路 首先oracle是可以使用regexp_substr实现按照分隔符获取元素的。 1234567REGEXP_SUBSTR(source_char, pattern [, position [, occurrence [, match_parameter ] ] ] ) 其中： source_char是需要分割的字符串。 pattern为正则表达。 position是起始位置，默认为1。 occurrence是表示第几个匹配组，默认为1。 match_parameter是匹配参数： i：大小写不敏感 c：大小写敏感 n：允许(.)匹配换行符 m：oracle将把字符串当做多行字符处理，^和$是起始和结束 x：忽略空格 https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions131.htm 于是我们通过下面的表达式获取分割后的某个值： 1select regexp_substr('1,2,3', '[^,]+', 1, 2) from dual; --获取第二个数 结果如下： 现在我们需要把其中的occurrence变成一个和分割后等长的列即可。 获取分割后元素的个数 使用length()可以获取字符串的个数，而字符串是通过,分割的。使用replece()可以替换删除所有的分割符。分割后的字符串个数即是两者的差加1。 1select length('1,2,3') - length(replace('1,2,3',',')) + 1 from dual; 结果如下： 获取一个空列 可以通过level加上connect by来获取一个空列： 1select level from dual connect by level &lt;= 5; 结果如下： 把上述连起来就可以获取分割后的列了。 结论虽然有点长，但是总体思路还是比较清晰的。代码如下： 1select regexp_substr('1,2,3','[^,]+' , 1, level) from dual connect by level &lt;= (select length('1,2,3') - length(replace('1,2,3',',')) + 1 from dual); 结果如下： 最后我们需要判断某个人是否有权限，即某人的账号是否在我们获取的分割后的列里面。直接用count(1)是否大于0即可。 12345select count(1) from (select regexp_substr('1,2,3','[^,]+' , 1, level) as owner from dual connect by level &lt;= (select length('1,2,3') - length(replace('1,2,3',',')) + 1 from dual) twhere t.owner = i_owner]]></content>
      <categories>
        <category>数据库</category>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac常用装机及开发软件分享]]></title>
    <url>%2FMac%E5%B8%B8%E7%94%A8%E8%A3%85%E6%9C%BA%E5%8F%8A%E5%BC%80%E5%8F%91%E8%BD%AF%E4%BB%B6%E5%88%86%E4%BA%AB.html</url>
    <content type="text"><![CDATA[其实软件也不多，主要几个专业软件和开发软件要收费，经常会有一些失效的破解和假的破解版。亲测可用，在这里记录和分享，以备万一。 办公软件 Office2019 + 激活：链接:https://pan.baidu.com/s/1YgGS6AI-XdI5vCcyfPA8OQ 密码:f2dl 专业软件 Photoshop CC2018 + 激活：链接:https://pan.baidu.com/s/1ZdPU1GkkOmYYfgvtLOwpBA 密码:zv6u Auto CAD 2018 + 激活：链接:https://pan.baidu.com/s/1IYWly6SJuGvh8l0LI33hcg 密码:2tpg typora（markdown编辑器）：https://www.typora.io/ 开发软件 PyCharm：http://www.jetbrains.com/pycharm/download/#section=mac idea：https://www.jetbrains.com/idea/download/#section=mac WebStrom：https://www.jetbrains.com/webstorm/download/download-thanks.html Navcat Premium：链接:https://pan.baidu.com/s/1egvUoYnoi21980vo38vQUw 密码:7zn6 SubLime：http://www.sublimetext.com/3 其他 Parallel Desktop（虚拟机）+ 激活：链接:https://pan.baidu.com/s/1dPfTfCDVpzXMrivbt9OUqg 密码:ztns Win10官方镜像：链接:https://pan.baidu.com/s/11iOX061WNDlt9oSpNJh6lw 密码:xn7r Win10 激活工具：链接:https://pan.baidu.com/s/1FNbLuXKG5TPW9y_hvr5YmA 密码:jn9b Keka118（压缩软件）：链接:https://pan.baidu.com/s/16Yjo44uRRPk0aZY6ugZ4qg 密码:cli2]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用node-schedule实现node后台定时任务]]></title>
    <url>%2F%E7%94%A8node-schedule%E5%AE%9E%E7%8E%B0node%E5%90%8E%E5%8F%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.html</url>
    <content type="text"><![CDATA[背景最近在自己的小程序中增加了多币种支持涉及到汇率的更新，于是需要在后台服务端设置一个定时任务来自动通过响应的接口更新最新的汇率。获取汇率的接口目前通过聚合数据提供的免费接口实现（调到需要实名注册）。 node-schedule由于小程序后端服务器是基于node，查一下，果然是有对应的npm包的——node-schedule。 node-schedule可以使用多种方式定义定时任务的，一般使用类似liunx的cron方式就可以满足绝大部分需求了。cron的定义方式参考如下表： 123456789* * * * * *┬ ┬ ┬ ┬ ┬ ┬│ │ │ │ │ ││ │ │ │ │ └ 星期 day of week (0 - 7) (0 or 7 is Sun)│ │ │ │ └───── 月 month (1 - 12)│ │ │ └────────── 日期 day of month (1 - 31)│ │ └─────────────── 小时 hour (0 - 23)│ └──────────────────── 分钟 minute (0 - 59)└───────────────────────── 秒 second (0 - 59, OPTIONAL) 如果你熟悉linux系统的crontab定时任务的话，那就相当简单了。 它还支持基于日期的定时任务以及基于rule的定时任务。详细可以见一下官方说明。 https://www.npmjs.com/package/node-schedule 实现方法首先需要安装node-schedule包，并保存在项目的package.json中。 1cnpm install node-schedule --save 我的需求是每三十分钟自动更新一次数据，那么通过cron的方式就可以定义为如下规则： 10 */30 * * * * 其中*/30是指可以被30整除的，也就是0分和30分的时候。规则确定了，那么代码就很简单了： 12345678910const schedule = require('node-schedule')function scheduleCron () &#123; schedule.scheduleJob('0 */30 * * * *', function () &#123; console.log('scheduleCron ' + new Date()) // do something &#125;)&#125;scheduleCron() 输出结果： 12scheduleCron Wed Jan 02 2019 00:00:00 GMT+0800 (CST)scheduleCron Wed Jan 02 2019 00:30:00 GMT+0800 (CST) 注意！根据官方说明node-schedule是基本上支持所有的cron表达式，除了一下几个： W：最近的工作日，放在日期(day of month)字段，比如15W指到本月15日最近的工作日。 L：表示最后，放在星期(day of month)或者星期(day of week)字段。 #：表示每月的周几，放在星期(day of week)字段。 cron拓展配置规则时几个常用的符号： *：表示匹配任意值。 /：x/y表示等步长序列，可以理解为从x开始，每y个单位执行一次。其中*/5与0/5是等效的，都是指每5分钟执行一次。 ,：表示序列的分割，比如3,4指在3和4的时候执行。 -：表示一个范围，比如3-5指在3到5的时候执行。 ?：仅用在日期和星期字段，表示任意的值，相当于占位。]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序腾讯云环境安装依赖错误]]></title>
    <url>%2F%E5%B0%8F%E7%A8%8B%E5%BA%8F%E8%85%BE%E8%AE%AF%E4%BA%91%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E9%94%99%E8%AF%AF.html</url>
    <content type="text"><![CDATA[背景开发工具：微信开发者工具 操作系统：Mac 腾讯云环境：Node 问题最近使用node-sechedule开发了一个后台定时任务，这个是新增的node模块，所以需要安装依赖，即在package.json中增加对应的依赖： 1234"dependencies": &#123; ... "node-schedule": "^1.3.1" &#125;, 在本地测试通过之后，点击开发工具上面的“腾讯云”-“上传测试代码” 并勾选安装依赖。问题就出现了！！！ 排查及原因 感觉这是权限问题啊，于是切换了小程序的管理员用户，点击上传，情况一样。 无奈只能先恢复开发环境了，然后再次上传。根据文档推荐，首次上传最好使用模块上传，全部勾选，结果情况还是一样。 看了node_modules文件夹中，的确有node-schedule文件夹啊，不过有一点奇怪的是，安装每个安装的npm包都有一个_开头的文件夹： 看来得找专家了，于是在腾讯云控制台上提交了一个工单，过了半个小时的样子就有工程师电话来了。他首先在自己的环境里试了一下我的package.json包，竟然没有问题。。。 工程师登录我的后台服务器看了下： 感觉可能是因为上面这种奇怪的包文件夹造成的。于是乎，再恢复一次环境，然后我再次上传，这次不再勾选“上传node_modules代码”。 这样也即是让后台服务器自动安装所有的依赖。最后，成功了！ 这时候再看一下后台服务器上目录： 没有了那种_开头的目录，就成功了。 插曲有了上面的经验，本以为上传到生产环境就妥妥的了。结果点击“上传正式代码”之后，坑爹了。 正式代码上传是没有上面的这种选项的，本机上的那些_开头的node_modules文件也上传了，再一次入坑。 于是乎，只能再次联系腾讯云的工程师，删除了我工程的node_modules文件夹，然后点击控制台上的安装依赖。待依赖安装完成后，再次部署代码。 这时候后端启动已经没有问题，但是小程序竟然无法直接访问，最后发现是由于小程序解决方案里本来是自带ssl证书的，这个证书最近到期了，需要自己购买新的证书（选免费的）联系后台更新。 至此问题彻底解决。 结论可以看出，主要问题还是在于node_modules里文件的问题。而这个的罪魁祸首是——cnpm。 可以参考如下的issue，cnpm安装包时采用link的方式，与npm不一样。 https://github.com/cnpm/cnpmjs.org/issues/1000 在小程序上传之后自动安装包，则用的是npm安装，从而导致问题出现。以后在没弄明白之前还是先用原生的吧，在别人给你便利的时候，也要明白其中有什么道理。 另外，建议自己在本地开发的时候，server目录下还是把node_modules给删除了，以免后续一不小心又上传了什么问题文件。]]></content>
      <categories>
        <category>微信小程序</category>
      </categories>
      <tags>
        <tag>小程序</tag>
        <tag>node</tag>
        <tag>javascript</tag>
        <tag>腾讯云</tag>
        <tag>微信开发者工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Gulp实现Hexo网页压缩优化]]></title>
    <url>%2F%E7%94%A8Gulp%E5%AE%9E%E7%8E%B0Hexo%E7%BD%91%E9%A1%B5%E5%8E%8B%E7%BC%A9%E4%BC%98%E5%8C%96.html</url>
    <content type="text"><![CDATA[背景经过一段时间的折腾，也算是把这个Hexo的个人博客搭建起来了，换主题，加插件，文章里加图片是什么的，就发现网站有时候会有点慢，于是开始考虑做SEO以及一些优化工作，于是乎发现了Gulp这个神器。 Gulp是什么Hexo生成的静态网页其实是可读性比较好的，会有大量的空格、换行什么的，而实际浏览器解析式完全不需要的。如果把这些空格、换行全部删掉，就会节省很多空间出来，于是网站的响应速度也就变快了。 而Gulp是一种基于node的自动化构建工具，至于自动化构建这个我们目前不需要纠结，我们只要知道它有一些插件可以帮助我们自动化的对hexo生成的各种文件进行压缩。 Gulp怎么用 首先我们要在全局安装下gulp和我们要用到的插件 12npm install gulp -gnpm install gulp gulp-uglify gulp-minify-css gulp-imagemin gulp-htmlmin gulp-htmlclean gulp-concat --save 在hexo的根目录创建一个gulpfile.js文件 先放代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162var gulp = require('gulp'), uglify = require('gulp-uglify'), cssmin = require('gulp-minify-css'), imagemin = require('gulp-imagemin'), htmlmin = require('gulp-htmlmin'), htmlclean = require('gulp-htmlclean'); concat = require('gulp-concat');//JS压缩gulp.task('uglify', function() &#123; return gulp.src(['./public/js/**/.js','!./public/js/**/*min.js'])//只是排除min.js文件还是不严谨，一般不会有问题，根据自己博客的修改我的修改为return gulp.src(['./public/**/*.js','!./public/zuoxi/**/*.js',,'!./public/radio/**/*.js']) .pipe(uglify()) .pipe(gulp.dest('./public/js'));//对应修改为./public即可&#125;);//public-fancybox-js压缩gulp.task('fancybox:js', function() &#123; return gulp.src('./public/vendors/fancybox/source/jquery.fancybox.js') .pipe(uglify()) .pipe(gulp.dest('./public/vendors/fancybox/source/'));&#125;);// 合并 JSgulp.task('jsall', function () &#123; return gulp.src('./public/**/*.js') // 压缩后重命名 .pipe(concat('app.js')) .pipe(gulp.dest('./public'));&#125;);//public-fancybox-css压缩gulp.task('fancybox:css', function() &#123; return gulp.src('./public/vendors/fancybox/source/jquery.fancybox.css') .pipe(cssmin()) .pipe(gulp.dest('./public/vendors/fancybox/source/'));&#125;);//CSS压缩gulp.task('cssmin', function() &#123; return gulp.src(['./public/css/main.css','!./public/css/*min.css']) .pipe(cssmin()) .pipe(gulp.dest('./public/css/'));&#125;);//图片压缩gulp.task('images', function() &#123; gulp.src('./public/uploads/*.*') .pipe(imagemin(&#123; progressive: false &#125;)) .pipe(gulp.dest('./public/uploads/'));&#125;);// 压缩 public 目录 html文件 public/**/*.hmtl 表示public下所有文件夹中html，包括当前目录gulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// gulp.task('default', gulp.series('uglify', 'cssmin', 'fancybox:js', 'fancybox:css', 'jsall','images'));gulp.task('default', gulp.series('uglify', 'cssmin', 'jsall', 'minify-html'));//, 'minify-html' 这里要注意的是，默认安装的是gulp 4.0.0，而网上很多例子是基于gulp 3的，所以运行起来 会有如下的报错： 123456789101112131415assert.js:351 throw err; ^AssertionError [ERR_ASSERTION]: Task function must be specified at Gulp.set [as _setTask] (/Users/zhengk/Desktop/hexo/blog/node_modules/_undertaker@1.2.0@undertaker/lib/set-task.js:10:3) at Gulp.task (/Users/zhengk/Desktop/hexo/blog/node_modules/_undertaker@1.2.0@undertaker/lib/task.js:13:8) at Object.&lt;anonymous&gt; (/Users/zhengk/Desktop/hexo/blog/gulpfile.js:59:6) at Module._compile (internal/modules/cjs/loader.js:721:30) at Object.Module._extensions..js (internal/modules/cjs/loader.js:732:10) at Module.load (internal/modules/cjs/loader.js:620:32) at tryModuleLoad (internal/modules/cjs/loader.js:560:12) at Function.Module._load (internal/modules/cjs/loader.js:552:3) at Module.require (internal/modules/cjs/loader.js:657:17) at require (internal/modules/cjs/helpers.js:22:18) 这是由于gulp4中需要使用gulp.series 和 gulp.parallel来指定运行的任务。具体见上面gulpfile.js中的最后一行。 1gulp.task('default', gulp.series('uglify', 'cssmin', 'jsall', 'minify-html')); https://blog.csdn.net/qq_31975963/article/details/83034450 这一行是写明gulp需要执行的任务，然后需要注意的是，当其中某个任务失败或者没有东西需要压缩的时候，比如你没有用到fancybox却要执行fancybox:js任务，就会有如下的报错： 1234567891011121314151617181920[23:59:25] Using gulpfile ~/Desktop/hexo/blog/gulpfile.js[23:59:25] Starting 'default'...[23:59:25] Starting 'uglify'...[23:59:25] Finished 'uglify' after 24 ms[23:59:25] Starting 'cssmin'...[23:59:26] Finished 'cssmin' after 215 ms[23:59:26] Starting 'fancybox:js'...[23:59:26] 'fancybox:js' errored after 2.96 ms[23:59:26] Error: File not found with singular glob: /Users/zhengk/Desktop/hexo/blog/public/vendors/fancybox/source/jquery.fancybox.js (if this was purposeful, use `allowEmpty` option) at Glob.&lt;anonymous&gt; (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob-stream@6.1.0@glob-stream/readable.js:84:17) at Object.onceWrapper (events.js:277:13) at Glob.emit (events.js:189:13) at Glob.EventEmitter.emit (domain.js:441:20) at Glob._finish (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:197:8) at done (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:182:14) at Glob._processSimple2 (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:688:12) at /Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:676:10 at Glob._stat2 (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:772:12) at lstatcb_ (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:764:12)[23:59:26] 'default' errored after 246 ms 只要把对应的任务删掉就好了。 运行gult 先执行hexo g来生成静态网页，然后我们看下public文件夹下的静态文件大小： 1234$ls -lh...-rw-r--r-- 1 zhengk staff 57K 12 30 00:07 index.html... 然后我们再执行下gulp（执行时默认执行default任务，所以前面gulpfile.js中设置任务为default）对比下效果： 1234567891011121314151617$gulp[00:53:29] Working directory changed to ~/Desktop/hexo/blog[00:53:30] Using gulpfile ~/Desktop/hexo/blog/gulpfile.js[00:53:30] Starting 'default'...[00:53:30] Starting 'uglify'...[00:53:30] Finished 'uglify' after 23 ms[00:53:30] Starting 'cssmin'...[00:53:30] Finished 'cssmin' after 216 ms[00:53:30] Starting 'jsall'...[00:53:30] Finished 'jsall' after 56 ms[00:53:30] Starting 'minify-html'...[00:53:31] Finished 'minify-html' after 1.16 s[00:53:31] Finished 'default' after 1.46 s$ls -lh...-rw-r--r-- 1 zhengk staff 27K 12 30 00:07 hello-world.html... 可以发现足足小了30k，压缩了近一半大小。]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>hexo</tag>
        <tag>gulp</tag>
        <tag>seo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript中的Json、Map、Set]]></title>
    <url>%2FJavaScript%E4%B8%AD%E7%9A%84Json%E3%80%81Map%E3%80%81Set.html</url>
    <content type="text"><![CDATA[问题之前在一个项目中，需要根据申请的部门来获取对应的邮箱地址，想当然的使用了Map对象，结果在调试中完全没有问题，却在实际使用上失效了，查看了下后台log，提示获取到的邮箱地址是undefined。 排查及原因经过百度之后发现原来JS的Map对象的浏览器支持不好，虽然很多地方写IE11开始支持，但其实IE11是不支持new Map()这种方式新建的。 详见如下链接：https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Map#浏览器兼容 于是我把换成了用Json对象，果然就解决了。 所以尽量还是用Json吧。 总结Map对象JS的Map存放的是键值对，key值不允许重复。特别要注意的是在一些旧的浏览器中并不支持Map对象。 基本使用 12345678910111213141516171819202122// 新建Map对象&gt; m = new Map()Map &#123;&#125;// 新增Map键值对&gt; m.set('key1','value1')Map &#123; 'key1' =&gt; 'value1' &#125;// 获取Map中的key值&gt; m.get('key1')'value1'// 判断是否存在某key&gt; m.has('key1')true&gt; m.set('key2','value2')Map &#123; 'key1' =&gt; 'value1', 'key2' =&gt; 'value2' &#125;// 删除某键值对&gt; m.delete('key1')true&gt; mMap &#123; 'key2' =&gt; 'value2' &#125;// 清空Map对象&gt; m.clear()undefined 遍历 12345678910&gt; m.set('key1','value1')Map &#123; 'key1' =&gt; 'value1' &#125;&gt; m.set('key2','value2')Map &#123; 'key1' =&gt; 'value1', 'key2' =&gt; 'value2' &#125;&gt; m.forEach(function (value, key, map) &#123;... console.log(key + ":" + value)... &#125;)key1:value1key2:value2undefined Set对象Set对象可以理解为没有值的Map对象，一般用于存放一个不允许重复的列表 基本使用 123456789101112131415161718// 新建Set对象&gt; s = new Set()Set &#123;&#125;// 新增Set值&gt; s.add('key1')Set &#123; 'key1' &#125;&gt; s.add('key2')Set &#123; 'key1', 'key2' &#125;// 删除某值&gt; s.delete('key1')true&gt; sSet &#123; 'key2' &#125;// 清空Set对象&gt; s.clear()undefined&gt; sSet &#123;&#125; 遍历 Set对象的礼遍历和Map基本一样，但是由于Set对象没有value值，所以遍历的时候key和value是一样的。 12345678910&gt; s.add('key1')Set &#123; 'key1' &#125;&gt; s.add('key2')Set &#123; 'key1', 'key2' &#125;&gt; s.forEach(function (value, key, set) &#123;... console.log(key + ":" + value)... &#125;)key1:key1key2:key2undefined Json对象 基本使用 123456\\ 新建Json对象&gt; let currencyItems = &#123; '人民币': 1, '港币': 0.88, '澳门元': 0.86, '新台币': 0.2241, '美元': 6.905, '日元': 0.06, '英镑': 8.69, '欧元': 7.8, '韩元': 0.006, '泰铢': 0.21, '新西兰元': 4.69, '澳大利亚元': 4.96, '菲律宾比索': 0.13, '加拿大元': 5.16, '瑞士法郎': 6.92, '瑞典克朗': 0.76, '丹麦克朗': 1.05, '挪威克朗': 0.8 &#125;undefined\\ 获取Json对象某值&gt; currencyItems['人民币']1 获取Json对象的所有Key值 12345678910111213141516171819&gt; Object.keys(currencyItems)[ '人民币', '港币', '澳门元', '新台币', '美元', '日元', '英镑', '欧元', '韩元', '泰铢', '新西兰元', '澳大利亚元', '菲律宾比索', '加拿大元', '瑞士法郎', '瑞典克朗', '丹麦克朗', '挪威克朗' ] 获取Json对象的长度 12&gt; Object.keys(currencyItems).length18 遍历 12345678910111213141516171819202122&gt; for ( var i in currencyItems) &#123;... console.log(i + ":" + currencyItems[i])... &#125;人民币:1港币:0.88澳门元:0.86新台币:0.2241美元:6.905日元:0.06英镑:8.69欧元:7.8韩元:0.006泰铢:0.21新西兰元:4.69澳大利亚元:4.96菲律宾比索:0.13加拿大元:5.16瑞士法郎:6.92瑞典克朗:0.76丹麦克朗:1.05挪威克朗:0.8undefined]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>Json</tag>
        <tag>Map</tag>
        <tag>Set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows Server 2018 TCP 连接数限制问题]]></title>
    <url>%2FWindows-Server-2018-TCP-%E8%BF%9E%E6%8E%A5%E6%95%B0%E9%99%90%E5%88%B6%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景最近在查一个并发问题，在压测的时候，Nginx的error.log显示connect() failed(111: Connection refused)。而后端应用并未手工设置过拒绝连接。于是怀疑是在高并发的情况下，windows服务器可能存在自行拒绝连接的情况。 排查过程首先打开windows服务器上的 任务管理器 - 性能 - 资源监控器。TCP连接这儿显示总数为100。 然后开启压测，TCP连接开始飙升，然后问题出现了。 TCP连接满了，怎么就变成10了！不过瓶颈应该就是这儿了！ 结论经过各种百度，谷歌，发现我好像被误导了。 微软官方说从Windows Vista，Window server 2008 SP2 起，不在限制half-open TCP connections，也就是理论上不再有连接数的限制。 官方说明见这个地址：https://support.microsoft.com/zh-cn/help/969710/how-to-enable-the-half-open-tcp-connections-limit-in-windows-vista-wit 然后根据国外有个问答网站的结论，这个“10”，“100”这个显示应该是个Bug，并不是一共就10个或者100个。 可参考如下这个解释： https://serverfault.com/questions/448589/increasing-of-max-more-than-10-tcp-connections 那么怎么看确定的连接数呢？ 在 开始 - 运行 中输入 perfmon.exe打开性能监视器，然后添加TCPv4的计数器。 这里就可以看到当前的实际连接数了，图里当前最新连接数是“824“，远超前面显示的”10“或者”100“。 看来一不小心又碰到坑了。 那么最开始要查的Connection refused到底是什么原因呢，还得继续努力了。。。]]></content>
      <categories>
        <category>操作系统</category>
        <category>windows</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取当当网的书籍分类目录]]></title>
    <url>%2F%E8%8E%B7%E5%8F%96%E5%BD%93%E5%BD%93%E7%BD%91%E7%9A%84%E4%B9%A6%E7%B1%8D%E5%88%86%E7%B1%BB%E7%9B%AE%E5%BD%95.html</url>
    <content type="text"><![CDATA[背景之前单位新建了一个小图书馆，然后就有了这么一个需求，需要设置一下图书的分类与目录。要怎么定义呢，当然是百度咯。然后想到了卖书发家的当当网，打算把当当网上的所有图书分类全部抓下来提供给行政来作参考。 思路打开当当网的图书页面http://book.dangdang.com/，图书分类就在网页的左边，开启F12看源代码。 多看看就看出来规律了，关注红框部分。所有的分类其实都在&lt;a&gt;标签里，其中的href属性里的网址很有规律，去掉前面的域名之后，都以cp + 数字来命名，其中数字与数字之间用.来分割，代表一级目录和二级目录。 所以大体思路就是通过正则表达式先抓取href属性中含有cp开头的元素，然后找出所有第一节数字不同的元素，获取其text属性来当一级目录，然后把域名+cp+一级目录序号当做固定前缀来找对应的二级目录。 要注意就是去重还有一些删除一些网址不符合这个过滤的，以及所有的text记得用strip()来删除一下多余的空格和换行符号。 具体实现按上面的思路，主要用requests bs4就差不多了，详细代码就参考github吧，https://github.com/keejo125/ 有更好的方法的也欢迎分享。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开启又关闭icloud云盘，我的桌面文件去哪里了！]]></title>
    <url>%2F%E5%BC%80%E5%90%AF%E5%8F%88%E5%85%B3%E9%97%ADicloud%E4%BA%91%E7%9B%98%EF%BC%8C%E6%88%91%E7%9A%84%E6%A1%8C%E9%9D%A2%E6%96%87%E4%BB%B6%E5%8E%BB%E5%93%AA%E9%87%8C%E4%BA%86%EF%BC%81.html</url>
    <content type="text"><![CDATA[最近更换笔记本，又不想直接通过时间胶囊设置新mac，于是乎在整理好旧资料之后，准备拷贝到新电脑时发现了iCloud云盘这个东西，可以自动备份桌面和文稿的内容到iCloud，那么在新电脑中再通过iCloud下载就好了，完美！ 结果，高估了iCloud的效果，开启之后，mac会上传桌面和文稿，速度超级慢，然后mac风扇呼呼的转，果断放弃，关闭了iCloud云盘。 关闭也很慢，卡了一会儿，提示 慢的不行，反正也没上传多少东西，于是就点了“停止更新并关闭” 然后就问题出现了！！！ 桌面空空如也，我的东西呢！！！ 在翻翻iCloud云盘，只有已经上传了的那一丢丢！！！ 急中生智，赶紧百度，翻了好结果贴，结论如下： 其实前面已经提示了，文件都能被放在了一个叫做 “iCloud云盘（归档）”中了。路径如下：/User/xxx/中。 真是虚惊一场，看到网上好多碰到一样问题的，好多人以为就没有了。 特地记录一下。]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>iCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于这个”博客“]]></title>
    <url>%2F%E5%85%B3%E4%BA%8E%E8%BF%99%E4%B8%AA%E2%80%9D%E5%8D%9A%E5%AE%A2%E2%80%9C.html</url>
    <content type="text"><![CDATA[最初是在腾讯的云开发者平台上知道的hexo，好像很方便的样子，又很Geek的样子，于是打算尝试一下。 后来发现hexo作为博客的话，缺少了很大一部分功能——评论。 虽然可以使用第三方插件，但总觉得有点怪怪的。 于是乎，我打算把这里作为记录自己日常知识积累，或是感悟的地方。所以在标题中用了有引号的”博客“。 把每次百度或是谷歌出来的答案都记下来，希望不会再搜第二次。 感觉上好像有点跌跌撞撞，但方向是往前不就好了么，是吧]]></content>
      <categories>
        <category>生活随笔</category>
        <category>感悟</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world.html</url>
    <content type="text"><![CDATA[这是一个新的开始。 马东说：我的底色的悲凉的。 蔡康永说：只有底色悲凉的乐观，才是真的乐观啊。 乐观起来，哪怕人间不值得。]]></content>
      <categories>
        <category>生活随笔</category>
        <category>感悟</category>
      </categories>
  </entry>
</search>
