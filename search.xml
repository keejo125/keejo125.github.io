<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python基础之Python面面观1]]></title>
    <url>%2FPython%E5%9F%BA%E7%A1%80%E4%B9%8BPython%E9%9D%A2%E9%9D%A2%E8%A7%82.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 2.1 条件 编程练习题 编写一个输入分数，输出分数等级的程序，具体为： Score Grade 90~100 A 70~89 B 60~69 C 0~59 D others Invalid score 请添加必要的输入输出语句，尽量让程序友好。 123456789101112if __name__ == '__main__': score = int(input('please input your score:')) if score &gt;= 90: print('Your grade is A!') elif score &gt;= 70: print('Your grade is B!') elif score &gt;= 60: print('Your grade is C!') elif score &gt;=0: print('Your grade is D!') else: print('Invalid score!') 编写程序，从键盘输入一个二元一次方程ax^2+bx+c=0的三个参数a、b、c（均为整数），求此方程的实根。如果方程有实根，则输出实根（保留一位小数），如果没有实根则输出没有实根的信息。 [输入样例1] 1,0,-1 [输出样例1] x1 = 1.0, x2 = -1.0 [输入样例2] 1,2,1 [输出样例2] x = -1.0 [输入样例3] 2,2,3 [输出样例3] no real solution 123456789101112if __name__ == '__main__': a, b, c = eval(input("please input a,b,c values of a*x^2+b*x+c:")) t = b ** 2 - 4 * a * c if t &gt; 0: x1 = (-b + sqrt(t)) / (2 * a) x2 = (-b - sqrt(t)) / (2 * a) print('x1 = &#123;&#125;, x2 = &#123;&#125;'.format(x1, x2)) elif t == 0: x = -b / (2 * a) print('x = &#123;&#125;'.format(x)) else: print('no real solution') 2.2 Range函数 2.3 循环 编程练习 输入一个整数，求其逆序数。注：虽然可通过字符串切片等方法轻松获得一个数的逆序数，但用整数通过循环来获得逆序数是锻炼逻辑思维的一个好例子。 1234567891011121314151617181920212223def reversed1(number): length = len(number) out = '' for i in range(len(number)): out += number[length - 1 - i] return outdef reversed2(number): number = int(number) out = 0 while number != 0: out = out * 10 + number % 10 number = number // 10 return outif __name__ == '__main__': number = input('input a number:') out1 = reversed1(number) out2 = reversed2(number) print(out1) print(out2) 将一个正整数分解质因数。例如：输入90,打印出90=233*5。 1234567891011121314151617181920212223242526272829def sp1(number): start = str(number) out = [] i = 2 while number != 1: while number % i == 0: number //= i out.append(str(i)) i += 1 print(start + '=' + '*'.join(out))def sp2(number): print(str(number) + '=', end='') i = 2 while number != 1: while number % i ==0: number //= i if number == 1: print('&#123;:d&#125;'.format(i), end='') else: print('&#123;:d&#125;*'.format(i), end='') i += 1if __name__ == '__main__': number = eval(input('input a number:')) sp1(number) sp2(number) 2.4 循环中的break、continue和else 2.5 自定义函数]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础之走进Python]]></title>
    <url>%2FPython%E5%9F%BA%E7%A1%80%E4%B9%8B%E8%B5%B0%E8%BF%9BPython.html</url>
    <content type="text"><![CDATA[该系列为南京大学课程《用Python玩转数据》学习笔记，主要以思维导图的记录 1.1 Python简介 1.2 第一个Python程序 1.3 Python语法基础 1.4 Python数据类型 1.5 Python基本运算 1.6 Python的函数、模块和包 测试题目：简单的输入输出： 12345surname = input('input your surname:')lastname = input('input your lastname:)print('your surname is:', surname)pirnt('your lastname is:', lastname)print('your full name is:', lastname, surname) 讨论关于round函数： 在python2.x中：0.5会近似到距离0远的一端，比如 : 12round(0.5) = 1round(-0.5) = -1 Values are rounded to the closest multiple of 10 to the power minus ndigits; if two multiples are equally close, rounding is done away from 0. 在python3.x中：0.5会近似到偶数那边，比如： 12round(2.5) = 2round(3.5) = 4 values are rounded to the closest multiple of 10 to the power minus ndigits; if two multiples are equally close, rounding is done toward the even choice.]]></content>
      <categories>
        <category>python</category>
        <category>用python玩转数据</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas分组统计不重复值的数量]]></title>
    <url>%2Fpandas%E5%88%86%E7%BB%84%E7%BB%9F%E8%AE%A1%E4%B8%8D%E9%87%8D%E5%A4%8D%E5%80%BC%E7%9A%84%E6%95%B0%E9%87%8F.html</url>
    <content type="text"><![CDATA[基础数据数据格式如下，其中行只有一个值，所以每列均存在重复项（比如，一条策略含多个源地址，多个目标地址和多个端口情况）。 策略ID 策略描述 源地址 目标地址 端口 开通时间 结束时间 id description src dst port st et 12345678import pandas as pddf = pd.read_csv('/Users/zhengk/PycharmProjects/Mine/firewall/output/policy.csv', delimiter=';')df.columns = ['id', 'description', 'src', 'dst', 'port', 'st', 'et']df.head(2)Out[3]: id description src dst port st et0 5 icmp Any Any PING forever forever1 3 Tntrust2AllUntrust new_AG58 All_Untrust ANY forever forever 统计开通策略数最多的前5个源地址即对应的策略数根据要求，只要按照源地址src分组，统计每个不同的src有多少个不重复的策略ID数，在进行排序即可。 先筛选src和id两列进行计算 12345678df[['src', 'id']].head(5)Out[1]: src id0 Any 51 new_AG58 32 122.249.125.98/32 19733 122.249.125.160/32 19744 122.249.125.18/32 2100 先使用groupby进行分组，使用nunique()来统计不重复的个数 12345678df[['src', 'id']].groupby('src')['id'].nunique().head(5)Out[2]: src122.249.125.1/32 2122.249.125.100/32 1122.249.125.101/32 38122.249.125.102/32 227122.249.125.103/32 255 使用sort_values()对输出进行排序 123456789df[['src', 'id']].groupby('src')['id'].nunique().sort_values(ascending=False).head(5)Out[3]: src122.249.125.71/32 1272122.249.125.209/32 529122.249.125.73/32 424122.249.125.113/32 391122.249.125.210/32 361Name: id, dtype: int64 拓展 DataFrame.nunique(*axis=0*, *dropna=True*) 该函数根据axis轴参数统计非重复的个数，默认是按列来统计，如果指定axis=1则按行来统计。 123456789101112131415161718df1 = pd.DataFrame(&#123;'A': [1, 2, 3], 'B': [1, 1, 1]&#125;)df1Out[13]: A B0 1 11 2 12 3 1df1.nunique()Out[14]: A 3B 1dtype: int64df1.nunique(1)Out[15]: 0 11 22 2dtype: int64 http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nunique.html?highlight=nunique#pandas-dataframe-nunique Series.unique() 该函数返回非重复的具体值。是对series的操作，而不能直接操作dataframe。 1234df1['A'].unique()Out[22]: array([1, 2, 3])df1['B'].unique()Out[23]: array([1]) 比如上例中，获取每个源地址可所有不重复的策略ID： 123456789df[['src', 'id']].groupby('src')['id'].unique().head(5)Out[24]: src122.249.125.1/32 [42690, 42691]122.249.125.100/32 [31563]122.249.125.101/32 [12458, 12488, 12536, 14394, 19550, 22348, 227...122.249.125.102/32 [12124, 12134, 12148, 12159, 12222, 12688, 126...122.249.125.103/32 [26711, 26713, 26714, 26719, 26721, 26736, 267...Name: id, dtype: object http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html?highlight=unique#pandas-series-unique DataFrame.sort_values(*by*, *axis=0*, *ascending=True*, *inplace=False*, *kind=&#39;quicksort&#39;*, *na_position=&#39;last&#39;*) 该函数用于对指定axis轴中的值进行排序，默认为升序排列。需要注意的是其中的na_position参数，当源数据中存在空值时，确定空值的处理是很重要的，默认是空值放在最后。 123456df1.sort_values(by=['A'], ascending=False)Out[26]: A B2 3 11 2 10 1 1 http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html#pandas-dataframe-sort-values]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用selenium实现批量文件下载]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8selenium%E5%AE%9E%E7%8E%B0%E6%89%B9%E9%87%8F%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD.html</url>
    <content type="text"><![CDATA[背景实现需求：批量下载联想某型号的全部驱动程序。 一般在做网络爬虫的时候，都是保存网页信息为主，或者下载单个文件。当涉及到多文件批量下载的时候，由于下载所需时间不定，下载的文件名不定，所以有一定的困难。 思路 参数配置 在涉及下载的时候，需要先对chromedriver进行参数配置，设定默认下载目录： 12345678global base_pathprofile = &#123; 'download.default_directory': base_path&#125;chrome_options = webdriver.ChromeOptions()chrome_options.add_experimental_option('prefs', profile)driver = webdriver.Chrome(executable_path='../common/chromedriver', options=chrome_options)driver.implicitly_wait(10) 页面分析 联想官网上每个型号的驱动下载页面如上图所示，虽然前面有一个登陆的遮罩，但是实际上并不影响点击。需要注意的是： 驱动列表，需要点击才可以显示具体的下载项目表格 每个下载列表的表头建议做跳过处理 下载处理 在页面中，找到“普通下载”的元素，点击即可下载。最终实现结果是我们希望根据网页的列表进行重命名和重新归档到文件夹，但是我们会发现如下几个问题： 下载过来的文件名无法控制。 依次下载的话，我们无法确认需要下载多久。并行下载的话，无法有效的区分重命名。 在网上找了很久，也没找到在下载时直接重命名的方法，所以最终选择依次下载，当每次下载完成后进行重命名和归档，思路如下： 对每个驱动目录，先新建一个文件夹，如：主板 点击下载后开始下载文件 通过os模块，找到下载目录中所有文件，并按创建时间排序，找到最新创建的文件 由于未完成的文件后缀为.crdownload（chrome），那么根据后缀来判断是否已完成下载，未完成的话继续等待 待下载完成，将文件重命名并剪切到开始建立的归档目录 在后期测试的时候，发现还有几个坑需要注意： 在查找最新创建的文件时，需要注意.DS_Store文件的处理。（Mac系统，Windows则需要考虑thumbs.db） 需要判断一下最新创建的文件是否为文件夹，可以通过filter函数来处理 最新文件的排序查找实现如下： 12345678910111213141516def sort_file(): """排序文件""" dir_link = base_path dir_lists = list(filter(check_file, os.listdir(dir_link))) if len(dir_lists) == 0: return '' else: dir_lists.sort(key=lambda fn: os.path.getmtime(dir_link + os.sep + fn)) return os.path.join(base_path, dir_lists[-1])def check_file(filename): if filename == '.DS_Store' or filename == 'thumbs.db': return False global base_path return os.path.isfile(os.path.join(base_path, filename)) 总结最终实现效果如下： 完整代码参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/Lenovo 如果大家有更好的方法，也欢迎分享。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib画图时标注最大值]]></title>
    <url>%2Fmatplotlib%E7%94%BB%E5%9B%BE%E6%97%B6%E6%A0%87%E6%B3%A8%E6%9C%80%E5%A4%A7%E5%80%BC.html</url>
    <content type="text"><![CDATA[背景在上一篇使用matplotlib绘制时间序列图表中，本来想只展示最大值，一直没找到方法，就先标注了所有的点的数值，看起来有点不够直接。今天终于搞定了，记录一下。 思路源数据：index 为 ‘data’，数据为’title’ 123456789cacu.head(5)Out[5]: titledate 2015-01-01 22015-02-01 02015-03-01 02015-04-01 02015-05-01 2 matpoltlib标注数值 在绘图的时候，可以使用ax.text()方法在坐标系中标注文字，使用方法如下： 123# 显示全部数值for a,b in zip(cacu.index, cacu.values): ax.text(a, b, b[0]) 寻找最大值： 在pandas中有现成的方法可以找到最大值和最大值的索引： 最大值的索引： 1234cacu.idxmax()Out[6]: title 2018-07-01dtype: datetime64[ns] 最大值： 1234cacu.max()Out[8]: title 50dtype: int64 也可根据索引取值： 12345cacu.loc[cacu.idxmax()]Out[7]: titledate 2018-07-01 50 画图标注 根据最上面的标注，应该直接输入最大值的坐标（x为索引，y为值）就可以了，结果有了如下的报错： 1234567891011121314151617ax.text(cacu.idxmax(), cacu.max(), cacu.max())/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison result = method(y)Traceback (most recent call last): File "/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py", line 3291, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File "&lt;ipython-input-9-cebfed8d464d&gt;", line 1, in &lt;module&gt; ax.text(cacu.idxmax(), cacu.max(), cacu.max()) File "/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/matplotlib/axes/_axes.py", line 722, in text x=x, y=y, text=s) File "/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/matplotlib/text.py", line 163, in __init__ self.set_text(text) File "/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/matplotlib/text.py", line 1191, in set_text if s != self._text: File "/Users/zhengk/PycharmProjects/web_scraping_and_data_analysis/venv/lib/python3.7/site-packages/pandas/core/generic.py", line 1479, in __nonzero__ .format(self.__class__.__name__))ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). 我们仔细看上面最大值的返回结果类型，而并非一个坐标。 12type(cacu.idxmax())Out[11]: pandas.core.series.Series 而我们真正需要的应该是title列的最大索引： 1234type(cacu['title'].idxmax())Out[12]: pandas._libs.tslibs.timestamps.Timestampcacu['title'].idxmax()Out[13]: Timestamp('2018-07-01 00:00:00', freq='MS') 对应的最大值也一样： 1234cacu['title'].max()Out[15]: 50type(cacu['title'].max())Out[16]: numpy.int64 再次画图： 123x = cacu['title'].idxmax()y = cacu['title'].max()ax.text(x, y, y, verticalalignment='bottom', horizontalalignment='center', fontsize='large') 结果 结论这里出现问题其实还是对matplotlib和pandas的基本概念没有弄明白，什么时候获取的是值，什么时候获取的是序列，还需要多加练习。 另外，给图添加标注还有可以使用plt.annotate()方法： 123x = cacu['title'].idxmax()y = cacu['title'].max()plt.annotate(y, xy=(x,y)) 完整代码参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/weixin]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用matplotlib绘制时间序列图表]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8matplotlib%E7%BB%98%E5%88%B6%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%9B%BE%E8%A1%A8.html</url>
    <content type="text"><![CDATA[背景前面通过selenium爬取了微信公众号“新世相”的所有文章链接，详见使用Selenium获取微信公众号的所有文章。获取到的信息有：文章发表的时间、标题以及对应的url。那么根据时间可以绘制出文章发表情况的图表，先上结果图： 思路 读取csv 1df_ori = pd.read_csv('articles.csv', sep=';', header=None) 筛选数据 我们获取到的源数据如下： 123&quot;2019-03-02;看到44岁潘粤明的脸，我发现面对痛苦的最好方式是温柔&quot;;http://mp.weixin.qq.com/s?__biz=MzI2OTA3MTA5Mg==&amp;mid=2651791151&amp;idx=1&amp;sn=d24b36c7af7f3a13e8ab9b8d4d11143f&amp;chksm=f11e58b4c669d1a2fe70afe9fd2f2fdfe3cc1a17d0c48d8efa51ae8e10d17239c7ff0f8649b4&amp;scene=21#wechat_redirect&quot;2019-03-01;“同时被3个男人追求，我选了最丑的”：看完这些70岁老人的遗憾，我啥都想开了&quot;;http://mp.weixin.qq.com/s?__biz=MzI2OTA3MTA5Mg==&amp;mid=2651791096&amp;idx=1&amp;sn=eb68116d46fddf0e827475d9b26941b0&amp;chksm=f11e5963c669d075ec8716073bec685f819df8c2dbc03b87f23347ba5b01ea4219864877b4a0&amp;scene=21#wechat_redirect&quot;2019-02-28;在同学会上遇见初恋：他盯了我 5 秒，说出我等了 10 年的那句话&quot;;http://mp.weixin.qq.com/s?__biz=MzI2OTA3MTA5Mg==&amp;mid=2651790909&amp;idx=1&amp;sn=f642cd31fbc8d41d0b4fbe949b51425a&amp;chksm=f11e59a6c669d0b0e5e7ee12cece0a4afaa995aa123b8e993f8ec10d8ba38cccdcf5290de25a&amp;scene=21#wechat_redirect | 第一列 | 第二列 || ——— | —— || 日期;标题 | url | 那么我们只要获取第一列，并根据分隔符;进行拆分就可以得到日期与标题的对应关系： 123# 取第一列并分割日期与标题df = df_ori.iloc[:, 0]df = df.str.split(';', expand=True) 格式化数据 现在给数据设置列别名并进行格式化操作 1234# 格式化日期，设置column，并将日期设置为indexdf.columns = ['date', 'title']df.date = pd.to_datetime(df.date)df = df.set_index('date') 数据统计 我们需要计算每个月新世相公众号发表的文章数，那么需要按月重新取样，并汇总统计文章数 12# 按月统计文章数"MS"为月初cacu = df.resample("MS").count() 画图 配置画布大小 由于时间跨度从2015年至今比较长，我们需要先配置一下画布大小。 12# 画图fig, ax = plt.subplots(figsize=[20, 5]) 配置线条展示：值处用圆点加粗 12# 线条ax.plot(cacu, 'o-') 配置中文展示 12345# 通过设置中文字体方式解决中文展示问题font = FontProperties(fname='../font/PingFang.ttc')ax.set_title("新世相文章统计", fontproperties=font)ax.set_xlabel("日期", fontproperties=font)ax.set_ylabel("文章数", fontproperties=font) 配置横轴展示 由于按月画图，我们横轴显示为年-月，我们这里对横轴设置两层坐标，第一层仅展示月份，第二层展示年份，这样可以防止因为横轴字符挤在一起。第二层坐标格式中增加\n\n用于换行，给第一层月份留出空间。 1234567# 设置时间轴formater = mdate.DateFormatter('%Y-%m')ax.xaxis.set_major_formatter(formater)ax.xaxis.set_minor_locator(mdate.MonthLocator())ax.xaxis.set_minor_formatter(mdate.DateFormatter('%m'))ax.xaxis.set_major_locator(mdate.YearLocator())ax.xaxis.set_major_formatter(mdate.DateFormatter('\n\n%Y')) 显示数值 123# 显示数值for a,b in zip(cacu.index, cacu.values): ax.text(a, b, b[0]) 显示图片 1plt.show() 结论最后画图如下： 可见，新世相也是从16年开始逐步兴起，基本上保持每天一篇的频率一致稳定发展至今。 完整代码参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/weixin]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用selenium把网页保存为PDF]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8selenium%E6%8A%8A%E7%BD%91%E9%A1%B5%E4%BF%9D%E5%AD%98%E4%B8%BAPDF.html</url>
    <content type="text"><![CDATA[背景前面通过selenium爬取了微信公众号“新世相”的所有文章链接，详见使用Selenium获取微信公众号的所有文章。那么接下来就该获取具体文章了。由于网页是含有图片的，想想还是通过浏览器把网页打印成PDF保存好了，同时保存一份不含图片的文本文件，可以用于后续分析。 那么怎么使用selenium打印PDF呢？ 思路在网上找了找解决方案，主要有如下几种： 利用第三方包：pdfkit，可参考：https://www.cnblogs.com/silence-cc/p/9463227.html 使用chrome的—print-to-pdf模式，将请求到html导出为pdf，可参考：http://osask.cn/front/ask/view/1029784 使用js命令&#39;window.print();来调用浏览器打印，可参考：https://gitee.com/shinemic/codes/09y87ph6vf2c5zamwls3q48 这里我们选用第三种，相对来说适应性比较好，也方便查看进展，如果想隐藏页面，只需要加入—headlss选项即可。 实现如下： 配置chromedriver的options 1234567891011121314151617appState = &#123; "recentDestinations": [ &#123; "id": "Save as PDF", "origin": "local" &#125; ], "selectedDestinationId": "Save as PDF", "version": 2 &#125;profile = &#123; 'printing.print_preview_sticky_settings.appState': json.dumps(appState), 'savefile.default_directory': './articles'&#125;chrome_options = webdriver.ChromeOptions()chrome_options.add_experimental_option('prefs', profile)chrome_options.add_argument('--kiosk-printing') 这里savefile.default_directory用来指定保存文章的路径，需自行配置。 保存pdf 12345driver.get(url)time.sleep(5)# 保存PDFtemp_title = driver.titledriver.execute_script('window.print();') 这里chrome打印网页时默认文件名为网页的title，所以这里先保存一下temp_title=driver.title。 改名 1os.rename('./articles/' + temp_title + '.pdf', './articles/' + title + '.pdf') 由于如果打开同一个网站的多个页面并保存pdf，那么很可能就会出现由于网站title相同而覆盖的情况，所以每次保存完毕后，改一下pdf的文件名。 注意：当网页异常等情况可能出现title为空的情况，那么这里改名的时候就会报异常错误，需要进行异常处理。 实现根据上述思路，在打开网页、导出pdf、改名之后加上sleep，防止异常。实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344def get_articles(): appState = &#123; "recentDestinations": [ &#123; "id": "Save as PDF", "origin": "local" &#125; ], "selectedDestinationId": "Save as PDF", "version": 2 &#125; profile = &#123; 'printing.print_preview_sticky_settings.appState': json.dumps(appState), 'savefile.default_directory': './articles' &#125; chrome_options = webdriver.ChromeOptions() chrome_options.add_experimental_option('prefs', profile) chrome_options.add_argument('--kiosk-printing') driver = webdriver.Chrome(executable_path='./chromedriver', options=chrome_options) driver.implicitly_wait(60) count = 1 with open('articles.csv', newline='') as csvfile: spamreader = csv.reader(csvfile, delimiter=';') for line in spamreader: try: title = line[0].split(';')[1] url = line[1] print("下载第" + str(count) + "篇，标题：" + title) driver.get(url) time.sleep(5) # 保存PDF temp_title = driver.title driver.execute_script('window.print();') time.sleep(10) os.rename('./articles/' + temp_title + '.pdf', './articles/' + title + '.pdf') # 保存txt content = driver.find_element_by_id('js_article').text with open('./text/' + title + '.txt', 'w') as f: f.write(content) count += 1 except Exception as e: logging.exception(e) driver.quit() return 完整代码参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/weixin 如果大家有更好的方法，也欢迎分享。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Selenium获取微信公众号的所有文章]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8Selenium%E8%8E%B7%E5%8F%96%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E7%9A%84%E6%89%80%E6%9C%89%E6%96%87%E7%AB%A0.html</url>
    <content type="text"><![CDATA[背景前段时间有人在群里分享了爬虫咪蒙公众号的所有文章，可以通过深度学习进行各种分析，但由于咪蒙账号已封，所以链接点进去也看不到了。个人还是比较喜欢看新世相的公众号的，看看怎么把它的文章也都爬下来。 思路 从哪里爬？ 爬虫一般得用浏览器访问，然后找到相关的请求接口，通过修改参数来伪造请求获取数据。微信公众号文章浏览器上哪里看呢？搜了下，有三种方式： 搜狗的微信搜索：http://weixin.sogou.com 该方式只能搜到公众号最新10篇文章，放弃 使用fiddler抓包，并用手机模拟器模拟手机访问 难度比较大，放弃 在微信公众平台：https://mp.weixin.qq.com/ 通过插入文章的方式可以查找公众号的所有文章，相对简单，pick 爬取过程 由于微信公众平台登陆需要手机扫描二维码，不可避免的要“人机耦合”了，所以打算全程采取使用selenium来做： 新建ChromeDriver并登陆网页，sleep10秒钟用于手工扫码登陆 12345678910111213url = "https://mp.weixin.qq.com/"option = webdriver.ChromeOptions()# option.add_argument('headless')driver = webdriver.Chrome(executable_path='./chromedriver', options=option)driver.implicitly_wait(60)driver.get(url)driver.find_element_by_name('account').clear()driver.find_element_by_name('password').clear()driver.find_element_by_name('account').send_keys(config.account)driver.find_element_by_name('password').send_keys(config.password)driver.find_element_by_class_name('btn_login').click()# 手动扫码time.sleep(10) 登陆后打开素材管理，并新建图文素材： 1234# 打开素材管理driver.find_element_by_link_text('素材管理').click()# 点击新建图文素材driver.find_element_by_class_name('weui-desktop-btn_primary').click() 需要注意的是，这里会新建一个窗口，在selenium中页面切换操作，需要获取windows_handler然后切换： 1234# 新建图文素材 会有新建页面all_handles = driver.window_handles# 切换到新窗口driver.switch_to.window(all_handles[1]) 在新建素材页面，点击插入链接的按钮，并选择查找文章，输入“新世相”搜索找到对应的微信号 12# 点击插入链接driver.find_element_by_id('edui24_body').click() 12345678910# 选择查找文章driver.find_element_by_xpath('//*[@id="myform"]/div[3]/div[1]/div/label[2]/span').click()# 输入公众号名称driver.find_element_by_class_name('js_acc_search_input').send_keys('新世相')# 搜索driver.find_element_by_class_name('js_acc_search_btn').click()# 选择新世相# driver.find_element_by_class_name('search_biz_item').text# Out[53]: '订阅号\n新世相 微信号：thefair2'driver.find_element_by_class_name('search_biz_item').click() 新建一个csv文件用于保存文章标题与固定链接 123# 新建Csvf = open('articles.csv', 'w')writer = csv.writer(f, delimiter=';') 找到搜索结果部分，获取文章标题与链接，并通过不停的点击“下一页”，来爬取全部文章 12345678910111213141516171819202122# 选择结果输出部分search_article_result = driver.find_element_by_class_name('search_article_result')count = 1while True: # 获取列表 temp_list = search_article_result.find_elements_by_class_name('my_link_item') logging.info("抓取第" + str(count) + "页") for item in temp_list: title = item.text.replace('\n', ';') logging.info(title) # 文章链接 href = item.find_element_by_tag_name('a').get_attribute('href') writer.writerow([title, href]) try: search_article_result.find_element_by_class_name('page_next').click() count += 1 time.sleep(random.randint(20,40)) except Exception as e: logging.exception("获取列表失败") driver.quit() return 这里需要注意，爬取过程要多通过sleep来暂停下，不然很快就会提示“操作过于频繁”而被暂停了 爬取结果 如果中途被反爬了，就在上次爬到的页数那儿开始继续吧，多耦合几次还是可以爬完的 总结由于公众号管理这块登陆涉及到了扫描二维码，安全性高了很多，大大增加了爬虫的难度。相对来说，腾讯的反爬还是做的非常好的，直接使用selenium来模拟人操作，中间sleep也相当长的时间了，还是会被提示“操作太频繁”而被中断，且中断时间长达好几个小时，那么如果采用请求的方式，那估计会被封的更快吧。。。 完整代码参考：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/weixin 如果大家有更好的方法，也欢迎分享。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-5 最长回文子串]]></title>
    <url>%2FLeetCode-5-%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2.html</url>
    <content type="text"><![CDATA[题目：最长回文子串给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。 示例 1： 123输入: &quot;babad&quot;输出: &quot;bab&quot;注意: &quot;aba&quot; 也是一个有效答案。 示例 2： 12输入: &quot;cbbd&quot;输出: &quot;bb&quot; 思路 如何判断字符串是否为回文： 所谓回文，就是正反都一样，在python中反转字符串非常简单，如果源字符串等于反转后的字符串，那么就可以判断为回文： 12345def isPalindorme(str: str): if str == str[::-1]: return True else: return False 找出所有字符串 需要遍历两次，设i为初始index，j为index+1到结尾，就可以获取所有的子串 对所有子串来判断是否为回文，并记录长度，即可找到最大的回文子串 对于s长度为0，或者1的，需特殊处理一下 实现如下： 12345678910111213141516class Solution: def longestPalindrome(self, s: str) -&gt; str: res_len = 0 res = '' if len(s) == 0: return '' if len(s) == 1: return s for i in range(len(s)): for j in range(i+1, len(s) + 1): tempstr = s[i:j] if isPalindorme(tempstr): if len(tempstr) &gt; res_len: res_len = len(tempstr) res = tempstr return res 优化方法该方法参考了官方解答中java版，思路如下： 如何判断回文： 随便找字符串中的一个字符 如果是回文的话有两种情况： 当回文字符串长度为奇数：那么该字符串左边的字符和右边的相等（从第0个开始，即自己等于自己开始，然后依次往左右扩展，得到该字符为中间的最长回文子串。）如aba。 当回文字符串长度为偶数：那么左边第i个字符和右边第i+1个字符相等（从第0个开始，即自己等于下一个，然后依次往左右扩展，得到该字符为中间的最长回文子串。）如abba。 从字符串s开头起遍历整个字符串，并对每个字符分别做两种回文判断，得出最大回文字符串。 实现如下： 12345678910111213141516171819202122class Solution: def longestPalindrome(self, s: str) -&gt; str: if len(s) == 0: return '' if len(s) == 1: return s start = 0 end = 0 for i in range(len(s)): len1 = self.expandAroundCenter(s, i, i) len2 = self.expandAroundCenter(s, i, i + 1) max_len = max(len1, len2) if max_len &gt; (end - start): start = i - (max_len - 1) // 2 end = i + max_len // 2 return s[start:end + 1] def expandAroundCenter(self, s, left, right): while left &gt;= 0 and right &lt; len(s) and s[left] == s[right]: left -= 1 right += 1 return right - left - 1 如果大家有更好的方法或者思路，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-4 寻找两个有序数组的中位数]]></title>
    <url>%2FLeetCode-4-%E5%AF%BB%E6%89%BE%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0.html</url>
    <content type="text"><![CDATA[题目：寻找两个有序数组的中位数给定两个大小为 m 和 n 的有序数组 nums1 和 nums2。 请你找出这两个有序数组的中位数，并且要求算法的时间复杂度为 O(log(m + n))。 你可以假设 nums1 和 nums2 不会同时为空。 示例 1: 1234nums1 = [1, 3]nums2 = [2]则中位数是 2.0 示例 2: 1234nums1 = [1, 2]nums2 = [3, 4]则中位数是 (2 + 3)/2 = 2.5 思路这题目乍一看有点绕，其实简单一点考虑就是找这两个数组集合的中位数。 先把两个数组合并成一个数组temp = nums1 + nums2 然后再对这个新数组进行排序 找中位数： 当这个数组长度是奇数时：中位数就是排在最中间的那个数 当这个数组长度是偶数时：中位数就是排在最中间的两个数的平均数 实现如下： 12345678class Solution: def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: temp = nums1 + nums2 temp.sort() if len(temp) % 2 ==1: return temp[int(len(temp) / 2)] else: return (temp[int(len(temp) / 2) - 1] + temp[int(len(temp) / 2)]) / 2.0 官方解答这题的官方解答看起来比较复杂，但考虑到上面的解法用到了sort()方法，可能本身就存在算法复杂度高的问题。所以官方解答使用了递归排序的方法。 先不考虑特殊情况，分别将两个数组划分成两部分 123 left_part | right_partA[0], A[1], ..., A[i-1] | A[i], A[i+1], ..., A[m-1]B[0], B[1], ..., B[j-1] | B[j], B[j+1], ..., B[n-1] 那么如果left_part的长度等于right_part的长度，且right_part的值都大于left_part的值（即min(right_part)&gt;=max(left_part)）。我们就找到了中位数： $median=\frac{max(left_part)+min(right_part)}{2}​$ 那么i和j有什么关系呢？ 当总长度$m+n​$为偶数时： 两边长度相等：$i+j=m-i+n-j$ ，所以可以得到$j=\frac{m+n}{2}-i$。 当总长度$m+n​$为奇数时： 左边比右边多1个数：$i+j=m-i+n-j+1$，所以得到：$j=\frac{m+n+1}{2}-i$。 由于$i, j$会不断移动，我们取较大值：$j=\frac{m+n+1}{2}-i$。 先不考虑临界情况，并假设$m&lt;n$，我们需要在$[0,m]$中找到$i$可以使$B[j-1]&lt;=A[i]$且$A[i-1]&lt;=B[j]$。 现在我们可以开始按二叉树进行查找： 初始化i的查找范围$imin = 0​$,$ imax = m​$,$i=\frac{imin+imax}{2}​$。并动态调整$imin,imax​$的值,为了方便可以不需要考虑奇数偶数，通过int()取整即可，如果是奇数那么i与原值一样，会继续调整$imin,imax​$的值，直到下一个 当$i&lt;m, B[j-1]&gt;A[i]$时，我们需要增加$imin = imin + 1$ 当$i&gt;0, A[i-1]&gt;B[j]$时，我们需要减小$imax = imax -1$。 最终结果，要考虑下特殊情况： 左边最大值max_of_left： 当$i=0$时：$B[j-1]$ 当$j=0$时：$A[i-1]$ max(a[i-1], B[j-1]) 当总长度m+n为奇数时，直接返回左边最大值 右边最小值min_of_right: 当$i=m$时：$B[j]$ 当$j=0$时：$A[i]$ Min(A[i], B[j]) 当总长度为偶数时，返回（max_of_left + min_of_right）/ 2 实现如下： 1234567891011121314151617181920212223242526272829303132333435363738class Solution: def median(self, nums1, nums2): # Solution 2 m, n = len(nums1), len(nums2) if m &gt; n: nums1, nums2, m, n = nums2, nums1, n, m if n == 0: raise ValueError imin, imax, half_len = 0, m, int((m + n + 1) / 2) while imin &lt;= imax: i = int((imin + imax) / 2) j = half_len - i if i &lt; m and nums2[j - 1] &gt; nums1[i]: # i is too small, must increase it imin = i + 1 elif i &gt; 0 and nums1[i - 1] &gt; nums2[j]: # i is too big, must decrease it imax = i - 1 else: # i is perfect if i == 0: max_of_left = nums2[j - 1] elif j == 0: max_of_left = nums1[i - 1] else: max_of_left = max(nums1[i - 1], nums2[j - 1]) if (m + n) % 2 == 1: return max_of_left if i == m: min_of_right = nums2[j] elif j == n: min_of_right = nums1[i] else: min_of_right = min(nums1[i], nums2[j]) return (max_of_left + min_of_right) / 2.0 还是有点绕，如果大家有更好的方法或者思路，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-3 无重复字符串的最长子串]]></title>
    <url>%2FLeetCode-3-%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2.html</url>
    <content type="text"><![CDATA[题目：无重复字符串的最长子串给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。 示例 1: 123输入: &quot;abcabcbb&quot;输出: 3 解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。 示例 2: 123输入: &quot;bbbbb&quot;输出: 1解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。 示例 3: 1234输入: &quot;pwwkew&quot;输出: 3解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，&quot;pwke&quot; 是一个子序列，不是子串。 思路由于是在字符串中找无重复的子串，那么就可以用到set()类型。还是得遍历字符串，基本思路如下： 初始化最长无重复子串长度record=0 从头开始遍历字符串中的每一个字符 初始化一个临时的li1=set()来存放子串 判断当前字符是否存在于li1 如果不存在就存入li1，并判断当前子串长度是否大于record，是则更新record 如果存在，那么说明当前子串中有了重复字符，开始下一轮的查找 实现代码如下： 12345678910111213class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: record = 0 for i in range(len(s)): li1 = set() for st in s[i:]: if st not in li1: li1.add(st) if len(li1) &gt; record: record = len(li1) else: break return record 优化方法在上述方法中，每当扎到一个重复字符时，就从下一个i开始重新遍历。我们可以在每个j的遍历过程中，记录下每个子字符串的index和值，并设置一个偏移量carry。如果j和j‘是重复字符，那么偏移量carry就等于i到j的长度，下一次遍历从i=i+carry+1即可。 123456789101112131415161718class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: record = 0 i = 0 while i &lt; len(s): li1 = &#123;&#125; for j, char in enumerate(s[i:]): carry = 0 if char not in li1: li1[char] = j carry += j if len(li1) &gt; record: record = len(li1) else: carry = li1[char] break i = i + carry + 1 return record 优化方法二以上方法均需要做两层的循环，那么有没有只做一层循环的呢： 还是要用dict，我们需要寻找到两个下标i,j，使他们之间的长度最长，思路如下： 初始化i = 0，最长无重复子串长度record=0，一个存放无重复字符串和对应index的dict：li1 通过enumerate(s)的方式在遍历中获取每个字符char和其对应的index：j 如果字符char不存在于li1，那么把char和j存入li1，更新record=max(record, j-i+1) 如果字符char存在于li1，那么说明最长无重复子串需要从li1[char]后开始计算，即i=max(i,li1[char] + 1) 代码如下： 1234567891011class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: record = 0 i = 0 li1 = &#123;&#125; for j, char in enumerate(s): if char in li1: i = max(i, li1[char] + 1) record = max(record, j - i + 1) li1[char] = j return record 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-2 两数相加]]></title>
    <url>%2FLeetCode-2-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0.html</url>
    <content type="text"><![CDATA[题目： 两数相加给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。 如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。 您可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例： 123输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)输出：7 -&gt; 0 -&gt; 8原因：342 + 465 = 807 思路这题涉及数据结构和链表，这在Python中比较少，参考了官方解题思路才大概明白。 首先需要理解ListNode这个class。 12345# Definition for singly-linked list.class ListNode: def __init__(self, x): self.val = x self.next = None 每个ListNode()都有两个属性，一个是本身的值(val)，另一个则是指向另一个ListNode()对象。比如示例中的输出：7 -&gt; 0 -&gt; 8。其实是三个ListNode()对象，产生的过程如下： 12345678tmp = ListNode(0)res = tmptmp = ListNode(7)tmp = tmp.nexttmp.next = ListNode(0)tmp = tmp.nexttmp.next = ListNode(8)return res.next 对ListNode这样的链表有了理解之后，解题就比较方便了： 先初始化一个tmp = ListNode(0) 的临时变量，并将res.next指向tmp，并初始化一个进位临时变量carry=0 遍历l1,l2直到都为None: 如果l1为None，则x值为0，否则x值为l1.val 如果l2为None，则y值为0，否则y值为l2.val x,y相加并加上上一轮相加的进位carry得到tmpSum，该轮tmp的值为和的tmpSum的个位(tmpSum%10)，carry更新为tmpSum的十位(tmpSum//10) 将tmp在指向自己的下一个tmp.next 遍历结束后检查是否还有进位carry，有的话则tmp的下一位为ListNode(1) 返回结果：res.next 完整代码如下： 12345678910111213141516171819202122232425262728293031# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1, l2): tmp = ListNode(0) res = tmp carry=0 while l1 or l2: if l1: x = l1.val else: x = 0 if l2: y = l2.val else: y = 0 tmpSum = carry + x + y carry = tmpSum // 10 tmp.next = ListNode(tmpSum % 10) tmp = tmp.next if l1: l1 = l1.next if l2: l2 = l2.next if carry &gt; 0: tmp.next=ListNode(1) return res.next 优化方法上面方法中需要新建一个临时变量来存放l1,l2相加的结果，由于是相加，我们考虑开始就将结果指向l1，在l1的基础上进行计算，思路如下： 设置结果res=l1,初始化一个进位临时变量carry=0 第一阶段l1,l2都有值的时候 l1.val,l2.val相加并加上上一轮相加的进位carry得到tmpSum，该轮l1.val的值为更新为tmpSum的个位(tmpSum%10)，carry更新为tmpSum的十位(tmpSum//10) 设定一个临时变量prev指向l1 l1,l2均指向自己的下一个l1.next,l2.next 第一阶段结束时，说明l1,l2有一个已经结束了。这时候把l1设定为还未结束的序列（l1 = l1 or l2），并将临时变量prev.next指向新的l1(为了保证序列的连续性，如果前面没有设置的prev临时变量，那么返回结果就中断了) 第二阶段遍历新的l1： l1.val加上上一轮的进位carry得到tmpSum，该轮l1.val的值为更新为tmpSum的个位(tmpSum%10)，carry更新为tmpSum的十位(tmpSum//10) 设定一个临时变量prev指向l1 l1指向自己的下一个l1.next 第二阶段结束后，检查是否还有进位carry，有的话则tmp的下一位为ListNode(1) 返回结果：res 完整代码如下： 12345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1, l2): res = l1 carry = 0 while l1 and l2: tmpSum = l1.val + l2.val + carry carry = tmpSum // 10 l1.val = tmpSum % 10 prev = l1 l1 = l1.next l2 = l2.next l1 = l1 or l2 prev.next = l1 while l1: tmpSum = l1.val + carry carry = tmpSum // 10 l1.val = tmpSum % 10 prev = l1 l1 = l1.next if carry &gt; 0: prev.next = ListNode(1) return res 这道题对Python好像不是很友好，经常会出现超时的情况，如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-1 两数之和]]></title>
    <url>%2FLeetCode-1-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C.html</url>
    <content type="text"><![CDATA[题目：两数之和给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 示例: 1234给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 思路由于题目说明了只有一个对应答案，那么简单粗暴的想法就是进行两次遍历： 先从头取第一个数 将该数依次与后面的每个数相加判断是否等于target 实现： 1234567891011class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ for i in range(len(nums)): for j in range(i+1, len(nums)): if nums[i] + nums[j] == target: return [i, j] 优化方法上面的方法做了两次遍历，那么有没有办法做一次呢 参考了其他网友的答案，果真还是有办法的，先上代码： 12345678910111213class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ nums_dict = &#123;&#125; for index, num in enumerate(nums): another_num = target - num if another_num in nums_dict: return [nums_dict[another_num], index] nums_dict[num] = index 这里主要用到了dict()，思路是： 先新建一个空dict()用于存放列表中的数和对应的索引 先将使用enumerate()函数将列表转组合为一个索引序列，同时列出数据和下标： 12345list(enumerate(nums))[(0, 4), (1, 3), (2, 2), (3, 1)]nums = [2, 7, 11, 15]list(enumerate(nums))[(0, 2), (1, 7), (2, 11), (3, 15)] 然后用目标减去当前的值就获取我们需要找的另一个值 判断另一个值是否存在于dict中，在的话，直接返回两个数值的索引即可，否则将这个数也加入到dict中。 如果大家有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10分钟了解Pandas基础知识]]></title>
    <url>%2F10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3Pandas%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html</url>
    <content type="text"><![CDATA[背景在数据分析中pandas举足轻重，学习pandas最好的方法就是看官方文档，以下是根据官方文档10 Minutes to pandas学习记录。（官方标题10分钟，感觉起码得半个小时吧） 在pandas中主要有两种数据类型，可以简单的理解为： Series：一维数组 DateFrame：二维数组（矩阵） 有了大概的概念之后，开始正式认识pandas: 首先要引入对应的包： 12import numpy as npimport pandas as pd 新建对象 Object Creation Series 可以通过传入一个list对象来新建Series，其中空值为np.nan: 12345678910s = pd.Series([1,3,4,np.nan,7,9])sOut[5]: 0 1.01 3.02 4.03 NaN4 7.05 9.0dtype: float64 pandas会默认创建一列索引index（上面的0-5）。我们也可以在创建时就指定索引： 123456789pd.Series([1,3,4,np.nan,7,9], index=[1,1,2,2,'a',4])Out[9]: 1 1.01 3.02 4.02 NaNa 7.04 9.0dtype: float64 要注意的是，索引是可以重复的，也可以是字符。 DataFrame 新建一个DataFrame对象可以有多种方式： 通过传入一个numpy的数组、指定一个时间的索引以及一个列名。 12345678910111213141516dates = pd.date_range('20190101', periods=6)datesOut[11]: DatetimeIndex(['2019-01-01', '2019-01-02', '2019-01-03', '2019-01-04', '2019-01-05', '2019-01-06'], dtype='datetime64[ns]', freq='D')df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))dfOut[18]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.1743452019-01-06 0.221597 -0.753038 -1.741256 0.287280 通过传入一个dict对象 12345678910111213df2 = pd.DataFrame(&#123;'A':1., 'B':pd.Timestamp('20190101'), 'C':pd.Series(1, index=list(range(4)), dtype='float32'), 'D':np.array([3]*4, dtype='int32'), 'E':pd.Categorical(["test", "tain", "test", "train"]), 'F':'foo'&#125;)df2Out[27]: A B C D E F0 1.0 2019-01-01 1.0 3 test foo1 1.0 2019-01-01 1.0 3 tain foo2 1.0 2019-01-01 1.0 3 test foo3 1.0 2019-01-01 1.0 3 train foo 这里我们指定了不同的类型，可以通过如下查看： 123456789df2.dtypesOut[28]: A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 可以看出DataFrame和Series一样，在没有指定索引时，会自动生成一个数字的索引，这在后续的操作中十分重要。 查看 Viewing Data 查看开头几行或者末尾几行： 1234567891011121314df.head()Out[30]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.174345df.tail(3)Out[31]: A B C D2019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.1743452019-01-06 0.221597 -0.753038 -1.741256 0.287280 可以通过添加行数参数来输出，默认为输出5行。 查看索引和列名 1234567df.indexOut[32]: DatetimeIndex(['2019-01-01', '2019-01-02', '2019-01-03', '2019-01-04', '2019-01-05', '2019-01-06'], dtype='datetime64[ns]', freq='D')df.columnsOut[33]: Index(['A', 'B', 'C', 'D'], dtype='object') 使用DataFrame.to_numpy()转化为numpy数据。需要注意的是由于numpy array类型数据只可包含一种格式，而DataFrame类型数据可包含多种格式，所以在转换过程中，pandas会找到一种可以处理DateFrame中国所有格式的numpy array格式，比如object。这个过程会耗费一定的计算量。 123456789101112131415df.to_numpy()Out[35]: array([[ 0.67162219, 0.78572584, 0.39243527, 0.87469243], [-2.42070338, -1.11620768, -0.34607048, 0.78594081], [ 1.36442543, -0.94764138, 2.38688005, 0.58537186], [-0.48597971, -1.28145415, 0.35406263, -1.41885798], [-1.12271697, -2.78904135, -0.79181242, -0.17434484], [ 0.22159737, -0.75303807, -1.74125564, 0.28728004]])df2.to_numpy()Out[36]: array([[1.0, Timestamp('2019-01-01 00:00:00'), 1.0, 3, 'test', 'foo'], [1.0, Timestamp('2019-01-01 00:00:00'), 1.0, 3, 'tain', 'foo'], [1.0, Timestamp('2019-01-01 00:00:00'), 1.0, 3, 'test', 'foo'], [1.0, Timestamp('2019-01-01 00:00:00'), 1.0, 3, 'train', 'foo']], dtype=object) 上面df全部为float类型，所以转换会很快，而df2涉及多种类型转换，最后全部变成了object类型元素。 查看数据的简要统计结果 1234567891011df.describe()Out[37]: A B C Dcount 6.000000 6.000000 6.000000 6.000000mean -0.295293 -1.016943 0.042373 0.156680std 1.356107 1.144047 1.396030 0.860725min -2.420703 -2.789041 -1.741256 -1.41885825% -0.963533 -1.240143 -0.680377 -0.05893950% -0.132191 -1.031925 0.003996 0.43632675% 0.559116 -0.801689 0.382842 0.735799max 1.364425 0.785726 2.386880 0.874692 转置 1234567df.TOut[38]: 2019-01-01 2019-01-02 2019-01-03 2019-01-04 2019-01-05 2019-01-06A 0.671622 -2.420703 1.364425 -0.485980 -1.122717 0.221597B 0.785726 -1.116208 -0.947641 -1.281454 -2.789041 -0.753038C 0.392435 -0.346070 2.386880 0.354063 -0.791812 -1.741256D 0.874692 0.785941 0.585372 -1.418858 -0.174345 0.287280 按坐标轴排序，其中axis参数为坐标轴，axis默认为0，即横轴（对行排序），axis=1则为纵轴（对列排序）；asceding参数默认为True，即升序排序，ascending=False则为降序排序： 123456789101112131415161718df.sort_index(axis=1)Out[44]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.1743452019-01-06 0.221597 -0.753038 -1.741256 0.287280df.sort_index(axis=1, ascending=False)Out[45]: D C B A2019-01-01 0.874692 0.392435 0.785726 0.6716222019-01-02 0.785941 -0.346070 -1.116208 -2.4207032019-01-03 0.585372 2.386880 -0.947641 1.3644252019-01-04 -1.418858 0.354063 -1.281454 -0.4859802019-01-05 -0.174345 -0.791812 -2.789041 -1.1227172019-01-06 0.287280 -1.741256 -0.753038 0.221597 可见df.sort_index(axis=1)是按列名升序排序，所以看起来没有变化，当设置ascending=False时，列顺序变成了DCBA。 按数值排序： 123456789101112131415161718df.sort_values(by='B')Out[46]: A B C D2019-01-05 -1.122717 -2.789041 -0.791812 -0.1743452019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-06 0.221597 -0.753038 -1.741256 0.2872802019-01-01 0.671622 0.785726 0.392435 0.874692df.sort_values(by='B', ascending=False)Out[47]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-06 0.221597 -0.753038 -1.741256 0.2872802019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 筛选 Selection 获取某列 1234567891011df['A']Out[49]: 2019-01-01 0.6716222019-01-02 -2.4207032019-01-03 1.3644252019-01-04 -0.4859802019-01-05 -1.1227172019-01-06 0.221597Freq: D, Name: A, dtype: float64type(df.A)Out[52]: pandas.core.series.Series 也可直接用df.A，注意这里是大小写敏感的，这时候获取的是一个Series类型数据。 选择多行 123456789101112df[0:3]Out[53]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.585372df['20190102':'20190104']Out[54]: A B C D2019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-04 -0.485980 -1.281454 0.354063 -1.418858 通过一个[]会通过索引对行进行切片，由于前面设置了索引为日期格式，所以可以方便的直接使用日期范围进行筛选。 通过标签选择 选择某行 1234567df.loc[dates[0]]Out[57]: A 0.671622B 0.785726C 0.392435D 0.874692Name: 2019-01-01 00:00:00, dtype: float64 选择指定行列的数据 1234567891011121314151617df.loc[:, ('A', 'C')]Out[58]: A C2019-01-01 0.671622 0.3924352019-01-02 -2.420703 -0.3460702019-01-03 1.364425 2.3868802019-01-04 -0.485980 0.3540632019-01-05 -1.122717 -0.7918122019-01-06 0.221597 -1.741256df.loc['20190102':'20190105', ('A', 'C')]Out[62]: A C2019-01-02 -2.420703 -0.3460702019-01-03 1.364425 2.3868802019-01-04 -0.485980 0.3540632019-01-05 -1.122717 -0.791812 传入第一个参数是行索引标签范围，第二个是列索引标签，:代表全部。 选定某值 1234df.loc['20190102', 'A']Out[69]: -2.420703380445092df.at[dates[1], 'A']Out[70]: -2.420703380445092 可以通过loc[]和at[]两种方式来获取某值，但需要注意的是，由于行索引为datetime类型，使用loc[]方式获取时，可直接使用20190102字符串来代替，而在at[]中，必须传入datetime类型，否则会有报错： 1234567df.at['20190102', 'A'] File "pandas/_libs/index.pyx", line 81, in pandas._libs.index.IndexEngine.get_value File "pandas/_libs/index.pyx", line 89, in pandas._libs.index.IndexEngine.get_value File "pandas/_libs/index.pyx", line 449, in pandas._libs.index.DatetimeEngine.get_loc File "pandas/_libs/index.pyx", line 455, in pandas._libs.index.DatetimeEngine._date_check_typeKeyError: '20190102' 通过位置选择 选择某行 1234567df.iloc[3]Out[71]: A -0.485980B -1.281454C 0.354063D -1.418858Name: 2019-01-04 00:00:00, dtype: float64 iloc[]方法的参数，必须是数值。 选择指定行列的数据 123456789101112131415161718192021df.iloc[3:5, 0:2]Out[72]: A B2019-01-04 -0.485980 -1.2814542019-01-05 -1.122717 -2.789041df.iloc[:,:]Out[73]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 -2.420703 -1.116208 -0.346070 0.7859412019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-04 -0.485980 -1.281454 0.354063 -1.4188582019-01-05 -1.122717 -2.789041 -0.791812 -0.1743452019-01-06 0.221597 -0.753038 -1.741256 0.287280df.iloc[[1, 2, 4], [0, 2]]Out[74]: A C2019-01-02 -2.420703 -0.3460702019-01-03 1.364425 2.3868802019-01-05 -1.122717 -0.791812 同loc[]，:代表全部。 选择某值 1234df.iloc[1, 1]Out[75]: -1.1162076820700824df.iat[1, 1]Out[76]: -1.1162076820700824 可以通过iloc[]和iat[]两种方法获取数值。 按条件判断选择 按某列的数值判断选择 123456df[df.A &gt; 0]Out[77]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-03 1.364425 -0.947641 2.386880 0.5853722019-01-06 0.221597 -0.753038 -1.741256 0.287280 筛选出符合要求的数据 123456789df[df &gt; 0]Out[78]: A B C D2019-01-01 0.671622 0.785726 0.392435 0.8746922019-01-02 NaN NaN NaN 0.7859412019-01-03 1.364425 NaN 2.386880 0.5853722019-01-04 NaN NaN 0.354063 NaN2019-01-05 NaN NaN NaN NaN2019-01-06 0.221597 NaN NaN 0.287280 不符合要求的数据均会被赋值为空NaN。 使用isin()方法筛选 12345678910111213141516171819202122232425df2 = df.copy()df2['E'] = ['one', 'one', 'two', 'three', 'four', 'three']df2Out[88]: A B C D E2019-01-01 0.671622 0.785726 0.392435 0.874692 one2019-01-02 -2.420703 -1.116208 -0.346070 0.785941 one2019-01-03 1.364425 -0.947641 2.386880 0.585372 two2019-01-04 -0.485980 -1.281454 0.354063 -1.418858 three2019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 four2019-01-06 0.221597 -0.753038 -1.741256 0.287280 threedf2['E'].isin(['two', 'four'])Out[89]: 2019-01-01 False2019-01-02 False2019-01-03 True2019-01-04 False2019-01-05 True2019-01-06 FalseFreq: D, Name: E, dtype: booldf2[df2['E'].isin(['two', 'four'])]Out[90]: A B C D E2019-01-03 1.364425 -0.947641 2.386880 0.585372 two2019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 four 注意：isin必须严格一致才行，df中的默认数值小数点位数很长，并非显示的5位，为了方便展示，所以新增了E列。直接用原数值，情况如下，可看出[1,1]位置符合要求。 123456789df.isin([-1.1162076820700824])Out[95]: A B C D2019-01-01 False False False False2019-01-02 False True False False2019-01-03 False False False False2019-01-04 False False False False2019-01-05 False False False False2019-01-06 False False False False 设定值 通过指定索引设定列 1234567891011121314151617181920s1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range('20190102', periods=6))s1Out[98]: 2019-01-02 12019-01-03 22019-01-04 32019-01-05 42019-01-06 52019-01-07 6Freq: D, dtype: int64df['F']=s1dfOut[101]: A B C D F2019-01-01 0.671622 0.785726 0.392435 0.874692 NaN2019-01-02 -2.420703 -1.116208 -0.346070 0.785941 1.02019-01-03 1.364425 -0.947641 2.386880 0.585372 2.02019-01-04 -0.485980 -1.281454 0.354063 -1.418858 3.02019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 4.02019-01-06 0.221597 -0.753038 -1.741256 0.287280 5.0 空值会自动填充为NaN。 通过标签设定值 12345678910df.at[dates[0], &apos;A&apos;] = 0dfOut[103]: A B C D F2019-01-01 0.000000 0.785726 0.392435 0.874692 NaN2019-01-02 -2.420703 -1.116208 -0.346070 0.785941 1.02019-01-03 1.364425 -0.947641 2.386880 0.585372 2.02019-01-04 -0.485980 -1.281454 0.354063 -1.418858 3.02019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 4.02019-01-06 0.221597 -0.753038 -1.741256 0.287280 5.0 通过为止设定值 12345678910df.iat[0, 1] = 0dfOut[105]: A B C D F2019-01-01 0.000000 0.000000 0.392435 0.874692 NaN2019-01-02 -2.420703 -1.116208 -0.346070 0.785941 1.02019-01-03 1.364425 -0.947641 2.386880 0.585372 2.02019-01-04 -0.485980 -1.281454 0.354063 -1.418858 3.02019-01-05 -1.122717 -2.789041 -0.791812 -0.174345 4.02019-01-06 0.221597 -0.753038 -1.741256 0.287280 5.0 通过NumPy array设定值 12345678910df.loc[:, 'D'] = np.array([5] * len(df))dfOut[109]: A B C D F2019-01-01 0.000000 0.000000 0.392435 5 NaN2019-01-02 -2.420703 -1.116208 -0.346070 5 1.02019-01-03 1.364425 -0.947641 2.386880 5 2.02019-01-04 -0.485980 -1.281454 0.354063 5 3.02019-01-05 -1.122717 -2.789041 -0.791812 5 4.02019-01-06 0.221597 -0.753038 -1.741256 5 5.0 通过条件判断设定值 1234567891011df2 = df.copy()df2[df2 &gt; 0] = -df2df2Out[112]: A B C D F2019-01-01 0.000000 0.000000 -0.392435 -5 NaN2019-01-02 -2.420703 -1.116208 -0.346070 -5 -1.02019-01-03 -1.364425 -0.947641 -2.386880 -5 -2.02019-01-04 -0.485980 -1.281454 -0.354063 -5 -3.02019-01-05 -1.122717 -2.789041 -0.791812 -5 -4.02019-01-06 -0.221597 -0.753038 -1.741256 -5 -5.0 空值处理 Missing Datapandas默认使用np.nan来表示空值，在统计计算中会直接忽略。 通过reindex()方法可以新增、修改、删除某坐标轴（行或列）的索引，并返回一个数据的拷贝： 123456789df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])df1.loc[dates[0]:dates[1], 'E'] = 1df1Out[115]: A B C D F E2019-01-01 0.000000 0.000000 0.392435 5 NaN 1.02019-01-02 -2.420703 -1.116208 -0.346070 5 1.0 1.02019-01-03 1.364425 -0.947641 2.386880 5 2.0 NaN2019-01-04 -0.485980 -1.281454 0.354063 5 3.0 NaN 删除空值 1234df1.dropna(how='any')Out[116]: A B C D F E2019-01-02 -2.420703 -1.116208 -0.34607 5 1.0 1.0 填充空值 1234567df1.fillna(value=5)Out[117]: A B C D F E2019-01-01 0.000000 0.000000 0.392435 5 5.0 1.02019-01-02 -2.420703 -1.116208 -0.346070 5 1.0 1.02019-01-03 1.364425 -0.947641 2.386880 5 2.0 5.02019-01-04 -0.485980 -1.281454 0.354063 5 3.0 5.0 判断是否为空值 1234567pd.isna(df1)Out[118]: A B C D F E2019-01-01 False False False False True False2019-01-02 False False False False False False2019-01-03 False False False False False True2019-01-04 False False False False False True 运算 Operations 统计 注意 所有的统计默认是不包含空值的 平均值 默认情况是按列求平均值： 12345678df.mean()Out[119]: A -0.407230B -1.147897C 0.042373D 5.000000F 3.000000dtype: float64 如果需要按行求平均值，需指定轴参数： 123456789df.mean(1)Out[120]: 2019-01-01 1.3481092019-01-02 0.4234042019-01-03 1.9607332019-01-04 1.3173262019-01-05 0.8592862019-01-06 1.545461Freq: D, dtype: float64 数值移动 1234567891011121314151617181920s = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates)sOut[122]: 2019-01-01 1.02019-01-02 3.02019-01-03 5.02019-01-04 NaN2019-01-05 6.02019-01-06 8.0Freq: D, dtype: float64s = s.shift(2)sOut[125]: 2019-01-01 NaN2019-01-02 NaN2019-01-03 1.02019-01-04 3.02019-01-05 5.02019-01-06 NaNFreq: D, dtype: float64 这里将s的值移动两个，那么空出的部分会自动使用NaN填充。 不同维度间的运算，pandas会自动扩展维度： 123456789df.sub(s, axis='index')Out[128]: A B C D F2019-01-01 NaN NaN NaN NaN NaN2019-01-02 NaN NaN NaN NaN NaN2019-01-03 0.364425 -1.947641 1.386880 4.0 1.02019-01-04 -3.485980 -4.281454 -2.645937 2.0 0.02019-01-05 -6.122717 -7.789041 -5.791812 0.0 -1.02019-01-06 NaN NaN NaN NaN NaN 应用 通过apply()方法，可以对数据进行逐一操作: 累计求和 123456789df.apply(np.cumsum)Out[130]: A B C D F2019-01-01 0.000000 0.000000 0.392435 5 NaN2019-01-02 -2.420703 -1.116208 0.046365 10 1.02019-01-03 -1.056278 -2.063849 2.433245 15 3.02019-01-04 -1.542258 -3.345303 2.787307 20 6.02019-01-05 -2.664975 -6.134345 1.995495 25 10.02019-01-06 -2.443377 -6.887383 0.254239 30 15.0 这里使用了apply()方法调用np.cumsum方法，也可直接使用df.cumsum(): 123456789df.cumsum()Out[133]: A B C D F2019-01-01 0.000000 0.000000 0.392435 5.0 NaN2019-01-02 -2.420703 -1.116208 0.046365 10.0 1.02019-01-03 -1.056278 -2.063849 2.433245 15.0 3.02019-01-04 -1.542258 -3.345303 2.787307 20.0 6.02019-01-05 -2.664975 -6.134345 1.995495 25.0 10.02019-01-06 -2.443377 -6.887383 0.254239 30.0 15.0 自定义方法 通过自定义函数，配合apply()方法，可以实现更多数据处理： 12345678df.apply(lambda x: x.max() - x.min())Out[134]: A 3.785129B 2.789041C 4.128136D 0.000000F 4.000000dtype: float64 矩阵 统计矩阵中每个元素出现的频次： 1234567891011121314151617181920212223s = pd.Series(np.random.randint(0, 7, size=10))sOut[136]: 0 21 02 43 04 35 36 67 48 69 5dtype: int64s.value_counts()Out[137]: 6 24 23 20 25 12 1dtype: int64 String方法 所有的Series类型都可以直接调用str的属性方法来对每个对象进行操作。 比如转换成大写： 12345678910111213s = pd.Series([&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;Aaba&apos;, &apos;Baca&apos;, np.nan, &apos;CABA&apos;, &apos;dog&apos;, &apos;cat&apos;])s.str.upper()Out[139]: 0 A1 B2 C3 AABA4 BACA5 NaN6 CABA7 DOG8 CATdtype: object 分列： 1234567891011s = pd.Series(['A,b', 'c,d'])sOut[142]: 0 A,b1 c,ddtype: objects.str.split(',', expand=True)Out[143]: 0 10 A b1 c d 其他方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647dir(str)Out[140]: [&apos;capitalize&apos;, &apos;casefold&apos;, &apos;center&apos;, &apos;count&apos;, &apos;encode&apos;, &apos;endswith&apos;, &apos;expandtabs&apos;, &apos;find&apos;, &apos;format&apos;, &apos;format_map&apos;, &apos;index&apos;, &apos;isalnum&apos;, &apos;isalpha&apos;, &apos;isascii&apos;, &apos;isdecimal&apos;, &apos;isdigit&apos;, &apos;isidentifier&apos;, &apos;islower&apos;, &apos;isnumeric&apos;, &apos;isprintable&apos;, &apos;isspace&apos;, &apos;istitle&apos;, &apos;isupper&apos;, &apos;join&apos;, &apos;ljust&apos;, &apos;lower&apos;, &apos;lstrip&apos;, &apos;maketrans&apos;, &apos;partition&apos;, &apos;replace&apos;, &apos;rfind&apos;, &apos;rindex&apos;, &apos;rjust&apos;, &apos;rpartition&apos;, &apos;rsplit&apos;, &apos;rstrip&apos;, &apos;split&apos;, &apos;splitlines&apos;, &apos;startswith&apos;, &apos;strip&apos;, &apos;swapcase&apos;, &apos;title&apos;, &apos;translate&apos;, &apos;upper&apos;, &apos;zfill&apos;] 合并 Mergepandas`可以提供很多方法可以快速的合并各种类型的Series、DataFrame以及Panel Object。 Concat方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344df = pd.DataFrame(np.random.randn(10, 4))dfOut[145]: 0 1 2 30 -0.227408 -0.185674 -0.187919 0.1856851 1.132517 -0.539992 1.156631 -0.0224682 0.214134 -1.283055 -0.862972 0.5189423 0.785903 1.033915 -0.471496 -1.4037624 -0.676717 -0.529971 -1.161988 -1.2650715 0.670126 1.320960 -0.128098 0.7186316 0.589902 0.349386 0.221955 1.7491887 -0.328885 0.607929 -0.973610 -0.9284728 1.724243 -0.661503 -0.374254 0.4092509 1.346625 0.618285 0.528776 -0.628470# break it into piecespieces = [df[:3], df[3:7], df[7:]]piecesOut[147]: [ 0 1 2 3 0 -0.227408 -0.185674 -0.187919 0.185685 1 1.132517 -0.539992 1.156631 -0.022468 2 0.214134 -1.283055 -0.862972 0.518942, 0 1 2 3 3 0.785903 1.033915 -0.471496 -1.403762 4 -0.676717 -0.529971 -1.161988 -1.265071 5 0.670126 1.320960 -0.128098 0.718631 6 0.589902 0.349386 0.221955 1.749188, 0 1 2 3 7 -0.328885 0.607929 -0.973610 -0.928472 8 1.724243 -0.661503 -0.374254 0.409250 9 1.346625 0.618285 0.528776 -0.628470]pd.concat(pieces)Out[148]: 0 1 2 30 -0.227408 -0.185674 -0.187919 0.1856851 1.132517 -0.539992 1.156631 -0.0224682 0.214134 -1.283055 -0.862972 0.5189423 0.785903 1.033915 -0.471496 -1.4037624 -0.676717 -0.529971 -1.161988 -1.2650715 0.670126 1.320960 -0.128098 0.7186316 0.589902 0.349386 0.221955 1.7491887 -0.328885 0.607929 -0.973610 -0.9284728 1.724243 -0.661503 -0.374254 0.4092509 1.346625 0.618285 0.528776 -0.628470 Merge方法 这是类似sql的合并方法： 12345678910111213141516171819left = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'lval': [1, 2]&#125;)right = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'rval': [4, 5]&#125;)leftOut[151]: key lval0 foo 11 foo 2rightOut[152]: key rval0 foo 41 foo 5pd.merge(left, right, on='key')Out[153]: key lval rval0 foo 1 41 foo 1 52 foo 2 43 foo 2 5 另一个例子： 1234567891011121314151617left = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'lval': [1, 2]&#125;)right = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'rval': [4, 5]&#125;)leftOut[156]: key lval0 foo 11 bar 2rightOut[157]: key rval0 foo 41 bar 5pd.merge(left, right, on='key')Out[158]: key lval rval0 foo 1 41 bar 2 5 Append方法 在DataFrame中增加行 1234567891011121314151617181920212223242526272829303132df = pd.DataFrame(np.random.randn(8, 4), columns=['A', 'B', 'C', 'D'])dfOut[160]: A B C D0 -0.496709 0.573449 0.076059 0.6852851 0.479253 0.587376 -1.240070 -0.9079102 -0.052609 -0.287786 -1.949402 1.1633233 -0.659489 0.525583 0.820922 -1.3685444 1.270453 -1.813249 0.059915 0.5867035 1.859657 0.564274 -0.198763 -1.7941736 -0.649153 -3.129258 0.063418 -0.7279367 0.862402 -0.800031 -1.954784 -0.028607s = df.iloc[3]sOut[162]: A -0.659489B 0.525583C 0.820922D -1.368544Name: 3, dtype: float64df.append(s, ignore_index=True)Out[163]: A B C D0 -0.496709 0.573449 0.076059 0.6852851 0.479253 0.587376 -1.240070 -0.9079102 -0.052609 -0.287786 -1.949402 1.1633233 -0.659489 0.525583 0.820922 -1.3685444 1.270453 -1.813249 0.059915 0.5867035 1.859657 0.564274 -0.198763 -1.7941736 -0.649153 -3.129258 0.063418 -0.7279367 0.862402 -0.800031 -1.954784 -0.0286078 -0.659489 0.525583 0.820922 -1.368544 这里要注意，我们增加了ignore_index=True参数，如果不设置的话，那么增加的新行的index仍然是3，这样在后续的处理中可能有存在问题。具体也需要看情况来处理。 123456789101112df.append(s)Out[164]: A B C D0 -0.496709 0.573449 0.076059 0.6852851 0.479253 0.587376 -1.240070 -0.9079102 -0.052609 -0.287786 -1.949402 1.1633233 -0.659489 0.525583 0.820922 -1.3685444 1.270453 -1.813249 0.059915 0.5867035 1.859657 0.564274 -0.198763 -1.7941736 -0.649153 -3.129258 0.063418 -0.7279367 0.862402 -0.800031 -1.954784 -0.0286073 -0.659489 0.525583 0.820922 -1.368544 分组 Grouping一般分组统计有三个步骤： 分组：选择需要的数据 计算：对每个分组进行计算 合并：把分组计算的结果合并为一个数据结构中 12345678910111213141516df = pd.DataFrame(&#123;'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'], 'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'], 'C': np.random.randn(8), 'D': np.random.randn(8)&#125;)dfOut[166]: A B C D0 foo one -1.252153 0.1728631 bar one 0.238547 -0.6489802 foo two 0.756975 0.1957663 bar three -0.933405 -0.3200434 foo two -0.310650 -1.3882555 bar two 1.568550 -1.9118176 foo one -0.340290 -2.141259 按A列分组并使用sum函数进行计算： 123456df.groupby('A').sum()Out[167]: C DA bar 0.873692 -2.880840foo -1.817027 -5.833961 这里由于B列无法应用sum函数，所以直接被忽略了。 按A、B列分组并使用sum函数进行计算： 12345678910df.groupby(['A', 'B']).sum()Out[168]: C DA B bar one 0.238547 -0.648980 three -0.933405 -0.320043 two 1.568550 -1.911817foo one -1.592443 -1.968396 three -0.670909 -2.673075 two 0.446325 -1.192490 这样就有了一个多层index的结果集。 整形 Reshaping 堆叠 Stack python的zip函数可以将对象中对应的元素打包成一个个的元组： 123456789101112131415161718192021222324252627282930313233343536373839404142434445tuples = list(zip(['bar', 'bar', 'baz', 'baz','foo', 'foo', 'qux', 'qux'],['one', 'two', 'one', 'two','one', 'two', 'one', 'two']))tuplesOut[172]: [('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')]## 设置两级索引index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])indexOut[174]: MultiIndex(levels=[['bar', 'baz', 'foo', 'qux'], ['one', 'two']], codes=[[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]], names=['first', 'second'])## 创建DataFramedf = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])dfOut[176]: A Bfirst second bar one -0.501215 -0.947993 two -0.828914 0.232167baz one 1.245419 1.006092 two 1.016656 -0.441073foo one 0.479037 -0.500034 two -1.113097 0.591696qux one -0.014760 -0.320735 two -0.648743 1.499899## 选取DataFramedf2 = df[:4]df2Out[179]: A Bfirst second bar one -0.501215 -0.947993 two -0.828914 0.232167baz one 1.245419 1.006092 two 1.016656 -0.441073 使用stack()方法，可以通过堆叠的方式将二维数据变成为一维数据： 12345678910111213stacked = df2.stack()stackedOut[181]: first second bar one A -0.501215 B -0.947993 two A -0.828914 B 0.232167baz one A 1.245419 B 1.006092 two A 1.016656 B -0.441073dtype: float64 对应的逆操作为unstacked()方法： 123456789101112131415161718192021222324stacked.unstack()Out[182]: A Bfirst second bar one -0.501215 -0.947993 two -0.828914 0.232167baz one 1.245419 1.006092 two 1.016656 -0.441073stacked.unstack(1)Out[183]: second one twofirst bar A -0.501215 -0.828914 B -0.947993 0.232167baz A 1.245419 1.016656 B 1.006092 -0.441073stacked.unstack(0)Out[184]: first bar bazsecond one A -0.501215 1.245419 B -0.947993 1.006092two A -0.828914 1.016656 B 0.232167 -0.441073 unstack()默认对最后一层级进行操作，也可通过输入参数指定。 表格转置 1234567891011121314151617181920df = pd.DataFrame(&#123;'A': ['one', 'one', 'two', 'three'] * 3,'B': ['A', 'B', 'C'] * 4,'C': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2,'D': np.random.randn(12),'E': np.random.randn(12)&#125;)dfOut[190]: A B C D E0 one A foo -0.933264 -2.3874901 one B foo -0.288101 0.0232142 two C foo 0.594490 0.4185053 three A bar 0.450683 1.9396234 one B bar 0.243897 -0.9657835 one C bar -0.705494 -0.0782836 two A foo 1.560352 0.4199077 three B foo 0.199453 0.9987118 one C foo 1.426861 -1.1082979 one A bar -0.570951 -0.02256010 two B bar -0.350937 -1.76780411 three C bar 0.983465 0.065792 通过pivot_table()方法可以很方便的进行行列的转换： 12345678910111213pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'])Out[191]: C bar fooA B one A -0.570951 -0.933264 B 0.243897 -0.288101 C -0.705494 1.426861three A 0.450683 NaN B NaN 0.199453 C 0.983465 NaNtwo A NaN 1.560352 B -0.350937 NaN C NaN 0.594490 转换中，涉及到空值部分会自动填充为NaN。 时间序列 Time Seriespandas的在时序转换方面十分强大，可以很方便的进行各种转换。 时间间隔调整 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748rng = pd.date_range('1/1/2019', periods=100, freq='S')rng[:5]Out[214]: DatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 00:00:01', '2019-01-01 00:00:02', '2019-01-01 00:00:03', '2019-01-01 00:00:04'], dtype='datetime64[ns]', freq='S')ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)ts.head(5)Out[216]: 2019-01-01 00:00:00 2452019-01-01 00:00:01 3472019-01-01 00:00:02 1132019-01-01 00:00:03 1962019-01-01 00:00:04 131Freq: S, dtype: int64## 按10s间隔进行重新采样ts1 = ts.resample('10S')ts1Out[209]: DatetimeIndexResampler [freq=&lt;10 * Seconds&gt;, axis=0, closed=left, label=left, convention=start, base=0]## 用求平均的方式进行数据整合 ts1.mean()Out[218]: 2019-01-01 00:00:00 174.02019-01-01 00:00:10 278.52019-01-01 00:00:20 281.82019-01-01 00:00:30 337.22019-01-01 00:00:40 221.02019-01-01 00:00:50 277.12019-01-01 00:01:00 171.02019-01-01 00:01:10 321.02019-01-01 00:01:20 318.62019-01-01 00:01:30 302.6Freq: 10S, dtype: float64## 用求和的方式进行数据整合 ts1.sum()Out[219]: 2019-01-01 00:00:00 17402019-01-01 00:00:10 27852019-01-01 00:00:20 28182019-01-01 00:00:30 33722019-01-01 00:00:40 22102019-01-01 00:00:50 27712019-01-01 00:01:00 17102019-01-01 00:01:10 32102019-01-01 00:01:20 31862019-01-01 00:01:30 3026Freq: 10S, dtype: int64 这里先通过resample进行重采样，在指定sum()或者mean()等方式来指定冲采样的处理方式。 显示时区： 123456789101112131415161718192021222324rng = pd.date_range('1/1/2019 00:00', periods=5, freq='D')rngOut[221]: DatetimeIndex(['2019-01-01', '2019-01-02', '2019-01-03', '2019-01-04', '2019-01-05'], dtype='datetime64[ns]', freq='D')ts = pd.Series(np.random.randn(len(rng)), rng)tsOut[223]: 2019-01-01 -2.3276862019-01-02 1.5278722019-01-03 0.0639822019-01-04 -0.2135722019-01-05 -0.014856Freq: D, dtype: float64ts_utc = ts.tz_localize('UTC')ts_utcOut[225]: 2019-01-01 00:00:00+00:00 -2.3276862019-01-02 00:00:00+00:00 1.5278722019-01-03 00:00:00+00:00 0.0639822019-01-04 00:00:00+00:00 -0.2135722019-01-05 00:00:00+00:00 -0.014856Freq: D, dtype: float64 转换时区： 12345678ts_utc.tz_convert(&apos;US/Eastern&apos;)Out[226]: 2018-12-31 19:00:00-05:00 -2.3276862019-01-01 19:00:00-05:00 1.5278722019-01-02 19:00:00-05:00 0.0639822019-01-03 19:00:00-05:00 -0.2135722019-01-04 19:00:00-05:00 -0.014856Freq: D, dtype: float64 时间格式转换 123456789101112131415161718192021222324252627rng = pd.date_range('1/1/2019', periods=5, freq='M')ts = pd.Series(np.random.randn(len(rng)), index=rng)tsOut[230]: 2019-01-31 0.1971342019-02-28 0.5690822019-03-31 -0.3221412019-04-30 0.0057782019-05-31 -0.082306Freq: M, dtype: float64ps = ts.to_period()psOut[232]: 2019-01 0.1971342019-02 0.5690822019-03 -0.3221412019-04 0.0057782019-05 -0.082306Freq: M, dtype: float64ps.to_timestamp()Out[233]: 2019-01-01 0.1971342019-02-01 0.5690822019-03-01 -0.3221412019-04-01 0.0057782019-05-01 -0.082306Freq: MS, dtype: float64 在是时间段和时间转换过程中，有一些很方便的算术方法可以使用，比如我们转换如下两个频率： 1、按季度划分，且每个年的最后一个月是11月。 2、按季度划分，每个月开始为频率一中下一个月的早上9点。 123456789101112131415161718192021222324252627282930prng = pd.period_range('2018Q1', '2019Q4', freq='Q-NOV')prngOut[243]: PeriodIndex(['2018Q1', '2018Q2', '2018Q3', '2018Q4', '2019Q1', '2019Q2', '2019Q3', '2019Q4'], dtype='period[Q-NOV]', freq='Q-NOV')ts = pd.Series(np.random.randn(len(prng)), prng)tsOut[245]: 2018Q1 -0.1126922018Q2 -0.5073042018Q3 -0.3248462018Q4 0.5496712019Q1 -0.8977322019Q2 1.1300702019Q3 -0.3998142019Q4 0.830488Freq: Q-NOV, dtype: float64ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9tsOut[247]: 2018-03-01 09:00 -0.1126922018-06-01 09:00 -0.5073042018-09-01 09:00 -0.3248462018-12-01 09:00 0.5496712019-03-01 09:00 -0.8977322019-06-01 09:00 1.1300702019-09-01 09:00 -0.3998142019-12-01 09:00 0.830488Freq: H, dtype: float64 注意：这个例子有点怪。可以这样理解，我们先将prng直接转换为按小时显示： 123456prng.asfreq('H', 'end') Out[253]: PeriodIndex(['2018-02-28 23:00', '2018-05-31 23:00', '2018-08-31 23:00', '2018-11-30 23:00', '2019-02-28 23:00', '2019-05-31 23:00', '2019-08-31 23:00', '2019-11-30 23:00'], dtype='period[H]', freq='H') 我们要把时间转换为下一个月的早上9点，所以先转换为按月显示，并每个月加1（即下个月），然后按小时显示并加9（早上9点）。 另外例子中s参数是start的简写，e参数是end的简写，Q-NOV即表示按季度，且每年的NOV是最后一个月。 更多了freq简称可以参考：http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#anchored-offsets asfreq（）方法介绍可参考：http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.asfreq.html#pandas-dataframe-asfreq 分类目录类型 Categoricals 关于Categories类型介绍可以参考：http://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html#categorical 类型转换：astype(&#39;category&#39;) 12345678910111213141516171819202122df = pd.DataFrame(&#123;"id": [1, 2, 3, 4, 5, 6],"raw_grade": ['a', 'b', 'b', 'a', 'a', 'e']&#125;)dfOut[255]: id raw_grade0 1 a1 2 b2 3 b3 4 a4 5 a5 6 edf['grade'] = df['raw_grade'].astype('category')df['grade']Out[257]: 0 a1 b2 b3 a4 a5 eName: grade, dtype: categoryCategories (3, object): [a, b, e] 重命名分类：cat 1234567891011df["grade"].cat.categories = ["very good", "good", "very bad"]df['grade']Out[269]: 0 very good1 good2 good3 very good4 very good5 very badName: grade, dtype: categoryCategories (3, object): [very good, good, very bad] 重分类： 1234567891011df['grade'] = df['grade'].cat.set_categories(["very bad", "bad", "medium","good", "very good"])df['grade']Out[271]: 0 very good1 good2 good3 very good4 very good5 very badName: grade, dtype: categoryCategories (5, object): [very bad, bad, medium, good, very good] 排列 123456789df.sort_values(by="grade")Out[272]: id raw_grade grade5 6 e very bad1 2 b good2 3 b good0 1 a very good3 4 a very good4 5 a very good 分组 123456789df.groupby("grade").size()Out[273]: gradevery bad 1bad 0medium 0good 2very good 3dtype: int64 画图 Plotting Series 123456789ts = pd.Series(np.random.randn(1000),index=pd.date_range('1/1/2000', periods=1000))ts = pd.Series(np.random.randn(1000),index=pd.date_range('1/1/2019', periods=1000))ts = ts.cumsum()ts.plot()Out[277]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1135bcc50&gt;import matplotlib.pyplot as pltplt.show() DataFrame画图 使用plot可以把所有的列都通过标签的形式展示出来： 12345678df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index,columns=['A', 'B', 'C', 'D'])df = df.cumsum()plt.figure()Out[282]: &lt;Figure size 640x480 with 0 Axes&gt;df.plot()Out[283]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x11587e4e0&gt;plt.legend(loc='best') 导入导出数据 Getting Data In/Out CSV 写入： 1df.to_csv('foo.csv') 读取： 1pd.read_csv('foo.csv') HDF5 写入： 1df.to_hdf('foo.h5', 'df') 读取： 1pd.read_hdf('foo.h5', 'df') Excel 写入： 1df.to_excel('foo.xlsx', sheet_name='Sheet1') 读取： 1pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA']) 异常处理 Gotchas如果有一些异常情况比如： 12345&gt;&gt;&gt; if pd.Series([False, True, False]):... print("I was true")Traceback ...ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all(). 可以参考如下链接： http://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html#basics-compare http://pandas.pydata.org/pandas-docs/stable/user_guide/gotchas.html#gotchas]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib中文展示乱码问题]]></title>
    <url>%2Fmatplotlib%E4%B8%AD%E6%96%87%E5%B1%95%E7%A4%BA%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景使用Python做数据分析时，不可避免的需要使用matplotlib画图。但它对中文并不友好，使用中文作为标签或者抬头会有乱码方框的问题： 解决方案方案一：设置中文字体matplotlib提供了FontProperties对象，可以通过手工设置字体来格式化生成的展示图片。具体实现如下： 找到中文字体文件 在Mac下，可通过打开“字体册”应用找到对应的字体和字体所在的路径（默认为：/System/Library/Fonts/PingFang.ttc） 在windows下，一般在C:\Windows\Fonts目录下。 也可直接网上下载需要的字体 配置字体属性 12from matplotlib.font_manager import FontPropertiesfont = FontProperties(fname='/System/Library/Fonts/PingFang.ttc') 引用字体属性 1plt.title("中文测试", fontproperties=font) 画图即可： 方案二： 使用第三方库为解决中文画图的问题，强大的网友自己做了第三方支持中文库可以方便的展示中文，且支持 matplotlib 混合编程, 完全相同的API设计。 安装pyployz 1pip install pyplotz 使用pyplotz和matplotlib混合配置 12345678910import matplotlib.pyplot as pltplt.plot()# 新建pltz对象，用于显示中文from pyplotz.pyplotz import PyplotZpltz = PyplotZ()pltz.enable_chinese()pltz.title('中文测试')plt.savefig('Chinese_1')plt.show() 效果如下： 更多介绍可以参考官方说明：https://github.com/songlinhou/pyplotz 总结两种方式都可以实现中文画图，方法一灵活性更好，但每行涉及到中文的地方都需要配置，方法二需要另外引入第三方包，并进行初始化，在首次使用时会自动下载中文字符集。 建议大家还是按需选用，如果有更好的方法，欢迎交流。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用snownlp分析流浪地球评论]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8snownlp%E5%88%86%E6%9E%90%E6%B5%81%E6%B5%AA%E5%9C%B0%E7%90%83%E8%AF%84%E8%AE%BA.html</url>
    <content type="text"><![CDATA[背景前面通过爬虫获取了猫眼上”流浪地球“的评论：获取猫眼电影的评论，并制作了词云，使用pandas初步分析了电影热度情况，还可以分析什么呢？ SnowNLP是个开源的NPL分析库，可以方便的进行中文分词，特点是可以进行情感评测，来分析评论的积极程度，主要用于电商网站中的商品评论的分析。那么分析下电影评论情况如何呢？ 思路前面我们再抓取时，将评论的时间和内容通过csv的格式保存下来，并使用;分割。 读取数据pandas提供read_csv方法来直接独处数据保存为DateFrame格式。 1df = pd.read_csv('comment.csv', sep=';', header=None) 获取情感评分通过pandas读取csv后，评论会保存在df[1]中，我们需要对每一条评论进行情感评分，pandas提供了apply功能配合lambda就可以方便的实现了： 12sentiment = lambda x:SnowNLP(x).sentimentsdf[2] = df[1].apply(sentiment) 数据处理 设置数据列名 1df.columns = ['date', 'comment', 'sentiment'] 时间日期处理 在date列，我们保存的数据格式是string，需要把转换为日期格式才能进一步处理。 1df['date'] = pd.to_datetime(df['date']) 我们需要按时间来统计，所以把date列设置为index: 1df = df.set_index('date') 统计处理 日期筛选 由于我们知道《流浪地球》是2月5日上映的，我们可以对日期进行限定，以免出现有些在上映前的评论，会占用大段的空白情况。 设置index之后，可以参考list类型操作，由于时间是倒序的，所以可以直接使用[:&#39;2019-02-04&#39;]来选取2月4日之后到今天的所有数据，并选取sentiment列。 1cacu_df = df[:'2019-02-04']['sentiment'] 按日期进行数量统计 pandas中，通过resample方法进行重新按日采样，并求汇总的平均值。 1cacu = cacu_df.resample('D').mean() 这样就完成了按日期求和统计操作。 绘图画图需要使用matplotlib库，通过导入该库，可直接对DateFrame对象进行画图处理。画图及图表格式化如下： 12345678910111213141516171819# 通过设置中文字体方式解决中文展示问题font = FontProperties(fname='../font/PingFang.ttc')plt.title("流浪地球评论分析", fontproperties=font)plt.xlabel("日期", fontproperties=font)plt.ylabel("好感度", fontproperties=font)plt.plot(cacu)plt.axis("tight")# 显示网格plt.grid(True)# 自动旋转横轴日期plt.gcf().autofmt_xdate()# 显示数值for a, b in zip(cacu.index, cacu.values): plt.text(a, b, str(round(b, 4)))# 保存图片plt.savefig('comment_sentiment_analysis.png')# 查看图片plt.show() 结论以上就是使用抓取的评论生成情感分析统计图片的大致思路，完成的实现代码请见：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/maoyan 结果如下： 根据SnowNLP，sentiment值是指：positive的概率，可以简单理解为分值大于0.5为积极，小于为消极。那么这个结论就很奇怪了，那么我们看下每个评论的评分情况： 123456789100 中国科幻电影，继续加油↖(^ω^)↗ 0.51 特效什么的真的不错，不过俩小主演感觉演绎经验不是很足，达叔演技是真的很好，，每个他的镜头还是... 0.52 的确是国产中最好的科幻片了 0.53 非常棒，特效完全是大师水准 0.54 完美。。。。。。。。。。 0.55 挺不错，大型科幻片，很有意义 0.56 简直是侮辱我的智商啊，一点常识都没有，。这种片子怎么能九点多分，刷出来的吧。 0.57 太好看了赞...大卖.. 0.58 虽然和最理想的大片上有一定差距，但是已经明显能够感受到慢慢的诚意，值得肯定 0.59 酷炫吊炸天，第一次看我们国家出来这么厉害的科幻，希望以后哆哆出。一定会支持！！ 0.5 这个评分。。。相当不准。。。 再看下官网的说明： 情感分析（现在训练数据主要是买卖东西时的评价，所以对其他的一些可能效果不是很好，待解决） 好吧，希望以后可以尽快解决啦]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Pandas分析流浪地球评论数据]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8Pandas%E5%88%86%E6%9E%90%E6%B5%81%E6%B5%AA%E5%9C%B0%E7%90%83%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE.html</url>
    <content type="text"><![CDATA[背景前面通过爬虫获取了猫眼上”流浪地球“的评论：获取猫眼电影的评论，除了做词云之外，还可以对数据进行分析，看看大家对电影的反响如何。 思路前面我们再抓取时，将评论的时间和内容通过csv的格式保存下来，并使用;分割。读取csv文件并统计处理就要用到大名鼎鼎的pandas了。 读取数据pandas提供read_csv方法来直接独处数据保存为DateFrame格式。 1df = pd.read_csv('comment.csv', sep=';', header=None) 数据处理 设置数据列名 由于我们知道数据有两列，先通过这只列名可以方便后续引用。 1df.columns = ['date', 'comment'] 时间日期处理 在date列，我们保存的数据格式是string，需要把转换为日期格式才能进一步处理。 1df['date'] = pd.to_datetime(df['date']) 我们需要按时间来统计，所以把date列设置为index: 1df = df.set_index('date') 统计处理 日期筛选 由于我们知道《流浪地球》是2月5日上映的，我们可以对日期进行限定，以免出现有些在上映前的评论，会占用大段的空白情况。 设置index之后，可以参考list类型操作，由于时间是倒序的，所以可以直接使用[:&#39;2019-02-04&#39;]来选取2月4日之后到今天的所有数据。pandas在数据筛选方面相当智能，按照datetime的格式直接筛选即可。 1cacu_df = df[:'2019-02-04'] 按日期进行数量统计 pandas中，通过resample方法进行重新采样，通过传入rule参数就可以按需要的频率获取数据，获得一个resampler对象。 1DataFrame.resample(rule, how=None, axis=0, fill_method=None, closed=None, label=None, convention='start', kind=None, loffset=None, limit=None, base=0, on=None, level=None) resampler对象提供了很多的统计方法，比如汇总求和可使用Resampler.count()。 12# 按日统计数量cacu = cacu_df.resample('D').count() 这样就完成了按日期求和统计操作。 绘图画图需要使用matplotlib库，通过导入该库，可直接对DateFrame对象进行画图处理。画图及图表格式化如下： 123456789101112131415161718# 设置中文字体font = FontProperties(fname='/System/Library/Fonts/PingFang.ttc')plt.plot(cacu)plt.title("流浪地球评论分析", fontproperties=font)plt.xlabel("日期", fontproperties=font)plt.ylabel("评论数", fontproperties=font)plt.axis("tight")# 显示网格plt.grid(True)# 自动旋转横轴日期plt.gcf().autofmt_xdate()# 显示数值for a, b in zip(cacu.index, cacu.values):plt.text(a, b, str(b[0]))# 保存图片plt.savefig('comment_analysis.png')# 查看图片plt.show() 结论以上就是使用抓取的评论生成日期统计图片的大致思路，完成的实现代码请见：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/maoyan 结果如下： 可见从上映之后，关注度直线飙升，到2月10日之后（上映5天），大家关注度逐渐下降。其中2月14日为情人节，大家的关注又有了小幅的上升。也许很多人在这天通过看《流浪地球》过节吧。 拓展 matplotlib画图说明 该库是默认不支持中文的，所以如果不经过配置直接画图，设置中文图标会显示为空白方框。详细配置参考上文。 pandas使用 更多的resampler的方法，可见：http://pandas.pydata.org/pandas-docs/stable/reference/resampling.html 更多resample的rule参考下表： Date Offset Frequency String Description DateOffset None Generic offset class, defaults to 1 calendar day BDay or BusinessDay &#39;B&#39; business day (weekday) CDay or CustomBusinessDay &#39;C&#39; custom business day Week &#39;W&#39; one week, optionally anchored on a day of the week WeekOfMonth &#39;WOM&#39; the x-th day of the y-th week of each month LastWeekOfMonth &#39;LWOM&#39; the x-th day of the last week of each month MonthEnd &#39;M&#39; calendar month end MonthBegin &#39;MS&#39; calendar month begin BMonthEnd or BusinessMonthEnd &#39;BM&#39; business month end BMonthBegin or BusinessMonthBegin &#39;BMS&#39; business month begin CBMonthEnd or CustomBusinessMonthEnd &#39;CBM&#39; custom business month end CBMonthBegin or CustomBusinessMonthBegin &#39;CBMS&#39; custom business month begin SemiMonthEnd &#39;SM&#39; 15th (or other day_of_month) and calendar month end SemiMonthBegin &#39;SMS&#39; 15th (or other day_of_month) and calendar month begin QuarterEnd &#39;Q&#39; calendar quarter end QuarterBegin &#39;QS&#39; calendar quarter begin BQuarterEnd &#39;BQ business quarter end BQuarterBegin &#39;BQS&#39; business quarter begin FY5253Quarter &#39;REQ&#39; retail (aka 52-53 week) quarter YearEnd &#39;A&#39; calendar year end YearBegin &#39;AS&#39; or &#39;BYS&#39; calendar year begin BYearEnd &#39;BA&#39; business year end BYearBegin &#39;BAS&#39; business year begin FY5253 &#39;RE&#39; retail (aka 52-53 week) year Easter None Easter holiday BusinessHour &#39;BH&#39; business hour CustomBusinessHour &#39;CBH&#39; custom business hour Day &#39;D&#39; one absolute day Hour &#39;H&#39; one hour Minute &#39;T&#39; or &#39;min&#39; one minute Second &#39;S&#39; one second Milli &#39;L&#39; or &#39;ms&#39; one millisecond Micro &#39;U&#39; or &#39;us&#39; one microsecond Nano &#39;N&#39; one nanosecond]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac上如何快速获得文件路径]]></title>
    <url>%2FMac%E4%B8%8A%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E8%8E%B7%E5%BE%97%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84.html</url>
    <content type="text"><![CDATA[背景在使用开发中，免不了要涉及文件的读写。在Mac环境下如何快速的获得某文件的路径呢？ 思路方法一：pwd如果是习惯于使用命令行的用户，那么直接在命令行通过cd来找到对应的文件，再使用pwd即可： 12$ pwd/Users/zhengk/PycharmProjects/Mine/maoyan 该方法简单，直观，但需要对文件所在位置有较明确的认识。 方法二：Finder使用Mac的Finder（达坊）功能： 先打开Finder，在“显示”中勾选“显示路径栏”； 这时候在Finder窗口最下端会有显示文件的路径，选中需要的文件，鼠标右键“路径栏”中的文件名，就可以看到“拷贝为路径名称”的选项； 拷贝即可，结果如下： 1/Users/zhengk/PycharmProjects/Mine/maoyan/jupiter.png 方法三：结合Finder与终端 先在Finder中找到需要确认路径的文件 打开一个空白的终端窗口 将文件拖到终端窗口中，终端中就会出现该文件的路径地址。 1$ /Users/zhengk/PycharmProjects/Mine/maoyan/jupiter.png 结论以上三种方法都可以在Mac中获取对应文件的路径。 方法一针对深度终端用户会比较好，如果明确知道文件的位置也会很快。 方法二和方法三通过可视化界面获取，比较直观。]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用wordcloud绘制词云]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8wordcloud%E7%BB%98%E5%88%B6%E8%AF%8D%E4%BA%91.html</url>
    <content type="text"><![CDATA[背景前面通过爬虫获取了猫眼上”流浪地球“的评论：获取猫眼电影的评论 那么就做一个酷炫的词云吧，直观的看看大家都在想什么，先上效果图： 思路数据清洗首先由于评论是用户发表的，可能什么字符都会有，要先把一些特殊符号去掉，这里就用到了正则替换： 1msg = re.sub("[\s+\.\!\/_,$%^*()+\"\'\?]+|[+——！，。？、~@#￥%……&amp;*（）【】；：]+|\[.+\]|\［.+\］", "", line) 分词与标签清洗后的数据，可以使用jieba分词包来进行分词，并把所有的分词保存在一个list中，然后计算出每个分词出现的次数。 12345678910# 分词tags = jieba.analyse.extract_tags(msg)for t in tags: word_list.append(t)# 计算词频for word in word_list: if word not in word_dict: word_dict[word] = 1 else: word_dict[word] += 1 生成词云使用wordcloud包，就可以很方便的生成词云图片了。 先新建一个WordCloud对象，进行配置，然后利用前面的分词词频就可以生成对应的图片了。 12345678# 计算图片颜色alice_coloring = np.array(img)my_wordcloud = WordCloud(background_color="white", max_words=500, mask=alice_coloring, max_font_size=200, random_state=42, font_path=(os.path.join(d, "font/msyh.ttf"))) my_wordcloud = my_wordcloud.generate_from_frequencies(wordList) 这里需要注意的是： mask=alice_coloring：这里通过numpy将图片矩阵化，来获取图片的颜色作为WordCloud的mask，是为了最后生成的图云不仅外形与我们输入的图片保持一致，而且整体颜色也保持一致。 输入的原图，背景色需要设置为白色而不是透明色，否则会全屏幕都是字。。。 对于中文的词云，需要制定中文的字体，这里用的是微软雅黑 保存图片最后使用matplotlib.pyplot来保存图片，保存前要进行图片属性的一些设置。 12345678910width = img.width/80height = img.height/80plt.figure(figsize=(width, height))plt.imshow(my_wordcloud.recolor(color_func=image_colors))plt.imshow(my_wordcloud)plt.axis("off")# 通过设置subplots_adjust来控制画面外边框plt.subplots_adjust(bottom=.01, top=.99, left=.01, right=.99)plt.savefig("jupiter_wordcloud_1.png")plt.show() 这里需要注意的是： 建议根据原图片的长宽比例进行一定的缩小，以免生成的图片像素过大而产生报错。 1ValueError: Image size of 98400x46500 pixels is too large. It must be less than 2^16 in each direction. 结论以上就是使用抓取的评论生成词云的大致思路，完成的实现代码请见：https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/maoyan 最后放一张原图，你能看的出来嘛，抠图技术有限O(∩_∩)O哈哈~]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用WMIC获取系统硬件信息]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8WMIC%E8%8E%B7%E5%8F%96%E7%B3%BB%E7%BB%9F%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF.html</url>
    <content type="text"><![CDATA[WMIC是扩展WMI（Windows Management Instrumentation，Windows管理工具），提供了从命令行接口和批命令脚本执行系统管理的支持。 在日常工作中，我们可以通过WMIC来获取计算机的硬件信息用于一些统计和分析。 使用WMIC直接在cmd中输入wmic即可进入交互式命令行： 12C:\Users&gt;wmicwmic:root\cli&gt; 获取硬盘信息硬盘信息可以通过输入diskdrive来获取相关信息 12345wmic:root\cli&gt;diskdriveAvailability BytesPerSector Capabilities CapabilityDescriptions Caption CompressionMethod ConfigManagerErrorCode ConfigManagerUserConfig CreationClassName DefaultBlockSize Description DeviceID ErrorCleared ErrorDescription ErrorMethodology FirmwareRevision Index InstallDate InterfaceType LastErrorCode Manufacturer MaxBlockSize MaxMediaSize MediaLoaded MediaType MinBlockSize Model Name NeedsCleaning NumberOfMediaSupported Partitions PNPDeviceID PowerManagementCapabilities PowerManagementSupported SCSIBus SCSILogicalUnit SCSIPort SCSITargetId SectorsPerTrack SerialNumber Signature Size Status StatusInfo SystemCreationClassName SystemName TotalCylinders TotalHeads TotalSectors TotalTracks TracksPerCylinder 512 &#123;3, 4, 7&#125; &#123;"Random Access", "Supports Writing", "Supports Removable Media"&#125; SanDisk Ultra Fit USB Device 0 FALSE Win32_DiskDrive 磁盘驱动器 \\.\PHYSICALDRIVE2 1.00 2 USB (标准磁盘驱动器) TRUE Removable Media SanDisk Ultra Fit USB Device \\.\PHYSICALDRIVE2 1 USBSTOR\DISK&amp;VEN_SANDISK&amp;PROD_ULTRA_FIT&amp;REV_1.00\4C530001140919121090&amp;0 0 0 0 0 63 4C530001140919121090 4047072857 250443325440 OK Win32_ComputerSystem KFZXZHENGKQ 30448 255 489147120 7764240 255 512 &#123;3, 4, 10&#125; &#123;"Random Access", "Supports Writing", "SMART Notification"&#125; PLEXTOR PX-128M6G-2242 0 FALSE Win32_DiskDrive 磁盘驱动器 \\.\PHYSICALDRIVE1 1.01 1 IDE (标准磁盘驱动器) TRUE Fixed hard disk media PLEXTOR PX-128M6G-2242 \\.\PHYSICALDRIVE1 2 SCSI\DISK&amp;VEN_PLEXTOR&amp;PROD_PX-128M6G-2242\4&amp;28BCE847&amp;0&amp;010000 1 0 0 0 63 P02512105359 1914799601 128034708480 OK Win32_ComputerSystem KFZXZHENGKQ 15566 255 250067790 3969330 255 512 &#123;3, 4, 10&#125; &#123;"Random Access", "Supports Writing", "SMART Notification"&#125; LITEON LCH-256V2S 0 FALSE Win32_DiskDrive 磁盘驱动器 \\.\PHYSICALDRIVE0 3C87901 0 IDE (标准磁盘驱动器) TRUE Fixed hard disk media LITEON LCH-256V2S \\.\PHYSICALDRIVE0 2 SCSI\DISK&amp;VEN_LITEON&amp;PROD_LCH-256V2S\4&amp;28BCE847&amp;0&amp;000000 0 0 0 0 63 SD0F66157L1TH57102RR 2070240311 256052966400 OK Win32_ComputerSystem KFZXZHENGKQ 31130 255 500103450 7938150 255 信息有点多，使用\?来看下有什么选项： 1234567891011121314wmic:root\cli&gt;diskdrive /?DISKDRIVE - 物理磁盘驱动器管理。提示: BNF 的别名用法。(&lt;别名&gt; [WMI 对象] | &lt;别名&gt; [&lt;路径 where&gt;] | [&lt;别名&gt;] &lt;路径 where&gt;) [&lt;谓词子句&gt;]。用法:DISKDRIVE ASSOC [&lt;格式说明符&gt;]DISKDRIVE CREATE &lt;分配列表&gt;DISKDRIVE DELETEDISKDRIVE GET [&lt;属性列表&gt;] [&lt;获取开关&gt;]DISKDRIVE LIST [&lt;列表格式&gt;] [&lt;列表开关&gt;] 根据帮助，我们可以使用get来获取需要的信息，比如获取容量信息： 12345wmic:root\cli&gt;diskdrive get SizeSize250443325440128034708480256052966400 获取内存信息内存信息可以使用memorychip来获取，我们试下不进入CLI交互模式（在命令前加上wmic即可）： 1234C:\Users&gt;wmic memorychipAttributes BankLabel Capacity Caption ConfiguredClockSpeed ConfiguredVoltage CreationClassName DataWidth Description DeviceLocator FormFactor HotSwappable InstallDate InterleaveDataDepth InterleavePosition Manufacturer MaxVoltage MemoryType MinVoltage Model Name OtherIdentifyingInfo PartNumber PositionInRow PoweredOn Removable Replaceable SerialNumber SKU SMBIOSMemoryType Speed Status Tag TotalWidth TypeDetail Version0 BANK 0 4294967296 物理内存 1600 Win32_PhysicalMemory 64 物理内存 ChannelA-DIMM0 12 Samsung 24 物理内存 K4B8G1646B-MYK0 78111110 24 1600 Physical Memory 0 64 128 0 BANK 2 4294967296 物理内存 1600 Win32_PhysicalMemory 64 物理内存 ChannelB-DIMM0 12 Samsung 24 物理内存 M471B5173QH0-YK0 18128315 24 1600 Physical Memory 1 64 128 获取内存的容量信息： 1234C:\Users&gt;wmic memorychip get CapacityCapacity42949672964294967296 WMIC其他功能通过\?可以发现WMIC的其他所有功能，有需要的话可以关注下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118wmic:root\cli&gt;/?[全局开关] &lt;命令&gt;可以使用以下全局开关:/NAMESPACE 别名在其上操作的命名空间的路径。/ROLE 包含别名定义的角色的路径。/NODE 别名在其上操作的服务器。/IMPLEVEL 客户端模拟级别。/AUTHLEVEL 客户端身份验证级别。/LOCALE 客户端应使用的语言 ID。/PRIVILEGES 启用或禁用所有权限。/TRACE 将调试信息输出到 stderr。/RECORD 记录所有输入命令和输出内容。/INTERACTIVE 设置或重置交互模式。/FAILFAST 设置或重置 FailFast 模式。/USER 会话期间要使用的用户。/PASSWORD 登录会话时要使用的密码。/OUTPUT 指定输出重定向模式。/APPEND 指定输出重定向模式。/AGGREGATE 设置或重置聚合模式。/AUTHORITY 指定连接的 &lt;授权类型&gt;。/?[:&lt;BRIEF|FULL&gt;] 用法信息。有关特定全局开关的详细信息，请键入: switch-name /?当前角色中可以使用以下别名:ALIAS - 对本地系统上可用别名的访问BASEBOARD - 基板(也称为主板或系统板)管理。BIOS - 基本输入/输出服务(BIOS)管理。BOOTCONFIG - 启动配置管理。CDROM - CD-ROM 管理。COMPUTERSYSTEM - 计算机系统管理。CPU - CPU 管理。CSPRODUCT - SMBIOS 中的计算机系统产品信息。DATAFILE - 数据文件管理。DCOMAPP - DCOM 应用程序管理。DESKTOP - 用户的桌面管理。DESKTOPMONITOR - 桌面监视器管理。DEVICEMEMORYADDRESS - 设备内存地址管理。DISKDRIVE - 物理磁盘驱动器管理。DISKQUOTA - 用于 NTFS 卷的磁盘空间使用量。DMACHANNEL - 直接内存访问(DMA)通道管理。ENVIRONMENT - 系统环境设置管理。FSDIR - 文件系统目录项管理。GROUP - 组帐户管理。IDECONTROLLER - IDE 控制器管理。IRQ - 中断请求线路(IRQ)管理。JOB - 提供对使用计划服务安排的作业的访问。LOADORDER - 定义执行依赖关系的系统服务的管理。LOGICALDISK - 本地存储设备管理。LOGON - 登录会话。MEMCACHE - 缓存内存管理。MEMORYCHIP - 内存芯片信息。MEMPHYSICAL - 计算机系统的物理内存管理。NETCLIENT - 网络客户端管理。NETLOGIN - 网络登录信息(属于特定用户)管理。NETPROTOCOL - 协议(及其网络特征)管理。NETUSE - 活动网络连接管理。NIC - 网络接口控制器(NIC)管理。NICCONFIG - 网络适配器管理。NTDOMAIN - NT 域管理。NTEVENT - NT 事件日志中的项目。NTEVENTLOG - NT 事件日志文件管理。ONBOARDDEVICE - 主板(系统板)中内置的通用适配器设备的管理。OS - 已安装操作系统的管理。PAGEFILE - 虚拟内存文件交换管理。PAGEFILESET - 页面文件设置管理。PARTITION - 物理磁盘的已分区区域的管理。PORT - I/O 端口管理。PORTCONNECTOR - 物理连接端口管理。PRINTER - 打印机设备管理。PRINTERCONFIG - 打印机设备配置管理。PRINTJOB - 打印作业管理。PROCESS - 进程管理。PRODUCT - 安装程序包任务管理。QFE - 快速修复工程。QUOTASETTING - 卷上的磁盘配额设置信息。RDACCOUNT - 远程桌面连接权限管理。RDNIC - 对特定网络适配器的远程桌面连接管理。RDPERMISSIONS - 特定远程桌面连接的权限。RDTOGGLE - 远程打开或关闭远程桌面侦听程序。RECOVEROS - 操作系统出现故障时将从内存收集的信息。REGISTRY - 计算机系统注册表管理。SCSICONTROLLER - SCSI 控制器管理。SERVER - 服务器信息管理。SERVICE - 服务应用程序管理。SHADOWCOPY - 卷影副本管理。SHADOWSTORAGE - 卷影副本存储区域管理。SHARE - 共享资源管理。SOFTWAREELEMENT - 系统上安装的软件产品元素的管理。SOFTWAREFEATURE - SoftwareElement 的软件产品子集的管理。SOUNDDEV - 声音设备管理。STARTUP - 当用户登录到计算机系统时自动运行的命令的管理。SYSACCOUNT - 系统帐户管理。SYSDRIVER - 基本服务的系统驱动程序管理。SYSTEMENCLOSURE - 物理系统外壳管理。SYSTEMSLOT - 物理连接点(包括端口、插槽和外设以及专用连接点)的管理。TAPEDRIVE - 磁带驱动器管理。TEMPERATURE - 温度传感器(电子温度计)数据管理。TIMEZONE - 时区数据管理。UPS - 不间断电源(UPS)管理。USERACCOUNT - 用户帐户管理。VOLTAGE - 电压传感器(电子电压表)数据管理。VOLUME - 本地存储卷管理。VOLUMEQUOTASETTING - 将磁盘配额设置与特定磁盘卷相关联。VOLUMEUSERQUOTA - 每用户存储卷配额管理。WMISET - WMI 服务操作参数管理。有关特定别名的详细信息，请键入: alias /?CLASS - 按 Esc 键可获取完整 WMI 架构。PATH - 按 Esc 键可获取完整 WMI 对象路径。CONTEXT - 显示所有全局开关的状态。QUIT/EXIT - 退出程序。有关 CLASS/PATH/CONTEXT 的详细信息，请键入: (CLASS | PATH | CONTEXT) /?]]></content>
      <categories>
        <category>操作系统</category>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac上如何快速截图]]></title>
    <url>%2FMac%E4%B8%8A%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E6%88%AA%E5%9B%BE.html</url>
    <content type="text"><![CDATA[在MacOS的系统自带截图工具在：所有程序 - 其他 - 屏幕快照 中。 打开后就可以选择 捕捉整个屏幕、捕捉部分、录屏等功能。 那么在实际使用中，这样截图太麻烦了，怎么快速截图呢。 方法一： 用系统自带的截图工具快捷键： shift + command + 3:全屏截图 shift + command + 4：部分截图 系统自带截图都是默认保存在桌面上。 方法二： 如果你有安装QQ的话，那么QQ也是有截图功能的，默认的快捷键如下： control + command + a：部分截图 QQ的截图默认是保存在剪切板的，不会再屏幕上显示。 PS：不用登陆QQ也可以使用QQ的截图功能，在聊天或者编辑文本的时候使用非常方便。]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取猫眼电影的评论]]></title>
    <url>%2F%E8%8E%B7%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E7%9A%84%E8%AF%84%E8%AE%BA.html</url>
    <content type="text"><![CDATA[背景最近几年猫眼电影越来越热门了，都差不多和豆瓣并驾齐驱了。今年的《流浪地球》这么火，看看猫眼电影上网友对该片的评价如何。 思路找到评论网页地址先打开猫眼官网找到《流浪地球》的介绍页面：https://maoyan.com/films/248906 虽然显示有112.4万人评分，但是页面只有热门短评，其他评论都去哪里了，手机明明是有的。 那么我们用chrome切换到手机页面： 打开开发者工具 开启手机浏览功能 访问手机版地址：http://m.maoyan.com/movie/248906?_v_=yes&amp;channelId=4&amp;$from=canary# 这时候我们就看到了所有的评论。 获取评论请求地址在点击打开“查看全部330613条讨论”后，发现评论分为最热和最新两部分，最热数量有限，而最新则是未经过处理的，也正是我们需要的。通过search来查看下对应的请求： 发现，在chrome 的网络展示中发现只有一个类型为document的请求包含了所需的信息。那么这部分的评论获取就需要解析网页了，我们再把屏幕上的评论往下拉，发现会自动加载更多的评论，对应的chrome网络请求多出来了两个comments.json的请求： 果然这才是我们需要的！把初始页面的url和这两个json请求的url复制到一起比较一下： 123http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=0&amp;limit=15&amp;ts=0&amp;type=3http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=15&amp;limit=15&amp;ts=1549965527295&amp;type=3http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=30&amp;limit=15&amp;ts=1549965527295&amp;type=3 我们可以发现规律： 初始页面的ts值为0，随后会有ts值，且保持不变。这里的ts是当前的时间戳，可以通过转换工具查看： offset是请求评论开始的序号，limit为请求的条数 再看返回的json结果： data.comments中是评论的具体内容 paging中通过hasMore来告诉我们是否还有更多（判断是否继续抓取） 我们再尝试下将offset设置为0，也加上ts参数： 1http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=0&amp;limit=15&amp;ts=1549965527295&amp;type=3 发现也是可以获取数据的： 那么通过offset和limit来控制每次请求获取的数量。 我们还可以通过加大limit参数来尝试，是否可以一次性获取更多的评论: 1http://m.maoyan.com/review/v2/comments.json?movieId=248906&amp;userId=-1&amp;offset=0&amp;limit=30&amp;ts=1549965527295&amp;type=3 效果如下: 再增加limit的值，会发现评论数回到了15条，可见猫眼系统仅支持每次最多获取30条。 构造请求url 方法一根据上面的分析，我们构造请求的url就很明确了： 从offset=0&amp;limit=30开始 通过返回的paging.hasMore来判断是否继续抓取 下一个抓取的url中offset+=limit 只能抓取1000条？！根据上述分析，在返回的json数据中是可以看到总评论数的，但是实际抓取的时候，在offset超过1000之后，返回的数据中hasMore就变成了false。 于是尝试通过浏览器一直下拉刷新，到达offset超过1000的情况，发现页面会不停的发送请求，但也无法获取数据。 那应该就是网站做了控制，不允许offset超过1000。 构造请求URL 方法二那么就要考虑其他构造url的方法来抓取了。先观察下每个请求返回的信息： 发现每个comment里都包含有一个time信息，把time做一下处理： 123456789102019-02-13 13:38:00##感觉韩朵朵这个人设是多余的2019-02-13 13:38:00##真的感动 非常棒2019-02-13 13:38:00##这电影大陆的起航2019-02-13 13:38:00##不怎么样，剧情挺感人，但是有点尴尬2019-02-13 13:37:00##好看。。。。。。。。。。2019-02-13 13:37:00##超级超级超级超级超级超级超级好看2019-02-13 13:37:00##太牛逼了，中国科幻片可有一部能看的了。支持吴京2019-02-13 13:36:00##不错！中国科幻的希望2019-02-13 13:36:00##中国里程碑式的科幻电影。2019-02-13 13:36:00##什么垃圾座位没人管的么乱坐的 可以发现后台是按照时间顺序的，每分钟一个间隔，那么就可以考虑根据每次返回comment中的时间来更新url中的ts即可。 由于不确定每次请求返回的数据中包含了多长的时间段，且返回的第一个评论时间戳与第二个评论是不同的，所以抓取思路如下： 获取请求数据 记录第一个时间戳 记录第二个时间戳 当遇到第三个时间戳时，将ts设置为第二个时间戳，重新构造url 如果单次抓取中每遇到第三个时间戳，则通过修改offset来继续抓取，直到遇到第三个时间戳 实现根据上面思路，实现相对就比较简单了： 生成url 12345def get_url(): global offset url = 'http://m.maoyan.com/review/v2/comments.json?movieId=' + movieId + '&amp;userId=-1&amp;offset=' + str( offset) + '&amp;limit=' + str(limit) + '&amp;ts=' + str(ts) + '&amp;type=3' return url 访问url 123456789101112def open_url(url): global ua try: headers = &#123;'User-Agent': ua.random&#125; response = requests.get(url, headers=headers) if response.status_code == 200: return response.text else: return None except Exception as e: print(e) return None 数据处理：将评论保存并判断是否要继续抓取 1234567891011121314151617181920212223242526272829def parse_json(data): global count global offset global limit global ts ts_duration = ts res = json.loads(data) comments = res['data']['comments'] for comment in comments: comment_time = comment['time'] if ts == 0: ts = comment_time ts_duration = comment_time if comment_time != ts and ts == ts_duration: ts_duration = comment_time if comment_time !=ts_duration: ts = ts_duration offset = 0 return get_url() else: content = comment['content'].strip().replace('\n', '。') print('get comment ' + str(count)) count += 1 write_txt(time.strftime("%Y-%m-%d %H:%M:%S",time.localtime(comment_time/1000)) + '##' + content + '\n') if res['paging']['hasMore']: offset += limit return get_url() else: return None 最后一共抓取评论131106条，足够做各种分析了，完整代码可见GitHub:https://github.com/keejo125/web_scraping_and_data_analysis/tree/master/maoyan 123456789102019-02-13 18:13:10,962 - get_comments.py[line:78] - INFO: get comment 1310982019-02-13 18:13:11,079 - get_comments.py[line:78] - INFO: get comment 1310992019-02-13 18:13:11,189 - get_comments.py[line:78] - INFO: get comment 1311002019-02-13 18:13:11,307 - get_comments.py[line:78] - INFO: get comment 1311012019-02-13 18:13:11,411 - get_comments.py[line:78] - INFO: get comment 1311022019-02-13 18:13:11,527 - get_comments.py[line:78] - INFO: get comment 1311032019-02-13 18:13:11,625 - get_comments.py[line:78] - INFO: get comment 1311042019-02-13 18:13:11,729 - get_comments.py[line:78] - INFO: get comment 1311052019-02-13 18:13:11,827 - get_comments.py[line:78] - INFO: get comment 1311062019-02-13 18:13:15,416 - get_comments.py[line:98] - INFO: end 如果有更好的方法，欢迎一起探讨。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask在Windows环境下的部署]]></title>
    <url>%2FFlask%E5%9C%A8Windows%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E9%83%A8%E7%BD%B2.html</url>
    <content type="text"><![CDATA[背景由于目前在用的Flask项目涉及到一部分依赖Windows的处理，还无法迁移到linux平台，那么在windows环境下，要怎么部署呢？ 思路根据Flask官网介绍，由于Flask内置的服务器性能不佳，推荐的主要的部署方式有如下几种： mod_wsgi (Apache) 独立 WSGI 容器 Gunicorn Tornado Gevent uWSGI FastCGI CGI 上述这些部署方式，仅Tornado是支持在windows情况下部署的，配合上Nginx可以达到比较好的效果。可已参考Nginx与tornado框架的并发评测。 但是在实际使用中发现，tornado 的稳定性虽然很高，但是在tornado上部署Flask，并不会有异步的效果。实际上还是单进程阻塞运行的，即使在Flask中配置了threaded = True也无法实现多线程使用。 Flask多线程情况配置启用多线程： 1234# manage.pyfrom flask_script import Serverserver = Server(host="0.0.0.0", threaded=True) 在Flask中配置两条测试路由 123456789import time@main.route('/test')def maintest(): return 'hello world' @main.route('/sleep')def mainsleep(): time.sleep(60) return 'wake up' 先用浏览器访问\sleep： 随即立刻访问\test: 可见两次访问是不同的线程处理的，不会出现堵塞的情况。 tornado + Flask多线程情况使用tornado托管： 12345678from tornado.wsgi import WSGIContainerfrom tornado.httpserver import HTTPServerfrom tornado.ioloop import IOLoopfrom yourapplication import apphttp_server = HTTPServer(WSGIContainer(app))http_server.listen(5000)IOLoop.instance().start() 先用浏览器访问\sleep： 随即立刻访问\test: 可以发现，虽然tornado框架是支持异步的，但是由于实际上后台的处理是同步的，从而无法实现异步的处理的效果。如果想后台的处理也异步，则需要直接使用tornado来开发。 那么为什么使用tornado来托管flask呢？ Tornado 是一个开源的可伸缩的、非阻塞式的 web 服务器和工具集，它驱动了FriendFeed 。因为它使用了 epoll 模型且是非阻塞的，它可以处理数以千计的并发固定连接，这意味着它对实时 web 服务是理想的。把 Flask 集成这个服务是直截了当的 根据官网描述，其实也是为了弥足flask自带服务器不稳定的问题。 Flask高并发下的表现使用tsung进行压测，压力500： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 34.30 msec 31.91 msec 506 / sec 356.60 / sec 33.19 msec 103908 page 0.42 sec 0.29 sec 505 / sec 356.32 / sec 0.39 sec 103782 request 0.42 sec 0.29 sec 505 / sec 356.32 / sec 0.39 sec 103782 session 1mn 24sec 10.64 sec 11.4 / sec 1.21 / sec 14.24 sec 362 Code Highest Rate Mean Rate Total number 200 505 / sec 356.32 / sec 104792 Name Highest Rate Total number error_abort 0.5 / sec 1 error_abort_max_conn_retries 11.7 / sec 362 error_connect_econnrefused 58.6 / sec 1667 可见，在500的并发下，效果不佳，有很多的链接拒绝。 Flask + Nginx在高并发下的表现 使用tsung进行压测，压力500： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 30.95 msec 1810.5 / sec 626.43 / sec 0.11 sec 189853 page 0.68 sec 0.17 sec 1810.1 / sec 625.72 / sec 0.40 sec 189581 request 0.68 sec 0.17 sec 1810.1 / sec 625.72 / sec 0.40 sec 189581 Code Highest Rate Mean Rate Total number 200 906.4 / sec 196.08 / sec 60689 502 1443.9 / sec 430.02 / sec 129006 Name Highest Rate Total number error_abort 0.5 / sec 1 情况差不多，Flask服务器表现还算稳定，那么尝试增加后台Flask服务器数量（通过多端口实现）： 1234python manage.py runserver --port=8001python manage.py runserver --port=8002python manage.py runserver --port=8003python manage.py runserver --port=8004 使用tsung进行压测，压力500，4个Flask服务器： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.18 sec 32.57 msec 3510.1 / sec 639.92 / sec 0.11 sec 195154 page 0.49 sec 85.30 msec 3512.1 / sec 639.07 / sec 0.35 sec 194856 request 0.49 sec 85.30 msec 3512.1 / sec 639.07 / sec 0.35 sec 194856 Code Highest Rate Mean Rate Total number 200 3510.1 / sec 639.50 / sec 194986 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 这个效果妥妥的。 使用tsung进行压测，压力1000，4个Flask服务器： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 32.63 msec 2983.8 / sec 492.94 / sec 98.56 msec 150793 page 0.57 sec 90.00 msec 2976.4 / sec 491.31 / sec 0.40 sec 150275 request 0.57 sec 90.00 msec 2976.4 / sec 491.31 / sec 0.40 sec 150275 Code Highest Rate Mean Rate Total number 200 2981.4 / sec 488.92 / sec 149556 502 92.5 / sec 4.02 / sec 925 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 开始有一些502的超时错误了。 使用tsung进行压测，压力1000，4个tornado服务器： Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.18 sec 86.24 msec 2052.1 / sec 693.82 / sec 0.14 sec 208786 page 0.52 sec 0.24 sec 2060.7 / sec 693.34 / sec 0.45 sec 208606 request 0.52 sec 0.24 sec 2060.7 / sec 693.34 / sec 0.45 sec 208606 Code Highest Rate Mean Rate Total number 200 2056.6 / sec 693.67 / sec 208703 在并发1000的情况下，是否使用tornado托管Flask效果差不多。 结论根据上述测试，直接使用Flask服务器的话，由于并发处理较弱，会有各种超时或者连接拒绝的错误。通过搭配Nginx来进行缓冲，通过增加后端服务器数来提供并发处理量。 所以最终选择了Nginx+后台4个Flask服务器的方式。由于目前Flask项目全体用户只有几千，目前并发情况很低，该方式完全满足使用。 如果在更大型项目中，并发上万，建议还是考虑想办法迁移至Liunx环境，通过官方建议的方式部署。]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>nginx</tag>
        <tag>tornado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下安装富士施乐打印机]]></title>
    <url>%2FMac%E4%B8%8B%E5%AE%89%E8%A3%85%E5%AF%8C%E5%A3%AB%E6%96%BD%E4%B9%90%E6%89%93%E5%8D%B0%E6%9C%BA.html</url>
    <content type="text"><![CDATA[背景打印机在windows环境下安装还是很方便的，在mac下，一路默认安装会有点问题，记录一下。 安装 下载打印机驱动：http://onlinesupport.fujixerox.com/setupSupport.do?cid=3&amp;ctry_code=CN&amp;lang_code=zh_CN 安装打印机驱动，一路默认就好了 打开Mac的 系统偏好设置，点击打印机与扫描仪 输入打印机的IP地址，然后点击添加，注意“协议” 要修改为“HP Jetdirect-Socket”。 点击添加即可 注意Mac在安装打印机时，默认的协议”互联网打印协议-IPP”，如下图所示。 按照默认的协议，也会自动识别到打印机型号，但是点击添加之后，会有报错： 如果继续添加，虽然可以添加成功，但实际打印时就会打印机无响应。]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下使用tree命令展示文件树]]></title>
    <url>%2FMac%E4%B8%8B%E4%BD%BF%E7%94%A8tree%E5%91%BD%E4%BB%A4%E5%B1%95%E7%A4%BA%E6%96%87%E4%BB%B6%E6%A0%91.html</url>
    <content type="text"><![CDATA[背景在写代码文档的时候，经常会用到展示项目架构，这时候如果可以有命令直接打印出目录树那就再好不过了，免的截图了。 思路网上找了下，果然是有这种工具的，Mac - tree命令。 Mac默认是没有tree命令的，需要手工安装下： 1brew install tree 安装好之后，看下帮助文档： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364$ tree --helpusage: tree [-acdfghilnpqrstuvxACDFJQNSUX] [-H baseHREF] [-T title ] [-L level [-R]] [-P pattern] [-I pattern] [-o filename] [--version] [--help] [--inodes] [--device] [--noreport] [--nolinks] [--dirsfirst] [--charset charset] [--filelimit[=]#] [--si] [--timefmt[=]&lt;f&gt;] [--sort[=]&lt;name&gt;] [--matchdirs] [--ignore-case] [--fromfile] [--] [&lt;directory list&gt;] ------- Listing options ------- -a All files are listed. -d List directories only. -l Follow symbolic links like directories. -f Print the full path prefix for each file. -x Stay on current filesystem only. -L level Descend only level directories deep. -R Rerun tree when max dir level reached. -P pattern List only those files that match the pattern given. -I pattern Do not list files that match the given pattern. --ignore-case Ignore case when pattern matching. --matchdirs Include directory names in -P pattern matching. --noreport Turn off file/directory count at end of tree listing. --charset X Use charset X for terminal/HTML and indentation line output. --filelimit # Do not descend dirs with more than # files in them. --timefmt &lt;f&gt; Print and format time according to the format &lt;f&gt;. -o filename Output to file instead of stdout. ------- File options ------- -q Print non-printable characters as '?'. -N Print non-printable characters as is. -Q Quote filenames with double quotes. -p Print the protections for each file. -u Displays file owner or UID number. -g Displays file group owner or GID number. -s Print the size in bytes of each file. -h Print the size in a more human readable way. --si Like -h, but use in SI units (powers of 1000). -D Print the date of last modification or (-c) status change. -F Appends '/', '=', '*', '@', '|' or '&gt;' as per ls -F. --inodes Print inode number of each file. --device Print device ID number to which each file belongs. ------- Sorting options ------- -v Sort files alphanumerically by version. -t Sort files by last modification time. -c Sort files by last status change time. -U Leave files unsorted. -r Reverse the order of the sort. --dirsfirst List directories before files (-U disables). --sort X Select sort: name,version,size,mtime,ctime. ------- Graphics options ------- -i Don't print indentation lines. -A Print ANSI lines graphic indentation lines. -S Print with CP437 (console) graphics indentation lines. -n Turn colorization off always (-C overrides). -C Turn colorization on always. ------- XML/HTML/JSON options ------- -X Prints out an XML representation of the tree. -J Prints out an JSON representation of the tree. -H baseHREF Prints out HTML format with baseHREF as top directory. -T string Replace the default HTML title and H1 header with string. --nolinks Turn off hyperlinks in HTML output. ------- Input options ------- --fromfile Reads paths from files (.=stdin) ------- Miscellaneous options ------- --version Print version and exit. --help Print usage and this help message and exit. -- Options processing terminator. 可以添加的参数很多，那么该用那些呢？ 在一个python项目中，先只加文件夹名看下： 1234567891011121314151617181920212223$ tree appapp├── __init__.py├── __pycache__│ └── __init__.cpython-37.pyc├── main│ ├── __init__.py│ ├── __pycache__│ │ ├── __init__.cpython-37.pyc│ │ ├── functions.cpython-37.pyc│ │ └── views.cpython-37.pyc│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-37.pyc │ ├── functions.cpython-37.pyc │ └── views.cpython-37.pyc ├── functions.py └── views.py5 directories, 14 files pyc是编译的临时文件，我们要把删掉，看下说明，可以用-I来： 12345678910111213141516$ tree -I *.pyc appapp├── __init__.py├── __pycache__├── main│ ├── __init__.py│ ├── __pycache__│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── __pycache__ ├── functions.py └── views.py5 directories, 7 files __pycache__也是临时文件，也把删掉： 12345678910111213tree -I *.pyc -I __pycache__ appapp├── __init__.py├── main│ ├── __init__.py│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── functions.py └── views.py2 directories, 7 files 可以看出-I是可以加多个的，每个-I后面加一个pattern。 在上面的例子中，其实所有的.pyc文件都在__pychache__文件夹下，可以直接忽略该文件夹即可： 12345678910111213$ tree -I __pycache__ appapp├── __init__.py├── main│ ├── __init__.py│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── functions.py └── views.py2 directories, 7 files 那么如果只要文件夹的结构呢？-d参数 123456789$ tree -d appapp├── __pycache__├── main│ └── __pycache__└── module └── __pycache__5 directories 忽略__pycache__文件夹： 123456$ tree -d -I __pycache__ appapp├── main└── module2 directories 总结通过brew安装tree工具之后，即可在命令行中使用tree命令展示文件\文件夹目录树： 直接加对应的文件夹来展示某文件夹范围内的文件树 1$ tree app 使用-I参数来忽略不展示的文件或子文件夹，可添加多个-I 1$ tree -I *.pyc -I __pycache__ app 使用-d来仅展示文件夹树 1$ tree -d app 多参数可以混合使用 1$ tree -d -I __pycache__ app 更多的参数使用，可以在有需要的时候参考--help内容 1$ tree --help]]></content>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何优雅的在flask中记录log]]></title>
    <url>%2F%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E5%9C%A8flask%E4%B8%AD%E8%AE%B0%E5%BD%95log.html</url>
    <content type="text"><![CDATA[背景记录日志，在任何项目中，都是很重要的。在Flask项目中，即有Flask提供的logger可以用来记录log，也可以通过直接使用Python的logging模块自定义logger来记录。那么这两者是什么关系，又该怎么使用呢？ 思路 Python的logging模块 先看下对于logging模块的官方介绍 Loggers have the following attributes and methods. Note that Loggers are never instantiated directly, but always through the module-level function logging.getLogger(name). Multiple calls to getLogger() with the same name will always return a reference to the same Logger object. The name is potentially a period-separated hierarchical value, like foo.bar.baz (though it could also be just plain foo, for example). Loggers that are further down in the hierarchical list are children of loggers higher up in the list. For example, given a logger with a name of foo, loggers with names of foo.bar, foo.bar.baz, and foo.bam are all descendants of foo. The logger name hierarchy is analogous to the Python package hierarchy, and identical to it if you organise your loggers on a per-module basis using the recommended construction logging.getLogger(__name__). That’s because in a module, __name__ is the module’s name in the Python package namespace. https://docs.python.org/3/library/logging.html#logger-objects 上面主要告诉我们两点， 可以通过logging.getLogger(name)来获取一个logger，相同名字的logger，其实是同一个logger。 logger是通过name进行继承的，比如foo.bar就是foo 的子logger。就可以是实现我们通过配置一个rootLogger，然后直接使用rootLogger.sublogger来记录一下内容，而不需要单独再配置一遍。 当使用logging.getLogger(__name__)时，__name__就是这个模块所在的python package的namespace。 flask提供的logger 再看下flask中的logging模块： Flask uses standard Python logging. All Flask-related messages are logged under the &#39;flask&#39; logger namespace.Flask.logger returns the logger named &#39;flask.app&#39;, and can be used to log messages for your application. Depending on the situation, an extension may choose to log to app.logger or its own named logger. Consult each extension’s documentation for details. http://flask.pocoo.org/docs/1.0/logging/ 我们可以知道flask的logger就是一个标准的Python logging，它的命名是flask。我们既可以使用app.logger，也可以自己定义一个logger。 那么如何使用app.logger呢？ 有两种方式： 直接调用 12logger = logging.getLogger('flask.app')logger.info('flask.app') 使用Flask提供的接口 12from flask import current_appcurrent_app.logger.info('logged by current_app from main') 这里推荐还是使用第二种，current_app是一个单例，可以直接引用到app.logger。 通过修改app.logger的name，可以实现子logger的继承么？ 答案是否定的。 修改app.logger的name： 12# app/__init__.pyapp.logger.name = 'app' 然后在子模块中定义一个app.module的logger来记录： 123456789from flask import current_appimport logginglogger = logging.getLogger('app.module')@module.route('/test', methods=['GET'])def test(): logger.info('logged by app.module') current_app.logger.info('logged by current_app.logger') 输出结果： 12019-02-01 10:56:01,877 - Thread-2 - app - INFO - logged by current_app.logger 只有current_app.logger的输出。 修改app.logger的name是不是无效呢？ 我们把子模块中的logger的name修改为flask.app.module： 123456789from flask import current_appimport logginglogger = logging.getLogger('flask.app.module')@module.route('/test', methods=['GET'])def test(): logger.info('logged by flask.app.module') current_app.logger.info('logged by current_app.logger') 输出结果： 122019-02-01 11:00:10,944 - Thread-2 - flask.app.module - INFO - logged by flask.app.module2019-02-01 11:00:10,946 - Thread-2 - app - INFO - logged by current_app.logger 两个logger均输出了。 可见，通过修改app.logger.name可以在记录的时候显示为我们设置的名称，但实际上这个logger还是flask.app。 __name__的使用 在自定义logger的情况下，为了方便起见，我们可以利用__name__这个参数。 前面说到：当使用logging.getLogger(__name__)时，__name__就是这个模块所在的python package的namespace。 一般Flask的工厂模式结构如下： 12345678910app├── __init__.py├── main│ ├── __init__.py│ ├── functions.py│ └── views.py└── module ├── __init__.py ├── functions.py └── views.py 那么我们在先在app.__init__中定义rootLogger，然后再在app.module.functions.py中定义子Logger，均使用logging.getLogger(__name__): 1234567891011121314# app.__init__.py 初始化rootloggerrootLogger = logging.getLogger(__name__) rootLogger.setLevel(logging.DEBUG) socketHandler = logging.handlers.SocketHandler('localhost',logging.handlers.DEFAULT_TCP_LOGGING_PORT) rootLogger.addHandler(socketHandler) rootLogger.setLevel(logging.DEBUG)# app.module.functions.pyimport logginglogger = logging.getLogger(__name__)def record_from_logging(): logger.info('logged by logging from __name__') 输出： 122019-02-01 12:18:34,743 - MainThread - app - INFO - register root logger by __name__2019-02-01 12:19:24,954 - Thread-4 - app.module.functions - INFO - logged by logging from __name__ 可以发现输出的logger.name就是所在的文件目录，logger之间的继承关系与整个程序包保持一致。 总结根据上面分析，那么怎么优雅的记录logger呢？ 如果没有对模块进行分logger记录要求的话。可以直接使用在程序初始化的时候配置app.logger（可以自行设置logger.name）。在模块中通过import current_app来记录： 123456789101112131415161718# app.__init__.pydef register_logging(app): app.logger.name = 'app' # logstash_handler stashHandler = logstash.LogstashHandler('app.config.get('ELK_HOST')', 'app.config.get('ELK_PORT')') app.logger.addHandler(stashHandler) # socket_handler socketHandler = logging.handlers.SocketHandler('localhost', logging.handlers.DEFAULT_TCP_LOGGING_PORT) app.logger.addHandler(socketHandler) # app.module.function.pyfrom flask import current_app@module.route('/test', methods=['GET'])def test(): current_app.logger.info('logging someting') return 'logged by current_app.logger' 输出效果： 122019-02-01 13:49:28,998 - Thread-2 - app - INFO - logged by current_app from main2019-02-01 13:49:38,346 - Thread-3 - app - INFO - logged by current_app of functions 注意: 对于current_app.logger的引用不能通过如下方式，会有RuntimeError的报错。 123456789101112from flask import current_applogger = current_app.logger## 异常 raise RuntimeError(_app_ctx_err_msg)RuntimeError: Working outside of application context.This typically means that you attempted to use functionality that neededto interface with the current application object in some way. To solvethis, set up an application context with app.app_context(). See thedocumentation for more information. 如果希望按自己的实际需求，对模块进行分logger记录要求的话。那么建议自己设置logger。 12345678910111213141516171819202122# app.__init__.pydef register_logging(): # set own root logger rootLogger = logging.getLogger(__name__) rootLogger.setLevel(logging.DEBUG) # socketHandler socketHandler = logging.handlers.SocketHandler('localhost',logging.handlers.DEFAULT_TCP_LOGGING_PORT) rootLogger.addHandler(socketHandler) # logstash_handler stashHandler = logstash.LogstashHandler('app.config.get('ELK_HOST')', 'app.config.get('ELK_PORT')') rootLogger.addHandler(stashHandler) rootLogger.setLevel(logging.DEBUG)# app.module.function.pyimport logginglogger = logging.getLogger(__name__)@module.route('/test', methods=['GET'])def test(): logger.info('logging someting') return 'logged by logging module' 输出效果： 122019-02-01 13:49:49,297 - Thread-5 - app.module.views - INFO - logged by flask.app.module2019-02-01 13:50:01,013 - Thread-7 - app.module.functions - INFO - logged by logging module of functions 完整代码可参考：https://github.com/keejo125/flask_logging_demo 注意关于python中logging的配置可参考官网： https://docs.python.org/3/library/logging.config.html?highlight=logging 在配置handler时，经常会希望日志可以按时间分割(TimedRotatingFileHandler)或者按大小分割(RotatingFileHandler). 但是在flask项目中，尤其开启多线程之后，在分割日志(doRollover())时会有文件读写的异常: 1WindowsError: [Error 32] 建议使用SocketHandler，将日志发送给单独的LogServer来进行二次处理。 简易的接收socketlog的LogServer可参考：https://github.com/keejo125/flask_logging_demo/blob/master/LogServer.py 或者现在流行的stashHandler，将日志发送给ELK来进行二次处理。]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>logging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL按时间统计数据]]></title>
    <url>%2FMySQL%E6%8C%89%E6%97%B6%E9%97%B4%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE.html</url>
    <content type="text"><![CDATA[背景在做数据库的统计时，经常会需要根据年、月、日来统计数据，然后配合echarts来制作可视化效果。 数据库：MySQL 思路 按照时间维度进行统计的前提是需要数据库中有保留时间信息，建议是使用MySQL自带的datetime类型来记录时间。 1`timestamp` datetime DEFAULT NULL, 在MySQL中对于时间日期的处理的函数主要是DATE_FORMAT(date,format)。可用的参数如下 格式 描述 %a 缩写星期名 %b 缩写月名 %c 月，数值 %D 带有英文前缀的月中的天 %d 月的天，数值(00-31) %e 月的天，数值(0-31) %f 微秒 %H 小时 (00-23) %h 小时 (01-12) %I 小时 (01-12) %i 分钟，数值(00-59) %j 年的天 (001-366) %k 小时 (0-23) %l 小时 (1-12) %M 月名 %m 月，数值(00-12) %p AM 或 PM %r 时间，12-小时（hh:mm:ss AM 或 PM） %S 秒(00-59) %s 秒(00-59) %T 时间, 24-小时 (hh:mm:ss) %U 周 (00-53) 星期日是一周的第一天 %u 周 (00-53) 星期一是一周的第一天 %V 周 (01-53) 星期日是一周的第一天，与 %X 使用 %v 周 (01-53) 星期一是一周的第一天，与 %x 使用 %W 星期名 %w 周的天 （0=星期日, 6=星期六） %X 年，其中的星期日是周的第一天，4 位，与 %V 使用 %x 年，其中的星期一是周的第一天，4 位，与 %v 使用 %Y 年，4 位 %y 年，2 位 注：当涉及到按日统计是，需要使用%j，而如果使用%d, %e, %w的话，那么不同月份/周里的相同值会统计在一起。 涉及到获取当前时间，则可以通过now()或者sysdate()来获取。 12SELECT SYSDATE() FROM DUAL;SELECT NOW() FROM DUAL; 按照实际需求使用group by查询即可。 结论需统计的表结构如下： 123456789CREATE TABLE `apilog` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(64) DEFAULT NULL, `action` varchar(64) DEFAULT NULL, `params` text, `result` text, `timestamp` datetime DEFAULT NULL, PRIMARY KEY (`id`)) 统计时间范围内不同分类action的数量 12345678# 当日SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%j&apos;) = DATE_FORMAT(now(),&apos;%j&apos;) ORDER BY count desc;# 当周SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%u&apos;) = DATE_FORMAT(now(),&apos;%u&apos;) ORDER BY count desc;# 当月SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%m&apos;) = DATE_FORMAT(now(),&apos;%m&apos;) ORDER BY count desc;# 当年SELECT action, COUNT(id) count FROM apilog WHERE DATE_FORMAT(`timestamp`,&apos;%Y&apos;) = DATE_FORMAT(now(),&apos;%Y&apos;) ORDER BY count desc; 统计某分类action的时间维度数量 12345678# 按日SELECT action, DATE_FORMAT(`timestamp`,&apos;%j&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%j&apos;)# 按周SELECT action, DATE_FORMAT(`timestamp`,&apos;%u&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%u&apos;)# 按月SELECT action, DATE_FORMAT(`timestamp`,&apos;%m&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%m&apos;)# 按年SELECT action, DATE_FORMAT(`timestamp`,&apos;%Y&apos;), COUNT(id) count FROM apilog WHERE action = &apos;xxx&apos; GROUP BY DATE_FORMAT(`timestamp`,&apos;%Y&apos;) 同时按action和时间维度统计 12345678# 按日SELECT action, DATE_FORMAT(`timestamp`,&apos;%j&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%j&apos;)# 按周SELECT action, DATE_FORMAT(`timestamp`,&apos;%u&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%u&apos;)# 按月SELECT action, DATE_FORMAT(`timestamp`,&apos;%m&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%m&apos;)# 按年SELECT action, DATE_FORMAT(`timestamp`,&apos;%Y&apos;), COUNT(id) count FROM apilog GROUP BY action, DATE_FORMAT(`timestamp`,&apos;%Y&apos;) 以上就是比较常用的时间统计了，更多的时间维度，可以参考上面的参数表类似处理即可。]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的KeyError异常处理]]></title>
    <url>%2FPython%E4%B8%AD%E7%9A%84KeyError%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86.html</url>
    <content type="text"><![CDATA[背景在检查web服务器日志的时候，发现有KeyError的异常报错。检查了下出错的代码： 1applyId = session['applyId'] 应该是用户首次访问时，session为空，所以就获取异常了。 思路根据上面的情况，KeyError就是在获取dict中不存在的key值时触发的。那么有解决方案就有两种： 在读取key值的时候，先校验一下是否存在改key： 1234567d = &#123;'a': 1, 'b': 2, 'c': 3&#125;if 'd' in d: print(d['d'])else: print('not exists!') 输出： 1not exists! 在读取dict时，设置default值。这时候需要使用dict内置的get(key[,default])方法： 1234d = &#123;'a': 1, 'b': 2, 'c': 3&#125;print(d.get('d', 'not exits!')) 输出： 1not exists! 结论最后采用了方法二，如果没有该key，就设置默认为空&#39;&#39;。 1applyId = session.get(&apos;applyId&apos;, &apos;&apos;) 后续再涉及到dict的时候都要注意下默认值的处理，避免出现KeyError的方法还有其他几种方法，可以参考如下的链接，会比较麻烦一点： https://blog.csdn.net/u011089523/article/details/72887163/]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests发请求时timeout配置及异常捕获]]></title>
    <url>%2Frequests%E5%8F%91%E8%AF%B7%E6%B1%82%E6%97%B6timeout%E9%85%8D%E7%BD%AE%E5%8F%8A%E5%BC%82%E5%B8%B8%E6%8D%95%E8%8E%B7.html</url>
    <content type="text"><![CDATA[背景今天有用户在访问web系统时，出现了Nginx返回的超时报错。经排查是由于某台服务器异常，导致web系统requests请求时，一直等待响应，等待时间超过了Nginx配置的超时时间，所以Nginx就直接返回了。 思路 配置timeout 一般在使用requests库时，是不设置超时时间的，那么请求就会一直等待，直到某一方关闭。所以在发请求的时候手工指定下timeout参数。 requests的timeout配置如下： 12345678def request(method, url, **kwargs): """Constructs and sends a :class:`Request &lt;Request&gt;`.... :param timeout: (optional) How many seconds to wait for the server to send data before giving up, as a float, or a :ref:`(connect timeout, read timeout) &lt;timeouts&gt;` tuple. :type timeout: float or tuple... 官网介绍如下： 如果你制订了一个单一的值作为timeout，如下所示： 1r = requests.get('https://github.com', timeout=5) 这一timeout值将会用作 connect和 read二者的 timeout。如果要分别制定，就传入一个元组： 1r = requests.get('https://github.com', timeout=(3.05, 27)) 如果远端服务器很慢，你可以让 Request 永远等待，传入一个None作为timeout值，然后就冲咖啡去吧。 1r = requests.get('https://github.com', timeout=None) http://docs.python-requests.org/zh_CN/latest/user/advanced.html#timeout 捕获异常 通过try... except...可以捕获异常。获取的异常信息如下： 在老版本的python中，有e.message可以获取str格式的报错信息，但要注意的是，虽然在定义中e.message是str类型的，但是实际上未必。类型错误的话，在后续的处理中就可能会造成其他异常了。 1234567891011121314151617181920# e.message 定义class BaseException(object):... args = property(lambda self: tuple()) """:type: tuple""" message = property(lambda self: '', lambda self, v: None, lambda self: None) """:type: string"""...# 输入try: requests.get('http://122.248.19.3', timeout=0.001)except requests.exceptions.ReadTimeout as e: print(e.message) print(type(e.message))# 输出HTTPConnectionPool(host='122.248.19.3', port=80): Read timed out. (read timeout=0.001)&lt;class 'requests.packages.urllib3.exceptions.ReadTimeoutError'&gt; 而在新版本的python中，e.message已经被淘汰了。 1234567891011121314151617181920212223242526272829303132333435363738394041class BaseException(object): """Superclass representing the base of the exception hierarchy. The __getitem__ method is provided for backwards-compatibility and will be deprecated at some point. The 'message' attribute is also deprecated. """ def __init__(self, *args): self.args = args def __str__(self): return str(self.args[0] if len(self.args) &lt;= 1 else self.args) def __repr__(self): func_args = repr(self.args) if self.args else "()" return self.__class__.__name__ + func_args def __getitem__(self, index): """Index into arguments passed in during instantiation. Provided for backwards-compatibility and will be deprecated. """ return self.args[index] def _get_message(self): """Method for 'message' property.""" warnings.warn("the 'message' attribute has been deprecated " "since Python 2.6") return self.args[0] if len(args) == 1 else '' message = property(_get_message, doc="access the 'message' attribute; " "deprecated and provided only for " "backwards-compatibility") https://www.python.org/dev/peps/pep-0352/#transition-plan 所以可以用e.args来获取异常说明，e.args返回的是一个tuple，直接通过str(e.args)转换成str类型做后续处理。 结论代码如下，对不通原因造成的异常，可以加不同的except。 12345678910import requeststry: r = requests.get('https://github.com', timeout=5)except requests.exceptions.ConnectTimeout as e: msg = str(e.args) # do sth with msgexcept requests.exceptions.ReadTimeout as e: msg = str(e.args) # do sth with msg]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取知乎种某问题的所有答案]]></title>
    <url>%2F%E8%8E%B7%E5%8F%96%E7%9F%A5%E4%B9%8E%E7%A7%8D%E6%9F%90%E9%97%AE%E9%A2%98%E7%9A%84%E6%89%80%E6%9C%89%E7%AD%94%E6%A1%88.html</url>
    <content type="text"><![CDATA[背景知乎是一个比较出名也很有趣的网站，里面很多问题和回答也很有意思。之前看了一些爬虫相关文章经常会以抓取知乎来做一些分析。本次也尝试使用python抓取知乎某问题的全部答案。 思路使用爬虫抓取数据其实主要还是要先弄清楚网页展示的方式，现在大部分网页是基于模板动态生成，具体数据通过json等方式传递，这样的话，其实我们只需要直接通过请求抓取json部分即可，而不需要通过获取整个html然后在分析抓取需要部分。 先随便打开一个知乎首页的问题，比如： 通过在chrome里F12查看网络加载可以发现，有一个answer?相关的请求，在右边的Preview里看下返回的数据，果然答案就在里面。 再看仔细分析下返回的这个json结构： 每个json数据中data字段都含有多个答案体 每个答案体里，具体内容存在于content字段，而该字段是一段html，需要使用BeautifulSoup来解析。 每个json数据中paging字段包含了返回的数据是否是这个问题的起止答案，并且给除了上一批答案的请求地址以及下一批答案的请求地址，和总答案数。 于是可以通过随便获取一个请求地址，然后在data字段中获取答案，并根据paging字段来迭代获取其他的答案。 具体实现根据上述的思路，那么实现起来就比较简单了 先获取手动获取感兴趣的问题的网址 在chrome的F12中找到对应的答案请求地址（开头为https://www.zhihu.com/api/v4/questions/） 通过发起请求获取返回的json数据 在返回的json数据中，遍历data字段，并使用·BeautifulSoup来解析出答案 在返回的json数据中，读取paging字段，获取上一个请求地址或者下一个请求地址 迭代请求，直到结束 具体的实现，可以参考我的github，https://github.com/keejo125/。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式中(pattern)和(?:pattern)的使用]]></title>
    <url>%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B8%AD-pattern-%E5%92%8C-pattern-%E7%9A%84%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[背景在项目中有这样一个需求，在页面提交时，需要验证用户输入的网络端口地址是否符合要求。合法的规则如下： 数字端口：123 udp端口：123udp 范围端口：123-234 udp范围端口：123-234udp 多端口使用;分割：123;123udp;123-234;123-234udp 思路 1、单个端口匹配： 数字端口：\d+ udp端口：\d+udp 范围端口：\d+\-\d+ udp范围端口：\d+\-\d+udp 2、多端口匹配 先使用(pattern)来匹配单个端口，然后(pattern)+就可以来匹配多端口的情况了。由于多端口使用;分割，那么每个pattern的开头有两种情况：字符串起始或者;。写成正则就是(^|;)。 将第一步中4中单端口情况使用|并列起来，就有了如下的正则： 1(^|;)(\d+|\d+udp|\d+\-\d+|\d+\-\d+udp) 多端口： 1((^|;)(\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+ 匹配到结尾： 1((^|;)(\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+$ 3、(?:)的使用 在正则中，通过增加?:，使(pattern)变成(?:pattern)，可以实现匹配效果不变，但是不捕获匹配到的内容，从而提升代码的效率。那么上述的正则就变成了： 1(?:(?:^|;)(?:\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+$ 结论在页面中，使用javascript就是： 123var pattern = /^(?:(?:^|;)(?:\d+|\d+udp|\d+\-\d+|\d+\-\d+udp))+$/str = '123;123udp;123-234;123-234udp'console.log(pattern.test(str)) 拓展在正则中(pattern)和(?:pattern)的描述如下表，在仅进行规则匹配而不需要获取匹配到的内容的时候，建议使用(?:pattern)。 字符 描述 (pattern) 匹配 pattern 并获取这一匹配。所获取的匹配可以从产生的 Matches 集合得到，在VBScript 中使用 SubMatches 集合，在JScript 中则使用 0…0…9 属性。 (?:pattern) 匹配 pattern 但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用 “或” 字符 (&#124;) 来组合一个模式的各个部分是很有用。例如， industr(?:y&#124;ies) 就是一个比 ‘industry&#124;industries’ 更简略的表达式。 还有其他更多的符号含义可参考： https://www.cnblogs.com/richiewlq/p/7308005.html]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批量获取AD计算机名信息]]></title>
    <url>%2F%E6%89%B9%E9%87%8F%E8%8E%B7%E5%8F%96AD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%90%8D%E4%BF%A1%E6%81%AF.html</url>
    <content type="text"><![CDATA[背景由于用户加域时需要制定计算机名，为了规范起见，计算机名与AD账号有严格的对应关系。对于一些公共账号来说，就会有很多计算机名。现在需要根据该计算机名的登录时间来筛选出一些废弃计算机名，然后做删除处理，以释放计算机名。 思路在AD管理工具（Active Directory 用户与计算机）中是可以查询计算机名的，并且在计算机的属性中可以查看创建时间和修改时间的。 那么用命令应该就可以批量获取了。AD获取信息的命令为dsget，通过\h获取帮助。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&gt;dsget \h说明: 该工具的命令显示目录中具体对象选择的属性。dsget 命令:dsget computer - 显示目录中计算机的属性。dsget contact - 显示目录中联系人的属性。dsget subnet - 显示目录中子网的属性。dsget group - 显示目录中组的属性。dsget ou - 显示目录中组织单位的属性。dsget server - 显示目录中服务器的属性。dsget site - 显示目录中站点的属性。dsget user - 显示目录中用户的属性。dsget quota - 显示目录中配额的属性。dsget partition - 显示目录中分区的属性。要显示目录中所给对象属性的任意集，请使用 dsquery * 命令 (参见以下示例)。要获取一个具体的命令，请键入 "dsget &lt;ObjectType&gt; /?"，这里的 &lt;ObjectType&gt;是以上显示的一个受支持的对象类型。例如，dsget ou /?。备注:dsget 命令有助于查看目录中特定对象的属性:dsget 的输入是一个对象，输出则是该对象的一系列属性。若要查找满足给定搜索标准的所有对象，请使用 dsquery 命令(dsquery /?)。dsget 命令支持输入管道，以允许您通过管道输入 dsquery 命令的结果，作为 dsget 命令的输入，然后显示 dsquery 命令所找到对象的详细信息。可分辨名称中不用作分隔符的逗号必须用反斜杠("\")字符转义(例如，"CN=Company\, Inc.,CN=Users,DC=microsoft,DC=com")。用在可分辨名称中的反斜杠必须用一个反斜杠转义(例如，"CN=Sales\\ Latin America,OU=Distribution Lists,DC=microsoft,DC=com")。示例:查找姓名以 "John" 开始的所有用户并显示他们的办公室号码: dsquery user -name John* | dsget user -office显示对象的 sAMAccountName、userPrincipalName 和 department 属性，该对象的 DN 是 ou=Test,dc=microsoft,dc=com: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr sAMAccountName userPrincipalName department读取使用 dsquery * 命令的任何对象的所有属性。例如，读取其 DN 为 ou=Test,dc=microsoft,dc=com的对象的所有属性: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr *目录服务命令行工具可帮助:dsadd /? - 帮助添加对象。dsget /? - 帮助显示对象。dsmod /? - 帮助修改对象。dsmove /? - 帮助移动对象。dsquery /? - 帮助查找匹配搜索标准的对象。dsrm /? - 帮助删除对象。dsget 成功 这里我们需要获取计算机名的信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133&gt;dsget computer /?描述: 显示目录中计算机的属性。此命令有两种用法。第一种用法允许您 查看多个计算机的属性。第二种用法允许您查看一个计算机成员身 份的信息。语法: dsget computer &lt;ComputerDN ...&gt; [-dn] [-samid] [-sid] [-desc] [-loc] [-disabled] [&#123;-s &lt;Server&gt; | -d &lt;Domain&gt;&#125;] [-u &lt;UserName&gt;] [-p &#123;&lt;Password&gt; | *&#125;] [-c] [-q] [-l] [&#123;-uc | -uco | -uci&#125;] [-part &lt;PartitionDN&gt; [-qlimit] [-qused]] dsget computer &lt;ComputerDN&gt; [-memberof [-expand]] [&#123;-s &lt;Server&gt; | -d &lt;Domain&gt;&#125;] [-u &lt;UserName&gt;] [-p &#123;&lt;Password&gt; | *&#125;] [-c] [-q] [-l] [&#123;-uc | -uco | -uci&#125;]参数:值 描述&lt;ComputerDN ...&gt; 必需项/stdin。要查看的一台或多台计算机 的可分辨名称(DN)。 如果省略了目标对象，则会从标准 输入(stdin)中读取这些对象，以支持 通过管道将其他命令的输出 用作此命令的输入。 请与以下的 &lt;ComputerDN&gt; 相比。-dn 显示计算机 DN。-samid 显示计算机的 SAM 帐户名。-sid 显示计算机的安全 ID(SID)。-desc 显示计算机的描述。-loc 显示计算机的位置。-disabled 显示计算机帐户是(yes)否(no) 被禁用。&lt;ComputerDN&gt; 必需项。要查看计算机的 可分辨名称(DN)。-memberof 显示计算机所属的组。-expand 显示计算机所属组的循环 扩展列表。此选项采用 计算机直属组成员列表 并递归扩展该列表中 的每个组，以决定其组成员 身份和获得组的完整集。&#123;-s &lt;Server&gt; | -d &lt;Domain&gt;&#125; -s &lt;Server&gt; 用 &lt;Server&gt; 名称连接到 AD DC/LDS 实例。 -d &lt;Domain&gt; 连接到域 &lt;Domain&gt; 中的 AD DC。 默认: 登录域中的 AD DC。-u &lt;UserName&gt; 以 &lt;UserName&gt; 身份连接。默认: 登录的用户。 用户名可以采用: 用户名、域\用户名 或用户主体名称(UPN)。-p &#123;&lt;Password&gt; | *&#125; 用户 &lt;UserName&gt; 的密码。如果是 *， 则会提示您输入密码。-c 连续操作模式: 指定了多个目标对象时，将 报告错误，但继续处理参数列表中的 下一个对象。若无此选项，命令将在遇到 第一个错误时退出。-q 安静模式: 将所有输出抑制到标准输出。-L 以列表格式显示搜索结果集中的项目。 默认: 表格格式。&#123;-uc | -uco | -uci&#125; -uc 指定来字管道的输入或至管道的输出 用 Unicode 格式。 -uco 指定至管道或文件的输出 用 Unicode 格式。 -uci 指定来自管道或文件的输入 用 Unicode 格式。-part &lt;PartitionDN&gt; 用 &lt;PartitionDN&gt; 的可分辨名称 连接到目录分区。-qlimit 显示计算机在指定目录分区中 的有效配额。-qused 显示计算机在指定目录分区中的 已使用配额。备注:如果您在命令提示符处没有提供目标对象，则会从标准输入(stdin)中获取目标对象。可以通过键盘、重定向文件或另一个命令的管道输出接受 Stdin 数据。若要通过键盘或在重定向文件中标记 stdin 数据的结束，请使用 Control+Z 表示文件结束(EOF)。配额规定决定一个给定安全主体在一个特定目录分区中能够拥有的最大目录对象数。dsget 命令帮助您查看目录中某个特定对象的属性: dsget 的输入是一个对象，输出是该对象的属性列表。若要查找满足所给搜索条件的所有对象，请使用 dsquery 命令(dsquery /?)。如果您提供的值包含空格，请在文本两边使用引号(例如，"CN=DC2,OU=Domain Controllers,DC=microsoft,DC=com")。如果您输入了多个值，这些值必须用空格隔开(例如，一个系列可分辨名称)。示例:查找在给定 OU 中名称以 "tst" 开头的所有计算机并显示其说明。 dsquery computer ou=Test,dc=microsoft,dc=com -name tst* | dsget computer -desc显示给定计算机 "MyDBServer" 所属的组的列表(以递归方式展开): dsget computer cn=MyDBServer,cn=computers,dc=microsoft,dc=com -memberof -expand要显示给定计算机 "MyDBServer" 在给定分区 "cn=domain1,dc=microsoft,dc=com" 上的有效配额和已用配额，请键入: dsget computer cn=MyDBServer,cn=computers,dc=microsoft,dc=com -part cn=domain1,dc=microsoft,dc=com -qlimit -qused另请参阅:dsget - 描述适用于所有命令的参数。dsget computer - 显示目录中计算机的属性。dsget contact - 显示目录中联系人的属性。dsget subnet - 显示目录中子网的属性。dsget group - 显示目录中组的属性。dsget ou - 显示目录中组织单位的属性。dsget server - 显示目录中服务器的属性。dsget site - 显示目录中站点的属性。dsget user - 显示目录中用户的属性。dsget quota - 显示目录中配额的属性。dsget partition - 显示目录中分区的属性。目录服务命令行工具帮助:dsadd /? - 有关添加对象的帮助。dsget /? - 有关显示对象的帮助。dsmod /? - 有关修改对象的帮助。dsmove /? - 有关移动对象的帮助。dsquery /? - 有关查找符合搜索条件的对象的帮助。dsrm /? - 有关删除对象的帮助。dsget 成功 可以看到，能显示的计算机信息如下： 12345678910111213141516-dn 显示计算机 DN。-samid 显示计算机的 SAM 帐户名。-sid 显示计算机的安全 ID(SID)。-desc 显示计算机的描述。-loc 显示计算机的位置。-disabled 显示计算机帐户是(yes)否(no) 被禁用。&lt;ComputerDN&gt; 必需项。要查看计算机的 可分辨名称(DN)。-memberof 显示计算机所属的组。-expand 显示计算机所属组的循环 扩展列表。此选项采用 计算机直属组成员列表 并递归扩展该列表中 的每个组，以决定其组成员 身份和获得组的完整集。 那么修改日期的信息就只能通过AD工具来查看了。 再看下dsquery命令会不会有戏： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&gt;dsquery /?描述: 该工具的命令集允许您根据指定的标准查询目录。除 dsquery * 之外 (dsquery * 可以查询任何类型的对象)，以下每一个 dsquery 命令均可查找一个特定对象类型:dsquery computer - 查找目录中的计算机。dsquery contact - 查找目录中的联系人。dsquery subnet - 查找目录中的子网。dsquery group - 查找目录中的组。dsquery ou - 查找目录中的组织单位。dsquery site - 查找目录中的站点。dsquery server - 查找目录中的 AD DC/LDS 实例。dsquery user - 查找目录中的用户。dsquery quota - 查找目录中的配额规定。dsquery partition - 查找目录中的分区。dsquery * - 用通用的 LDAP 查询来查找目录中的任何对象。若要查找特定命令的帮助，请键入 "dsquery &lt;ObjectType&gt; /?"，其中&lt;ObjectType&gt; 是以上所示的受支持对象类型之一。例如，dsquery ou /?。备注:dsquery 命令帮助您查找目录中与指定搜索标准匹配的对象: dsquery 的输入是一个搜索标准，其输出是与该搜索匹配的一系列对象。若要获取特定对象的属性，请使用 dsget 命令(dsget /?)。可以将 dsquery 命令的结果通过管道输出，作为一个其他目录服务命令行工具(如 dsmod、dsget、dsrm 或 dsmove)的输入。可分辨名称中不是用作分隔符的逗号必须用反斜杠("\")字符转义(例如，"CN=Company\, Inc.,CN=Users,DC=microsoft,DC=com")。用在可分辨名称中的反斜杠必须用一个反斜杠转义(例如，"CN=Sales\\ Latin America,OU=Distribution Lists,DC=microsoft,DC=com")。示例:查找过去四个星期内处于非活动状态的计算机并将其从目录中删除: dsquery computer -inactive 4 | dsrm查找组织单位所有的用户 "ou=Marketing,dc=microsoft,dc=com" 并将他们添加到Marketing Staff 组: dsquery user ou=Marketing,dc=microsoft,dc=com | smod group "cn=Marketing Staff,ou=Marketing,dc=microsoft,dc=com" -addmbr查找姓名以 "John" 开始的所有用户并显示他的办公室号码: dsquery user -name John* | dsget user -office要显示目录中所给对象属性的任意集，请使用 dsquery * 命令。例如，要显示对象(该对象的 DN 是 ou=Test，dc=microsoft，dc=com) 的 sAMAccountName，userPrincipalName 和 department 属性: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr sAMAccountName userPrincipalName department要读取对象(该对象的 DN 是 ou=Test，dc=microsoft，dc=com) 的所有属性: dsquery * ou=Test,dc=microsoft,dc=com -scope base -attr *目录服务命令行工具帮助:dsadd /? - 添加对象的帮助。dsget /? - 显示对象的帮助。dsmod /? - 修改对象的帮助。dsmove /? - 移动对象的帮助。dsquery /? - 查找与搜索标准匹配对象的帮助。dsrm /? - 删除对象的帮助。 虽然没有直接我们想要的，但是其中一个例子却很有启发： 123查找过去四个星期内处于非活动状态的计算机并将其从目录中删除: dsquery computer -inactive 4 | dsrm 没办法通过命令来回去修改时间，却可以直接根据非活动状态的时间选出计算机名。 那么我们最终想要的效果——获取长时间未活动的计算机名，就可以直接得到了，而不需要另行计算了。 结论根据上面的思路，可以使用dsquery命令 -name test*来筛选包含test 所有计算机名 -inactive 24来筛选过去24周，也就是半年非活动的计算机名 完整命令如下： 1dsquery computer -name TEST* -inactive 24]]></content>
      <categories>
        <category>操作系统</category>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>ad</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery4.2在Python3.7下无法运行的问题]]></title>
    <url>%2FCelery4-2%E5%9C%A8Python3-7%E4%B8%8B%E6%97%A0%E6%B3%95%E8%BF%90%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景之前使用Flask + Celery + Redis来实现异步队列处理，使用的环境是python3.6，后来由于Mac系统下，使用brew安装的python，直接升级到了python3.7，相同的程序运行就报错了。 问题报错情况提示如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445[2019-01-08 23:15:05,188: CRITICAL/MainProcess] Unrecoverable error: SyntaxError('invalid syntax', ('/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/backends/redis.py', 22, 19, 'from . import async, base\n'))Traceback (most recent call last): File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/kombu/utils/objects.py", line 42, in __get__ return obj.__dict__[self.__name__]KeyError: 'backend'During handling of the above exception, another exception occurred:Traceback (most recent call last): File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/worker/worker.py", line 205, in start self.blueprint.start(self) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/bootsteps.py", line 115, in start self.on_start() File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/apps/worker.py", line 139, in on_start self.emit_banner() File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/apps/worker.py", line 154, in emit_banner ' \n', self.startup_info(artlines=not use_image))), File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/apps/worker.py", line 217, in startup_info results=self.app.backend.as_uri(), File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/kombu/utils/objects.py", line 44, in __get__ value = obj.__dict__[self.__name__] = self.__get(obj) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/base.py", line 1196, in backend return self._get_backend() File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/base.py", line 914, in _get_backend self.loader) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/backends.py", line 70, in by_url return by_name(backend, loader), url File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/app/backends.py", line 50, in by_name cls = symbol_by_name(backend, aliases) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/kombu/utils/imports.py", line 56, in symbol_by_name module = imp(module_name, package=package, **kwargs) File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/importlib/__init__.py", line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) File "&lt;frozen importlib._bootstrap&gt;", line 1006, in _gcd_import File "&lt;frozen importlib._bootstrap&gt;", line 983, in _find_and_load File "&lt;frozen importlib._bootstrap&gt;", line 967, in _find_and_load_unlocked File "&lt;frozen importlib._bootstrap&gt;", line 677, in _load_unlocked File "&lt;frozen importlib._bootstrap_external&gt;", line 724, in exec_module File "&lt;frozen importlib._bootstrap_external&gt;", line 860, in get_code File "&lt;frozen importlib._bootstrap_external&gt;", line 791, in source_to_code File "&lt;frozen importlib._bootstrap&gt;", line 219, in _call_with_frames_removed File "/Users/zhengk/.local/share/virtualenvs/ToRandom-3HChPPPT/lib/python3.7/site-packages/celery/backends/redis.py", line 22 from . import async, base ^SyntaxError: invalid syntax 结论这个错误有点奇怪，经过一番百度谷歌，终于发现问题的原因。由于celery中有一个文件名命名为async，而在python3.7中，新增两个关键字，其中一个恰好就是async，另一个是await。 https://docs.python.org/3/whatsnew/3.7.html celery的作者也在issue中表示，后续版本会将async这个文件改为asynchronous，但目前版本还未发布（可能会在4.2.2）。现在可以通过github直接安装新版，pipenv环境下安装如下： 1234567pipenv install https://github.com/celery/celery/tarball/masterInstalling https://github.com/celery/celery/tarball/master…✔ Installation Succeeded Pipfile.lock (d4f0f1) out of date, updating to (dccb00)…Locking [dev-packages] dependencies…Locking [packages] dependencies…✔ Success! https://github.com/celery/celery/issues/4849 拓展flask + celery + redis 的单文件和工厂模式demo可参考如下git： https://github.com/keejo125/flask_celery_redis_demo]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[巧用kill重新加载配置并启动进程]]></title>
    <url>%2F%E5%B7%A7%E7%94%A8kill%E9%87%8D%E6%96%B0%E5%8A%A0%E8%BD%BD%E9%85%8D%E7%BD%AE%E5%B9%B6%E5%90%AF%E5%8A%A8%E8%BF%9B%E7%A8%8B.html</url>
    <content type="text"><![CDATA[背景前期在服务器上使用gunicorn托管了一个flask项目，近期修改了配置要重启一下。于是查了下如何优雅的重启进程。 思路修改配置重启进程，最简单的方法就是使用ps -ef|grep xxx命令来找到对应的进程，然后kill -9 pid来结束进程在重新启动。 对于gunicorn来说，稍微有点不同。一般会启动多个worker来跑，比如 1gunicorn -w 4 manager:app 这样的话，使用平常的ps -ef|grep gunicorn就会发现有多个进程，有时候直接kill后还会自动启动。 正确的方法是： 通过pstree来找到gunicorn 的主进程： 1234567$ pstree -ap|grep gunicorn | |-grep,18737 --color=auto gunicorn | `-gunicorn,18205/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | |-gunicorn,17455/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | |-gunicorn,17456/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | |-gunicorn,17457/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi | `-gunicorn,17458/home/torandom/.local/share/virtualenvs/ToRandom-w9b3mFRo/bi 这里就会看到，主进程是18737 然后在通过kill命令来结束进程 1kill -9 18737 在启动gunicorn。 这时候发现了有另外一个kill命令，可以直接让进程重新加载配置并启动，这就是 1kill -HUP 18737 重启之后再次使用pstree会发现guncorn的进程号发生了变化，即原进程已经销毁，新建了新的进程。 http://www.chenxm.cc/article/561.html 结论经查询，这其实是liunx中带信号的kill命令起的作用。上述命令其实是对进程发送了一个HUP信号，而很多程序会把HUP信号作为重新读取配置文件的触发条件，当接收到这个信号的时候，不会杀死进程，而是重新读取配置并运行。 1234567891011121314# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX 通过上述命令可以看到SIGHUP是1号信号，也就是说kill -1 pid和kill -HUP pid是等效的。 在logstash中，我们使用kill -1 pid 来实现重新加载配置，其实也是这个道理。]]></content>
      <categories>
        <category>操作系统</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>gunicorn</tag>
        <tag>logstash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript实现网页倒计时并跳转页面]]></title>
    <url>%2FJavaScript%E5%AE%9E%E7%8E%B0%E7%BD%91%E9%A1%B5%E5%80%92%E8%AE%A1%E6%97%B6%E5%B9%B6%E8%B7%B3%E8%BD%AC%E9%A1%B5%E9%9D%A2.html</url>
    <content type="text"><![CDATA[背景之前想自己从头搭建一个个人博客，后来各种原因直接用了hexo并托管在coding上，效果也不错。于是打算在个人服务器上直接建一个倒计时跳转页面，转到hexo， 也不浪费自己买的域名，哈哈哈。本来想直接跳转，感觉还是有一个倒计时提醒比较好一点。 思路 首先先写一个html5的简单的文字页面就好，留出一个div放倒计时的数字就好了。 然后JavaScript中可以用setInterval来实现按周期调用函数功能，也就是倒计时，每过一秒，数字减1。 setInterval()方法是按照指定的周期（毫秒）来调用函数或计算表达式。 12// 每三秒（3000 毫秒）弹出 "Hello" :setInterval(function()&#123; alert("Hello"); &#125;, 3000); http://www.runoob.com/jsref/met-win-setinterval.html 页面跳转就比较简单了location.href=就可以实现跳转了。 最后整个函数需要在页面加载完成之后进行，也就是放在window.onload里。 window.onload事件会在页面或者图像加载完成后立刻发生，通常用于&lt;body&gt;元素。 1234// html中&lt;body onload="SomeJavaScriptCode"&gt;// js中window.onload=function()&#123;SomeJavaScriptCode&#125;; http://www.runoob.com/jsref/event-onload.html 实现根据上面的思路，实现就比较简单了，在页面部分显示倒计时数字的div设置了id=&quot;time&quot;，在onload事件中，每隔1000毫秒调用一次函数使倒计时数字减1，当倒计时结束时跳转页面。 123456789101112131415161718192021222324252627282930313233343536&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;图兰登&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt; &lt;div style="text-align: center;"&gt; &lt;h1&gt;你好，这里是图兰登。&lt;/h1&gt; &lt;h3&gt;欢迎进入我的个人博客！&lt;/h3&gt; &lt;div style="text-align: center"&gt; &lt;div style="display: inline-block"&gt;倒计时：&lt;/div&gt; &lt;div id="time" style="display: inline-block"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;script&gt; window.onload = function () &#123; let oDiv = document.getElementById("time"); let count = 5; oDiv.innerHTML = count; let timer = null; timer = setInterval(function () &#123; if (count &gt; 0) &#123; count = count - 1; oDiv.innerHTML = count; &#125; else &#123; location.href='http://keejo.coding.me' &#125; &#125;, 1000); &#125;&lt;/script&gt;&lt;/html&gt; 最终效果可以见链接：https://www.torandom.com]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>html5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScrip实现Strip()功能]]></title>
    <url>%2FJavaScrip%E5%AE%9E%E7%8E%B0Strip-%E5%8A%9F%E8%83%BD.html</url>
    <content type="text"><![CDATA[背景之前用JS做了一个输入校验的功能，要求输入的必须是一个合法的email地址，结果在自己测试的时候发现输入不通过。检查发现，现在手机输入法（搜狗）联想输入的时候，默认会在输入的词语后面加上一个空格，所以导致校验的正则不通过。 思路Email校验的正则如下： 12let reg = new RegExp("^[a-z0-9]+([._\\-]*[a-z0-9])*@([a-z0-9]+[-a-z0-9]*[a-z0-9]+.)&#123;1,63&#125;[a-z0-9]+$")reg.test(email) 根据上述正则，是不允许字符串开头结尾有空格之类的多余字符的。那么解决这个问题就有两个方案： 修改正则，允许开头、结尾有空格等符号 对输入做处理，删除开头、结尾的空格等符号 这里我选了第二种，因为一直有在写python，其中有一个strip()函数，可以自动的删除字符串开头结尾的空格等多余符号的，所以理所当然认为javascript也有，结果发现竟然是没有的。 那么需要自己写一个了。 对字符串的处理，最简单粗暴的也就是正则了。 在正则中\s可以匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。注意 Unicode 正则表达式会匹配全角空格符。 http://www.runoob.com/regexp/regexp-syntax.html 先找从字符串开头起的\s : ^\s+ 再找字符串结尾的\s: \s+$ 将找到的字符串替换为空&#39;&#39; 结论根据上面的思路，表达式也就出来了： 1let email = email.replace(/^\s+|\s$/g, '') 其中/g是用于全局替换的，如果不加/g，那么当开头和结尾都有空格等符号时，仅会替换开头部分。 http://www.runoob.com/jsref/jsref-regexp-g.html]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序部署EACCES: permission denied问题]]></title>
    <url>%2F%E5%B0%8F%E7%A8%8B%E5%BA%8F%E9%83%A8%E7%BD%B2EACCES-permission-denied%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景开发工具： 微信开发者工具 腾讯云环境： node 问题之前做了一个账本excel导出功能，可见用exceljs实现Json对象导出excel。大体逻辑是在项目根目录生成一个临时的excel文件，然后通过邮件发送给用户之后，删除临时文件。 在本地测试通过之后，部署到腾讯云开发环境，点击导出按钮，就报错了。直接请求failed，看了下报错信息： 思路问题其实比较明确，没有权限。按照功能的实现思路，在生成excel文件时第一次打开，发送邮件时第二次用到这个文件。所以问题应该出在程序无法在根目录写入文件，生成excel文件。 注：后续和腾讯云确认，根目录的确是授权给单独一个用户用于部署程序，所以node运行用户无权限在根目录写入文件。 那么解决方法就只有两个 找一个有权限的目录 给node运行的用户授权 腾讯云的小程序服务器端部署是一套完整的自动化流程，个人无法直接访问服务器确认用户权限，所以在不更换部署服务的情况下，只能选择第一种。 解决由于使用的腾讯官方的wafer2框架，在demo中是有上传文件的案例的。 在wafer-node-sdk的node包中lib\upload\index.js中可以看到源代码： 1234567// 初始化 multiparty const form = new multiparty.Form(&#123; encoding: 'utf8', maxFilesSize: maxSize * 1024 * 1024, autoFiles: true, uploadDir: '/tmp' &#125;) demo中，文件是先上传至/tmp目录下，然后再转保存在cos中。由此可以确定/tmp目录是可以写入文件的。 将代码中临时生成的excel文件路径也放在/tmp目录下，果然问题解决了。 插曲在部署到生产环境时，先上传代码，然后点击“安装依赖”，最后点击“部署代码”。一路都显示成功，但结果却发现node环境挂了，直接用web访问生产环境提示bad getway。 心里一紧，再次点击“部署代码”，这次有报错出来了，提示是某依赖没有找到。 于是又点了一次安装依赖，待提示安装成功之后，再点部署代码，就ok了。 事后咨询了腾讯云的支持，理论上生产环境的部署是不需要手动安装依赖的。系统会自动从package.json中拉取依赖清单并安装。这次就不知道是什么情况，记录一下。]]></content>
      <categories>
        <category>微信小程序</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>微信小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用nodemailer实现邮件发送]]></title>
    <url>%2F%E7%94%A8nodemailer%E5%AE%9E%E7%8E%B0%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81.html</url>
    <content type="text"><![CDATA[背景由于随手记账本是基于小程序的，没办法直接通过浏览器下载的方式导出给用户。于是考虑在导出请求时，要求用户提供一个电子邮箱，后台生成导出的excel文件之后直接以附件的形式发送到用户邮箱中。目前是通过SendGrid提供的免费邮箱服务来实现邮箱发送。SendGrid也提供各个版本的webapi支持，不过考虑到后续兼容性，本次就摒弃了SendGrid提供的接口组件，使用nodemail。 思路发送邮件的方式基本大同小异，使用smtp协议的话需要知道： 邮箱服务器地址 端口号 用户名 密码 然后邮件内容上需要明确： 收件人 主题 邮件内容 是否为html格式 附件 发件人（如果通过个人邮箱开通smtp发件，那么可能不支持另外设置发件人。因为发件人就是自己，SendGrid是支持只是发件人的，比如`no-reply@torandom.com`。） 最后按照使用的邮件模块的说明发送即可。 实现nodemailer的文档不是太直观，说明的很详细，但是没有一个完整的简单例子。普通的单次邮件发送主要有这么几个步骤： 设置smtpConfig信息 设置message信息 创建transporter对象 调用transporter对象的sendMail()发送邮件并接受回调 代码如下： 12345678910111213141516171819202122232425262728293031323334353637const nodemailer = require('nodemailer')// 配置邮件服务器信息let smtpConfig = &#123; host: 'smtp.sendgrid.net', port: 587, secure: false, // upgrade later with STARTTLS auth: &#123; user: 'username', pass: 'password' &#125;&#125;// 配置邮件内容信息let message = &#123; from: 'no-reply@torandom.com', to: 'test@torandom.com', subject: '随手记账本 - 导出', text: '随手记账本导出测试', html: '&lt;p&gt;HTML version of the message&lt;/p&gt;', attachments: [ &#123; filename: 'package.json', path: './package.json' &#125; ]&#125;// 创建transporter对象let transporter = nodemailer.createTransport(smtpConfig)// 发送邮件transporter.sendMail(message) .then(info =&gt; &#123; if (info.accepted) &#123; console.log('已发送至' + info.accepted.toString()) &#125; else &#123; console.log('邮件发送失败') console.log(info) &#125; &#125;) 注意 在发送邮件部分。回调的info内容是由你的邮件服务器提供的，不同的邮件服务器提供的内容是不同的需要确认。 如果测试代码的时候，发送到自己的QQ邮箱。发送的数量太多了的话，会有如下的报错： 123456789101112550 Connection frequency limited出错原因：该服务器IP的发信频率超过腾讯邮箱限制。 腾讯邮箱对来自相同IP的外部发信服务器有一定的频率限制： 1、超过每分钟发信量限制，此IP地址被禁止发信若干分钟。 2、超过每小时发信量限制，此IP地址被禁止发信若干小时。 3、超过每日发信量限制，此IP地址本日内禁止再发信。 4、以上频率限制数值属于腾讯邮箱保密数据，恕不公开。 改善建议：如果您是该服务器IP的管理员，请暂停该服务器IP的发信，稍后降低频率重新尝试发信。 如果您是个人邮箱用户，请向您的电子邮件提供商报告此情况。 https://service.mail.qq.com/cgi-bin/help?subtype=1&amp;id=20022&amp;no=1000722 这时候需要把你的发件人加入到接受邮件的白名单中即可。 关于nodemailer的更多高级用法，比如邮件池之列的请参考如下官方文档。 https://nodemailer.com/about/]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>邮件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用exceljs实现Json对象导出excel]]></title>
    <url>%2F%E7%94%A8exceljs%E5%AE%9E%E7%8E%B0Json%E5%AF%B9%E8%B1%A1%E5%AF%BC%E5%87%BAexcel.html</url>
    <content type="text"><![CDATA[背景在做随手记账本项目的时候，很多网友在意见反馈中建议提供导出功能。由于小程序的后台是基于node的，于是在npm里找了下关于excel的包，也参考了百度建议，推荐比较多的是excelexport，但是我最后选了exceljs。主要是一直在持续更新，文档也很全面。 思路由于后台数据库保存的账本数据是采用json格式的，最简单的方法就是通过遍历账本中的所有条目逐行写入excel。但是发现在exceljs这个包中，是提供列定义的，通过定义每列的key，就可以直接把json数据写入了，这个功能非常赞。 这部分的文档说明如下： 123456789101112// Add column headers and define column keys and widths// Note: these column structures are a workbook-building convenience only,// apart from the column width, they will not be fully persisted.worksheet.columns = [ &#123; header: 'Id', key: 'id', width: 10 &#125;, &#123; header: 'Name', key: 'name', width: 32 &#125;, &#123; header: 'D.O.B.', key: 'DOB', width: 10, outlineLevel: 1 &#125;];// Add a couple of Rows by key-value, after the last current row, using the column keysworksheet.addRow(&#123;id: 1, name: 'John Doe', dob: new Date(1970,1,1)&#125;);worksheet.addRow(&#123;id: 2, name: 'Jane Doe', dob: new Date(1965,1,7)&#125;); 需要说明的说，文档里定义的column里的header，会自动写在表格的第一行当做表头，不需要另起一行。 实现主要有这么几个步骤： 新建workbook对象 新建sheet 定义column 写入row 导出 代码如下： 1234567891011121314151617181920212223242526272829303132333435const Excel = require('exceljs')// 测试数据let data = &#123; "subtitle":"偶遇美食节", "comment":"鲷鱼烧，烤鱼，年糕丸子。600+200+350", "cost":1150, "date":"2018-01-20", "time":"14:24", "member":1, "type":"餐饮", "currency":"日元", "location":"Ueno Park (上野恩賜公園)"&#125;// 新建workbook对象let workbook = new Excel.Workbook()// 设置workbook属性，比如作者workbook.creator = 'ToRandom'// 新建sheetlet tempWorksheet = workbook.addWorksheet('东京之旅')// 定义column, 日期比较长，设置为15 可以展示yyyy-mm-ddtempWorksheet.columns = [ &#123;header: '标题', key: 'subtitle'&#125;, &#123;header: '消费类型', key: 'type'&#125;, &#123;header: '评论', key: 'comment'&#125;, &#123;header: '币种', key: 'currency'&#125;, &#123;header: '费用', key: 'cost'&#125;, &#123;header: '日期', key: 'date', width: 15&#125;, &#123;header: '时间', key: 'time'&#125;, &#123;header: '人数', key: 'member'&#125;, &#123;header: '位置信息', key: 'location'&#125; ]// 写入tempWorksheet.addRow(data)// 保存文件，如有需要，前面加await等待执行workbook.xlsx.writeFile("随手记账本.xlsx) 注意 一般node默认是异步执行的，如有后续操作（比如先生成excel文件之后，以附件的形式发送），那么就需要在最后一步保存文件的代码前面加上await。 1await workbook.xlsx.writeFile("随手记账本.xlsx") 如果生成之后，处理完了要删除的话，可以使用fs.unlink来删除。但注意的是，这个unlink也是一个异步函数，删除效果会有延迟。 其他关于exceljs的操作，可以参考原文档，还是相当丰富的。 https://www.npmjs.com/package/exceljs]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>Json</tag>
        <tag>node</tag>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS下部署selenium环境]]></title>
    <url>%2FCentOS%E4%B8%8B%E9%83%A8%E7%BD%B2selenium%E7%8E%AF%E5%A2%83.html</url>
    <content type="text"><![CDATA[背景最近写了一个循环抓取某网站数据的代码，其中涉及到页面登陆，采用了selenium来做。考虑到循环抓取，本机跑容易因系统休眠断网造成爬取失败，于是在自己的服务器上部署一下。 操作系统：CentOS 7 Python版本：Python3.7 问题由于服务器抓取，其实不需要展示浏览器的界面，可以考虑使用PhantomJS来做，结果发现有如下的警告提示： 1UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead 既然后续不支持了，那么就按照官方建议，我选chrome。 不同于PhamtomJS，chromedriver需要和chrome配合使用，也就是如果不安装chrome，直接加载chromedriver那么就会有如下的报错： 123raise exception_class(message, screen, stacktrace)selenium.common.exceptions.WebDriverException: Message: unknown error: cannot find Chrome binary (Driver info: chromedriver=2.45.615279 (12b89733300bd268cff3b78fc76cb8f3a7cc44e5),platform=Linux 3.10.0-693.2.2.el7.x86_64 x86_64) 而且MacOS和CentOS的chromedriver是不同的。如果在linux下直接加载mac版的chromedriver就会如下的报错： 12raise child_exception_type(errno_num, err_msg, err_filename)OSError: [Errno 8] Exec format error: './chromedriver' 部署正确的部署应该是这样的： 安装chromeCentOS服务器是用ssh登陆的，就直接用yum来安装好了，由于存在翻墙的问题，要手动配置下源： 12cd /etc/yum.repos.d/vim google-chrome.repo 然后添加如下语句： 123456[google-chrome]name=google-chromebaseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearchenabled=1gpgcheck=1gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub 再用yum安装： 1yum -y install google-chrome-stable --nogpgcheck 在未翻墙的环境下，一定要加上--nogpgcheck选项，否则会因为检查失败而无法安装成功。 https://blog.csdn.net/u010472499/article/details/72327963 安装chromedriver墙内可以在如下地址下载对应系统的chromedriver: http://npm.taobao.org/mirrors/chromedriver/2.45/ 下载好的chromedriver需要放到PATH目录下，建议是自己使用virtualenv新建一个venv虚拟环境，然后将chromedriver放到虚拟环境的bin目录下（venv/bin）即可。 selenium的使用加上headless的配置即可，关键代码如下： 123456from selenium import webdriveroption = webdriver.ChromeOptions()option.add_argument('headless')driver = webdriver.Chrome(executable_path='./chromedriver', options=option)# do somethingdriver.quit()]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>selenium</tag>
        <tag>爬虫</tag>
        <tag>chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx与tornado框架的并发评测]]></title>
    <url>%2FNginx%E4%B8%8Etornado%E6%A1%86%E6%9E%B6%E7%9A%84%E5%B9%B6%E5%8F%91%E8%AF%84%E6%B5%8B.html</url>
    <content type="text"><![CDATA[背景分别测试在windows平台和linux平台(SuSE)下，tornado框架的并发效果，以及通过配置nginx对并发效果影响。 操作系统： windows: Windows Server 2008 SP2 （8C8G) linux: SuSE12 SP3 （8C8G) 并发测试工具：tsung 测试访问：仅返回”Hello World”字符 评测过程1、直接访问tornado，并发设置为500 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 36.11 msec 32.51 msec 620.3 / sec 138.53 / sec 33.18 msec 44464 page 0.11 sec 33.02 msec 1221.1 / sec 272.69 / sec 76.98 msec 87507 request 0.11 sec 33.02 msec 1221.1 / sec 272.69 / sec 76.98 msec 87507 session 1mn 56sec 8.66 sec 14.8 / sec 1.66 / sec 12.64 sec 498 Code Highest Rate Mean Rate Total number 200 616 / sec 138.70 / sec 44536 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 error_abort_max_conn_retries 4.7 / sec 104 error_abort_max_send_retries 13.9 / sec 394 error_connect_econnrefused 35.8 / sec 581 error_connection_closed 35.7 / sec 1023 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.21 sec 30.57 msec 760 / sec 127.01 / sec 34.44 msec 41788 page 0.21 sec 30.97 msec 1474.9 / sec 247.78 / sec 39.04 msec 81576 request 0.21 sec 30.97 msec 1474.9 / sec 247.78 / sec 39.04 msec 81576 session 2mn 43sec 3.33 sec 11.9 / sec 1.66 / sec 5.48 sec 498 Code Highest Rate Mean Rate Total number 200 762.6 / sec 127.15 / sec 41867 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 error_abort_max_send_retries 11.9 / sec 498 error_connection_closed 38.4 / sec 1500 说明 在500的并发量下，无论在windows还是liunx平台，均有较多的连接错误。但框架系统还是比较稳定的，没有出现崩溃等情况。 2 、通过Nginx代理访问，并发设置为500 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.18 sec 31.78 msec 2287.5 / sec 588.11 / sec 0.11 sec 179083 page 0.58 sec 0.17 sec 2283.4 / sec 587.32 / sec 0.36 sec 178778 request 0.58 sec 0.17 sec 2283.4 / sec 587.32 / sec 0.36 sec 178778 Code Highest Rate Mean Rate Total number 200 1736.4 / sec 307.54 / sec 94844 502 1122.9 / sec 280.16 / sec 84048 Name Highest Rate Total number error_abort 0.5 / sec 1 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.17 sec 32.16 msec 801 / sec 710.33 / sec 73.15 msec 216931 page 0.69 sec 0.16 sec 798 / sec 709.48 / sec 0.58 sec 216633 request 0.69 sec 0.16 sec 798 / sec 709.48 / sec 0.58 sec 216633 Code Highest Rate Mean Rate Total number 200 794.9 / sec 709.79 / sec 216726 504 0.7 / sec 0.04 / sec 11 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 说明 增加Nginx做反向代理之后，有效的提供了一定的缓冲。在windows平台下出现较多的code 502，说明后台没有及时返回，导致Nginx直接返回给压测工具code 502。在linux平台下就比较稳定了。 3、通过Nginx代理访问，后端设置4台服务器，并发设置为1000 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 35.78 msec 4122.2 / sec 508.31 / sec 0.12 sec 156715 page 0.48 sec 74.59 msec 4109.8 / sec 506.99 / sec 0.28 sec 156275 request 0.48 sec 74.59 msec 4109.8 / sec 506.99 / sec 0.28 sec 156275 Code Highest Rate Mean Rate Total number 200 4115.6 / sec 507.68 / sec 156481 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.21 sec 36.22 msec 4786.7 / sec 533.37 / sec 0.12 sec 163256 page 0.50 sec 73.71 msec 4784.9 / sec 532.57 / sec 0.29 sec 162988 request 0.50 sec 73.71 msec 4784.9 / sec 532.57 / sec 0.29 sec 162988 Code Highest Rate Mean Rate Total number 200 4792.2 / sec 533.28 / sec 163199 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 说明 由于后台扩充到了4台服务器，通过Nginx进行轮询访问，分散了压力。在1000的并发下，windows和suse平台表现不相上下，均无错误。 4、通过Nginx代理访问，后端设置4台服务器，并发设置为1500 windows Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.19 sec 32.09 msec 2576 / sec 1075.47 / sec 0.10 sec 327195 page 0.50 sec 71.39 msec 2602.4 / sec 1073.57 / sec 0.31 sec 326577 request 0.50 sec 71.39 msec 2602.4 / sec 1073.57 / sec 0.31 sec 326577 Code Highest Rate Mean Rate Total number 200 2502.1 / sec 1001.18 / sec 304862 502 770.7 / sec 84.82 / sec 22052 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 0.20 sec 33.41 msec 3275.7 / sec 990.22 / sec 0.11 sec 299354 page 0.48 sec 68.46 msec 3270.3 / sec 988.43 / sec 0.27 sec 298777 request 0.48 sec 68.46 msec 3270.3 / sec 988.43 / sec 0.27 sec 298777 Code Highest Rate Mean Rate Total number 200 3261.5 / sec 989.09 / sec 298975 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 说明 当并发增加到1500时，windows平台出现code 502，后端服务器出现瓶颈。linux平台表现稳定。 5、通过Nginx代理访问，后端设置4台服务器，并发设置为2000 liunx Name highest 10sec mean lowest 10sec mean Highest Rate Mean Rate Mean Count connect 19.30 sec 36.06 msec 2187.66666666667 / sec 611.56 / sec 0.48 sec 166002 page 19.36 sec 0.14 sec 2197 / sec 611.42 / sec 0.57 sec 165828 request 19.36 sec 0.14 sec 2197 / sec 611.42 / sec 0.57 sec 165828 Code Highest Rate Mean Rate Total number 200 2194.66666666667 / sec 611.54 / sec 172473 Name Highest Rate Total number error_abort 0.333333333333333 / sec 1 error_connect_etimedout 17.1 / sec 318 error_next_session 0.285714285714286 / sec 2 说明 在2000并发先，linux平台后端依旧稳定返回code 200，但是Nginx会直接返回error，瓶颈出现在Nginx，需要调整相关配置了。 结论这次测试中可以发现，当仅返回字符串Hello World时，无论是windows平台还是liunx平台，在并发500的情况下虽然框架可以稳定输出，但是会出现不同程度的系统处理不过来直接拒绝请求的情况。 通过增加Nginx，可以有效的为后端提供缓冲，同样500的并发下，liunx平台返回给Nginx的错误code 504要明显比windows平台code 502少很多。 通过增加后台服务器，使用Nginx进行轮询，可以增加并发，在后端4台服务器，1000的并发下，linux平台和windows平台表现不相上下。但并发增加到1500之后，windows平台开始出现大量的code 502错误，linux平台依旧稳定。把并发继续增加到2000，Nginx端出现瓶颈，返回连接错误，后端linux保持稳定。可考虑下通过调整Nginx配置或者增加Nginx来继续提升并发效果。 不过，从上述测试情况来看，torando框架还是很稳定的，不至于并高并发弄到崩溃的程度。]]></content>
      <categories>
        <category>中间件</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>SuSE</tag>
        <tag>windows</tag>
        <tag>高并发</tag>
        <tag>torando</tag>
        <tag>Nginx</tag>
        <tag>tsung</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SuSE缺失devel包的问题]]></title>
    <url>%2FSuSE%E7%BC%BA%E5%A4%B1devel%E5%8C%85%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景最近几天计划将原Python项目迁移到Liunx服务器上，操作系统是SuSE 12 SP3。原以为Python项目迁移会比较方面，使用pip安装requirements包就好了，结果遇到不少问题。 问题 安装mysqlclient包时，出现了如下报错： 12345678910creating build/temp.linux-x86_64-2.7gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -Dversion_info=(1,3,9,'final',1) -D__version__=1.3.9 -I/usr/include/mysql -I/usr/local/python/include/python2.7 -c _mysql.c -o build/temp.linux-x86_64-2.7/_mysql.o -m64 _mysql.c:29:23: fatal error: my_config.h: No such file or directory #include "my_config.h" ^ compilation terminated. error: command 'gcc' failed with exit status 1 ----------------------------------------Command "/home/sysop/webapp/hzinfo/venv/bin/python -u -c "import setuptools, tokenize;__file__='/tmp/pip-install-4FRt1w/mysqlclient/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))" install --record /tmp/pip-record-AgHBn8/install-record.txt --single-version-externally-managed --compile --install-headers /home/sysop/webapp/hzinfo/venv/include/site/python2.7/mysqlclient" failed with error code 1 in /tmp/pip-install-4FRt1w/mysqlclient/ 安装ldap库时，出现了如下报错： 123456In file included from Modules/LDAPObject.c:9:0:Modules/errors.h:8:18: fatal error: lber.h: No such file or directory #include "lber.h" ^compilation terminated.error: command 'gcc' failed with exit status 1 思路这些错乍看不一样，其实差不多，都是缺失了一些东西，导致安装失败。针对每个问题逐个谷歌百度就会发现，大家会告诉你需要安装对应的开发库及xxx-dev(el)包。 比如上面第一个问题里，安装mysqlclient包报错了，需要安装对应的dev包，在SuSE中是libmysqlclient-devel。 xxx与xxx-dev(el)的关系经过这两天的查询和总结，再Liunx中，一般会把软件拆分为两部分，一部分是直接使用的库即xxx，另一部分就是开发用的库，包含一些头文件之类的，就是xx-dev(el)。 差不多可以这样理解：当你只是使用某个软件的时候，你只要安装xxx即可，但当你需要二次开发或者使用对应的一些插件的时候，很可能你就会需要再安装xxx-dev(el)了。 https://blog.csdn.net/wangeen/article/details/14522227 解决办法知道了问题的原因，那么解决办法就简单了。但是对于SuSE，尤其是没有外网的SuSE就不是了。 SuSE是收费的系统，没办法直接下载到对应的rpm安装包，需要挂载对应系统版本的SDK光盘。而问题是不断暴露和修复的，我们需要的dev包可能在最初获取的SDK光盘里不存在。查看了SuSE官网，有些补丁也是建议通过网络更新的，或者直接下载更新SDK安装盘。 https://www.suse.com/zh-cn/documentation/sles-12/book_sle_deployment/data/sec_add-ons_sdk.html 不过官网下载是相当的慢。。。 zypper的使用zypper是SuSE的当我们有了对应的sdk光盘或者目录之后，可以通过nfs的方式挂载到zypper源中。 添加源 123zypper ar -t yast2 -n 'sles12sp3_sdk1' -fc nfs://122.16.125.112/iso/sles12sp3_sdk1 sles12sp3_sdk1zypper ar -t yast2 -n 'sles12sp3_sdk2' -fc nfs://122.64.29.85/approot1/sles12sp3_sdk sles12sp3_sdk2zypper ar -t yast2 -n 'sles12sp3_server' -fc fs://122.64.29.85/approot1/sles12sp3_server sles12sp3_server 搜索需要的安装包 当遇到上述问题中的安装失败，使用如下的命令搜索下有哪些包可以安装： 1zypper se mysql 显示结果如下： 安装对应的包 这里可以看到libmysqlclient-devel没有安装（前面有i标记的即为已经安装），安装即可。 1~ # zypper install libmysqlclient-devel 搜索安装其他devel包 同理，在第二个问题中，我们搜索ldap，就会发现可以libldapcpp-devel包没有安装。 装好之后继续安装python-ldap还会遇到一个错误： 12345Modules/LDAPObject.c:18:18: fatal error: sasl.h: No such file or directory #include &lt;sasl.h&gt; ^compilation terminated.error: command 'gcc' failed with exit status 1 同理，再搜一下： 发现cyrus-sasl-devel包没装，也把装上，问题就解决了。 注意python也是有python-devel包的，这个不要忘了。]]></content>
      <categories>
        <category>操作系统</category>
        <category>SuSE</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>MySQL</tag>
        <tag>SuSE</tag>
        <tag>zypper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL的安装]]></title>
    <url>%2FMySQL%E7%9A%84%E5%AE%89%E8%A3%85.html</url>
    <content type="text"><![CDATA[背景每次重搭环境都免不了要重新安装数据库，虽然频率不高，也发现竟然在3个平台上都装过了，记录一下。 Windows安装MySQL 下载MySql：http://dev.mysql.com/downloads/mysql/ 解压后放到安装目录 在环境变量中将mysql安装目录定义为%MYSQL_HOME%，并将bin目录加入环境变量（%MYSQL_HOME%\bin） 在mysql-5.6.24-winx64的根目录下，找到my-default.ini文件，改名为my.ini 。打开，添加如下信息： 123456789[mysqld]loose-default-character-set = utf8character-set-server = utf8basedir = D:\mysql-5.6.24-winx64 #写自己的mysql路径哦datadir = D:\mysql-5.6.24-winx64\data #写自己的mysql路径哦！[client]loose-default-character-set = utf8[WinMySQLadmin]Server = D:\mysql-5.6.24-winx64\bin\mysqld.exe # 写自己的mysql路径哟！ 以管理员身份运行cmd，进入到%MYSQL_HOME%到bin目录下运行如下命令： 12mysqld --initialize -insecure # 初始化mysql，创建root用户，密码为空mysqld -install 最后提示：Service successfully in installed! 启动mysql。 1net start mysql 停止mysql 1net stop mysql 首次登陆，无需密码 1mysql -u root 修改root密码 1mysql&gt; set password for root@localhost = password(&apos;root&apos;); 再次登陆 1mysql -u root -p 会提示输入密码，输入后已root用户进入。 其他命令 停止mysql 1net stop mysql 卸载服务 1mysqld -remove MAC下安装Mysql 下载Mac版的DMG Archive包。（不建议用brew安装，后续配置很麻烦） 后双击安装，一路下一步 会出现一个提醒，给了个默认root@localhost账号的密码。比如：root@localhost: wuKgf_mCK38z 安装完成后，在系统偏好设置中找到MySQL图标，点击进入，手动启动MySQL服务。 通过alias绑定命令：在命令行中运行如下命令，绑定mysql 12MacBook-Air:~ icbc$ alias mysql=/usr/local/mysql/bin/mysqlMacBook-Air:~ icbc$ alias mysqladmin=/usr/local/mysql/bin/mysqladmin ​ 注意：这种方式只能在当前命令行中有效。 建议是在环境变量中增加： 123456cd ~touch .bash_profilevi .bash_profile# 加入如下语句export PATH=$&#123;PATH&#125;:/usr/local/mysql/bin 进行root密码重置，比如重置为root，运行如下命令，然后输入临时密码即可。 12MacBook-Air:~ icbc$ mysqladmin -u root -p password rootEnter password: Suse下离线安装MySQL 先下载Suse版本的RPM包，官网有很多，要下载Bundle版，否则得分别下载各个组件。 SUSE Linux Enterprise Server 12 (x86, 64-bit), RPM Bundle 解压安装包 12345HzTomcat:/data # tar -xvf mysql-8.0.13-1.sles12.x86_64.rpm-bundle.tarHzTomcat:/data # cd mysql-8.0.13-1.sles12.x86_64/HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # lsmysql-community-client-8.0.13-1.sles12.x86_64.rpm mysql-community-devel-8.0.13-1.sles12.x86_64.rpm mysql-community-server-8.0.13-1.sles12.x86_64.rpmmysql-community-common-8.0.13-1.sles12.x86_64.rpm mysql-community-libs-8.0.13-1.sles12.x86_64.rpm mysql-community-test-8.0.13-1.sles12.x86_64.rpm 下面的安装要注意按顺序，先安装mysql-community-common-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-common-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-common-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-common-8.0.13-1.s################################# [100%] 再安装mysql-community-libs-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-libs-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-libs-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-libs-8.0.13-1.sle################################# [100%] 再安装mysql-community-client-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-client-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-client-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-client-8.0.13-1.s################################# [100%] 最后安装mysql-community-server-8.0.13-1.sles12.x86_64.rpm 12345HzTomcat:/data/mysql-8.0.13-1.sles12.x86_64 # rpm -ivh mysql-community-server-8.0.13-1.sles12.x86_64.rpm warning: mysql-community-server-8.0.13-1.sles12.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing... 1:mysql-community-server-8.0.13-1.s################################# [100%] 启动 1HzTomcat:~ # service mysql start 查看临时root密码 12HzTomcat:~ # grep 'temporary password' /var/log/mysql/mysqld.log 2019-01-09T07:16:31.105387Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: !kwpeD_Pc7/p 修改root密码 12345678910111213141516171819202122HzTomcat:~ # mysql -uroot -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 8Server version: 8.0.13Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; alter user 'root'@'localhost' IDENTIFIED BY 'Password123!';Query OK, 0 rows affected (0.14 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.07 sec)mysql&gt; exitBye 可能出现的问题问题在修改root密码时，忘了flush privileges，然后导致新旧密码都无法登陆的问题。 123HzTomcat:~ # mysql -uroot -pEnter password: ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES) 解决方法 修改mysql的配置文件： 1HzTomcat:~ # vim /etc/my.cnf 增加：skip-grant-tables 然后在登陆直接使用root登陆，重新设置密码，即可。 12345678910111213141516171819202122232425262728293031HzTomcat:/var/lib/mysql # mysql -uroot Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 8.0.13 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; flush privileges;Query OK, 0 rows affected (0.02 sec)mysql&gt; alter user &apos;root&apos;@&apos;localhost&apos; identified by &apos;Password123!&apos;;Query OK, 0 rows affected (0.07 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.07 sec)mysql&gt; exitByeHzTomcat:/var/lib/mysql # vim /etc/my.cnfHzTomcat:/var/lib/mysql # service mysql restartHzTomcat:/var/lib/mysql # mysql -uroot -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 8Server version: 8.0.13 MySQL Community Server - GPL]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask - sqlalchemy.orm.exc.DetachedInstanceError]]></title>
    <url>%2Fflask-sqlalchemy-orm-exc-DetachedInstanceError.html</url>
    <content type="text"><![CDATA[背景在一个基于Flask的项目中，使用到flask-sqlachemy的数据库orm工具。在一次数据插入之后再次查询数据时出现了如下的错误： 1DetachedInstanceError: Instance &lt;User at 0x7f2f54fc8750&gt; is not bound to a Session; attribute refresh operation cannot proceed 原因经查，由于之前开发的时候，参考《Flask Web开发 基于Python的Web应用开发实战》这本书时，作者建议在配置flask-sqlachemy时，加入如下配置： 1SQLALCHEMY_COMMIT_ON_TEARDOWN = Ture 这个配置是用来涉及在db操作时，自动提交的。以下两种情况是等效的。123456# SQLALCHEMY_COMMIT_ON_TEARDOWN = True 时db.add(User)# SQLALCHEMY_COMMIT_ON_TEARDOWN = False 时db.add(User)db.commit() 但其实自动提交时，系统会一并删除当前数据库的session，所以导致了上面出现的问题。目前flask-sqlachemy官方也认为这个设置可能存在问题，已经在文档中移除了。 http://flask-sqlalchemy.pocoo.org/2.3/changelog/ 结论那么建议的方法就是删除这条配置，手动提交了。在每次进行数据库新增、修改、删除时，手动的commit()。]]></content>
      <categories>
        <category>python</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>sqlachemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Navcat连MySQL提示Autentication错误]]></title>
    <url>%2FNavicat%E8%BF%9EMySQL%E6%8F%90%E7%A4%BAAutentication%E9%94%99%E8%AF%AF.html</url>
    <content type="text"><![CDATA[背景Mysql版本：8.0.13 Navicat版本：Navicat Premium Version 12 问题在MySQL中创建好用户，数据库，并将数据库的权限授权给用户之后 1234567891011mysql&gt; create user 'flask'@'%' identified by 'xxxx';Query OK, 0 rows affected (0.01 sec)mysql&gt; create database FlaskDB default charset utf8 collate utf8_general_ci;Query OK, 1 row affected (0.00 sec)mysql&gt; grant all privileges on FlaskDB.* to "flask"@"%"; Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 然后通过Navicat使用连接数据库，出现了下面的报错： 结论这个问题百度一下有很多帖子，是由于新版的MySQL8更新了认证方式，而其他的数据库软件没有跟上。大部分忒子会推荐我们重新配置MySQL，不启用新的认证方式，甚至有些建议删了重装。 个人还是推荐下面这种方式： 1ALTER USER 'username'@'%' IDENTIFIED WITH mysql_native_password BY 'password'; 不重新配置MySQL，暂时先指定使用旧版本的认证方式来设置密码。待后续软件更新了自然就可以使用新版本功能了。 https://stackoverflow.com/questions/49194719/authentication-plugin-caching-sha2-password-cannot-be-loaded]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
        <tag>navicat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle wm_concat实现字符串分割替换]]></title>
    <url>%2Foracle-wm-concat%E5%AE%9E%E7%8E%B0%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%88%86%E5%89%B2%E6%9B%BF%E6%8D%A2.html</url>
    <content type="text"><![CDATA[背景在数据库中有一个存储有权限用户的字段，通过,分割来存储了多个用户的账号。可以通过用户账号在用户表中查询出用户的姓名，现在需要将这个字段的账号转换成用户姓名在前台展示。 用户表user： id name 0001 小明 0002 小刚 需要处理的字段：0001,0002。 要求的返回结果：小明,小刚。 思路 首先我们需要把user表中id字段存在在处理字段中的值给查出来。 最开始考虑使用instr函数： 1select * from user where instr('0001,0002', id)&gt;0 instr()函数可以判断值在字符串中的位置，如果大于0，也就是存在。但是这是模糊匹配的，会有问题。比如上面场景中，如果有用户id是000那么也会匹配出来。 所以使用regexp_like()，通过正则来判断。 这里的正则表达如为：^(0001|0002)$。使用^确定开通，$确认结尾，|用来分割允许的值。对应到实际的就变成如下： 1select name from user where regexp_like(id, '^(' || replace('0001,0002', ',', '|') || ')$') 注意：很多正则判断工具里^0001|0002$也可以实现效果，但oracle中，必须加上()，否则oracle会认为是^0001 和0002$，0001001这样的也会被识别到。 输出结果： name 小明 小刚 然后可以使用wm_concat函数把连起来即可。 wm_concat是一个未被记录的函数，但是可以实现将列通过,连成一个字段的作用。 1select to_char(wmsys.wm_concat(name)) from user where id = '0001' or id = '0002' https://community.oracle.com/thread/1090158 结论连起来结果如下： 1select to_char(wmsys.wm_concat(name)) name from user where regexp_like(id, '^'|| replace('0001,0002', ',', '|')||'$') 输出结果： name 小明,小刚 注意由于wm_concat未被官方记录，不同版本的oracle有区别，如果输出是个clob类型，那么可以使用to_char()来转换。也可以考虑使用listagg()来实现。]]></content>
      <categories>
        <category>数据库</category>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>oracle</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle regexp_substr函数实现字符串split]]></title>
    <url>%2Foracle-regexp-substr%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E5%AD%97%E7%AC%A6%E4%B8%B2split.html</url>
    <content type="text"><![CDATA[背景数据库中有一个owner字段，里面存了多个有权限用户的账号，通过,分割。现前台返回一个用户账号，需要判断该用户是否有权限。首先要把这个字段进行分割成列，然后就可以判断是否存在。 思路 首先oracle是可以使用regexp_substr实现按照分隔符获取元素的。 1234567REGEXP_SUBSTR(source_char, pattern [, position [, occurrence [, match_parameter ] ] ] ) 其中： source_char是需要分割的字符串。 pattern为正则表达。 position是起始位置，默认为1。 occurrence是表示第几个匹配组，默认为1。 match_parameter是匹配参数： i：大小写不敏感 c：大小写敏感 n：允许(.)匹配换行符 m：oracle将把字符串当做多行字符处理，^和$是起始和结束 x：忽略空格 https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions131.htm 于是我们通过下面的表达式获取分割后的某个值： 1select regexp_substr('1,2,3', '[^,]+', 1, 2) from dual; --获取第二个数 结果如下： 现在我们需要把其中的occurrence变成一个和分割后等长的列即可。 获取分割后元素的个数 使用length()可以获取字符串的个数，而字符串是通过,分割的。使用replece()可以替换删除所有的分割符。分割后的字符串个数即是两者的差加1。 1select length('1,2,3') - length(replace('1,2,3',',')) + 1 from dual; 结果如下： 获取一个空列 可以通过level加上connect by来获取一个空列： 1select level from dual connect by level &lt;= 5; 结果如下： 把上述连起来就可以获取分割后的列了。 结论虽然有点长，但是总体思路还是比较清晰的。代码如下： 1select regexp_substr('1,2,3','[^,]+' , 1, level) from dual connect by level &lt;= (select length('1,2,3') - length(replace('1,2,3',',')) + 1 from dual); 结果如下： 最后我们需要判断某个人是否有权限，即某人的账号是否在我们获取的分割后的列里面。直接用count(1)是否大于0即可。 12345select count(1) from (select regexp_substr('1,2,3','[^,]+' , 1, level) as owner from dual connect by level &lt;= (select length('1,2,3') - length(replace('1,2,3',',')) + 1 from dual) twhere t.owner = i_owner]]></content>
      <categories>
        <category>数据库</category>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac常用装机及开发软件分享]]></title>
    <url>%2FMac%E5%B8%B8%E7%94%A8%E8%A3%85%E6%9C%BA%E5%8F%8A%E5%BC%80%E5%8F%91%E8%BD%AF%E4%BB%B6%E5%88%86%E4%BA%AB.html</url>
    <content type="text"><![CDATA[其实软件也不多，主要几个专业软件和开发软件要收费，经常会有一些失效的破解和假的破解版。亲测可用，在这里记录和分享，以备万一。 办公软件 Office2019 + 激活：链接:https://pan.baidu.com/s/1YgGS6AI-XdI5vCcyfPA8OQ 密码:f2dl 专业软件 Photoshop CC2018 + 激活：链接:https://pan.baidu.com/s/1ZdPU1GkkOmYYfgvtLOwpBA 密码:zv6u Auto CAD 2018 + 激活：链接:https://pan.baidu.com/s/1IYWly6SJuGvh8l0LI33hcg 密码:2tpg typora（markdown编辑器）：https://www.typora.io/ 开发软件 PyCharm：http://www.jetbrains.com/pycharm/download/#section=mac idea：https://www.jetbrains.com/idea/download/#section=mac WebStrom：https://www.jetbrains.com/webstorm/download/download-thanks.html Navcat Premium：链接:https://pan.baidu.com/s/1egvUoYnoi21980vo38vQUw 密码:7zn6 SubLime：http://www.sublimetext.com/3 其他 Parallel Desktop（虚拟机）+ 激活：链接:https://pan.baidu.com/s/1dPfTfCDVpzXMrivbt9OUqg 密码:ztns Win10官方镜像：链接:https://pan.baidu.com/s/11iOX061WNDlt9oSpNJh6lw 密码:xn7r Win10 激活工具：链接:https://pan.baidu.com/s/1FNbLuXKG5TPW9y_hvr5YmA 密码:jn9b Keka118（压缩软件）：链接:https://pan.baidu.com/s/16Yjo44uRRPk0aZY6ugZ4qg 密码:cli2]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用node-schedule实现node后台定时任务]]></title>
    <url>%2F%E7%94%A8node-schedule%E5%AE%9E%E7%8E%B0node%E5%90%8E%E5%8F%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.html</url>
    <content type="text"><![CDATA[背景最近在自己的小程序中增加了多币种支持涉及到汇率的更新，于是需要在后台服务端设置一个定时任务来自动通过响应的接口更新最新的汇率。获取汇率的接口目前通过聚合数据提供的免费接口实现（调到需要实名注册）。 node-schedule由于小程序后端服务器是基于node，查一下，果然是有对应的npm包的——node-schedule。 node-schedule可以使用多种方式定义定时任务的，一般使用类似liunx的cron方式就可以满足绝大部分需求了。cron的定义方式参考如下表： 123456789* * * * * *┬ ┬ ┬ ┬ ┬ ┬│ │ │ │ │ ││ │ │ │ │ └ 星期 day of week (0 - 7) (0 or 7 is Sun)│ │ │ │ └───── 月 month (1 - 12)│ │ │ └────────── 日期 day of month (1 - 31)│ │ └─────────────── 小时 hour (0 - 23)│ └──────────────────── 分钟 minute (0 - 59)└───────────────────────── 秒 second (0 - 59, OPTIONAL) 如果你熟悉linux系统的crontab定时任务的话，那就相当简单了。 它还支持基于日期的定时任务以及基于rule的定时任务。详细可以见一下官方说明。 https://www.npmjs.com/package/node-schedule 实现方法首先需要安装node-schedule包，并保存在项目的package.json中。 1cnpm install node-schedule --save 我的需求是每三十分钟自动更新一次数据，那么通过cron的方式就可以定义为如下规则： 10 */30 * * * * 其中*/30是指可以被30整除的，也就是0分和30分的时候。规则确定了，那么代码就很简单了： 12345678910const schedule = require('node-schedule')function scheduleCron () &#123; schedule.scheduleJob('0 */30 * * * *', function () &#123; console.log('scheduleCron ' + new Date()) // do something &#125;)&#125;scheduleCron() 输出结果： 12scheduleCron Wed Jan 02 2019 00:00:00 GMT+0800 (CST)scheduleCron Wed Jan 02 2019 00:30:00 GMT+0800 (CST) 注意！根据官方说明node-schedule是基本上支持所有的cron表达式，除了一下几个： W：最近的工作日，放在日期(day of month)字段，比如15W指到本月15日最近的工作日。 L：表示最后，放在星期(day of month)或者星期(day of week)字段。 #：表示每月的周几，放在星期(day of week)字段。 cron拓展配置规则时几个常用的符号： *：表示匹配任意值。 /：x/y表示等步长序列，可以理解为从x开始，每y个单位执行一次。其中*/5与0/5是等效的，都是指每5分钟执行一次。 ,：表示序列的分割，比如3,4指在3和4的时候执行。 -：表示一个范围，比如3-5指在3到5的时候执行。 ?：仅用在日期和星期字段，表示任意的值，相当于占位。]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序腾讯云环境安装依赖错误]]></title>
    <url>%2F%E5%B0%8F%E7%A8%8B%E5%BA%8F%E8%85%BE%E8%AE%AF%E4%BA%91%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E9%94%99%E8%AF%AF.html</url>
    <content type="text"><![CDATA[背景开发工具：微信开发者工具 操作系统：Mac 腾讯云环境：Node 问题最近使用node-sechedule开发了一个后台定时任务，这个是新增的node模块，所以需要安装依赖，即在package.json中增加对应的依赖： 1234"dependencies": &#123; ... "node-schedule": "^1.3.1" &#125;, 在本地测试通过之后，点击开发工具上面的“腾讯云”-“上传测试代码” 并勾选安装依赖。问题就出现了！！！ 排查及原因 感觉这是权限问题啊，于是切换了小程序的管理员用户，点击上传，情况一样。 无奈只能先恢复开发环境了，然后再次上传。根据文档推荐，首次上传最好使用模块上传，全部勾选，结果情况还是一样。 看了node_modules文件夹中，的确有node-schedule文件夹啊，不过有一点奇怪的是，安装每个安装的npm包都有一个_开头的文件夹： 看来得找专家了，于是在腾讯云控制台上提交了一个工单，过了半个小时的样子就有工程师电话来了。他首先在自己的环境里试了一下我的package.json包，竟然没有问题。。。 工程师登录我的后台服务器看了下： 感觉可能是因为上面这种奇怪的包文件夹造成的。于是乎，再恢复一次环境，然后我再次上传，这次不再勾选“上传node_modules代码”。 这样也即是让后台服务器自动安装所有的依赖。最后，成功了！ 这时候再看一下后台服务器上目录： 没有了那种_开头的目录，就成功了。 插曲有了上面的经验，本以为上传到生产环境就妥妥的了。结果点击“上传正式代码”之后，坑爹了。 正式代码上传是没有上面的这种选项的，本机上的那些_开头的node_modules文件也上传了，再一次入坑。 于是乎，只能再次联系腾讯云的工程师，删除了我工程的node_modules文件夹，然后点击控制台上的安装依赖。待依赖安装完成后，再次部署代码。 这时候后端启动已经没有问题，但是小程序竟然无法直接访问，最后发现是由于小程序解决方案里本来是自带ssl证书的，这个证书最近到期了，需要自己购买新的证书（选免费的）联系后台更新。 至此问题彻底解决。 结论可以看出，主要问题还是在于node_modules里文件的问题。而这个的罪魁祸首是——cnpm。 可以参考如下的issue，cnpm安装包时采用link的方式，与npm不一样。 https://github.com/cnpm/cnpmjs.org/issues/1000 在小程序上传之后自动安装包，则用的是npm安装，从而导致问题出现。以后在没弄明白之前还是先用原生的吧，在别人给你便利的时候，也要明白其中有什么道理。 另外，建议自己在本地开发的时候，server目录下还是把node_modules给删除了，以免后续一不小心又上传了什么问题文件。]]></content>
      <categories>
        <category>微信小程序</category>
      </categories>
      <tags>
        <tag>小程序</tag>
        <tag>node</tag>
        <tag>javascript</tag>
        <tag>腾讯云</tag>
        <tag>微信开发者工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Gulp实现Hexo网页压缩优化]]></title>
    <url>%2F%E7%94%A8Gulp%E5%AE%9E%E7%8E%B0Hexo%E7%BD%91%E9%A1%B5%E5%8E%8B%E7%BC%A9%E4%BC%98%E5%8C%96.html</url>
    <content type="text"><![CDATA[背景经过一段时间的折腾，也算是把这个Hexo的个人博客搭建起来了，换主题，加插件，文章里加图片是什么的，就发现网站有时候会有点慢，于是开始考虑做SEO以及一些优化工作，于是乎发现了Gulp这个神器。 Gulp是什么Hexo生成的静态网页其实是可读性比较好的，会有大量的空格、换行什么的，而实际浏览器解析式完全不需要的。如果把这些空格、换行全部删掉，就会节省很多空间出来，于是网站的响应速度也就变快了。 而Gulp是一种基于node的自动化构建工具，至于自动化构建这个我们目前不需要纠结，我们只要知道它有一些插件可以帮助我们自动化的对hexo生成的各种文件进行压缩。 Gulp怎么用 首先我们要在全局安装下gulp和我们要用到的插件 12npm install gulp -gnpm install gulp gulp-uglify gulp-minify-css gulp-imagemin gulp-htmlmin gulp-htmlclean gulp-concat --save 在hexo的根目录创建一个gulpfile.js文件 先放代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162var gulp = require('gulp'), uglify = require('gulp-uglify'), cssmin = require('gulp-minify-css'), imagemin = require('gulp-imagemin'), htmlmin = require('gulp-htmlmin'), htmlclean = require('gulp-htmlclean'); concat = require('gulp-concat');//JS压缩gulp.task('uglify', function() &#123; return gulp.src(['./public/js/**/.js','!./public/js/**/*min.js'])//只是排除min.js文件还是不严谨，一般不会有问题，根据自己博客的修改我的修改为return gulp.src(['./public/**/*.js','!./public/zuoxi/**/*.js',,'!./public/radio/**/*.js']) .pipe(uglify()) .pipe(gulp.dest('./public/js'));//对应修改为./public即可&#125;);//public-fancybox-js压缩gulp.task('fancybox:js', function() &#123; return gulp.src('./public/vendors/fancybox/source/jquery.fancybox.js') .pipe(uglify()) .pipe(gulp.dest('./public/vendors/fancybox/source/'));&#125;);// 合并 JSgulp.task('jsall', function () &#123; return gulp.src('./public/**/*.js') // 压缩后重命名 .pipe(concat('app.js')) .pipe(gulp.dest('./public'));&#125;);//public-fancybox-css压缩gulp.task('fancybox:css', function() &#123; return gulp.src('./public/vendors/fancybox/source/jquery.fancybox.css') .pipe(cssmin()) .pipe(gulp.dest('./public/vendors/fancybox/source/'));&#125;);//CSS压缩gulp.task('cssmin', function() &#123; return gulp.src(['./public/css/main.css','!./public/css/*min.css']) .pipe(cssmin()) .pipe(gulp.dest('./public/css/'));&#125;);//图片压缩gulp.task('images', function() &#123; gulp.src('./public/uploads/*.*') .pipe(imagemin(&#123; progressive: false &#125;)) .pipe(gulp.dest('./public/uploads/'));&#125;);// 压缩 public 目录 html文件 public/**/*.hmtl 表示public下所有文件夹中html，包括当前目录gulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// gulp.task('default', gulp.series('uglify', 'cssmin', 'fancybox:js', 'fancybox:css', 'jsall','images'));gulp.task('default', gulp.series('uglify', 'cssmin', 'jsall', 'minify-html'));//, 'minify-html' 这里要注意的是，默认安装的是gulp 4.0.0，而网上很多例子是基于gulp 3的，所以运行起来 会有如下的报错： 123456789101112131415assert.js:351 throw err; ^AssertionError [ERR_ASSERTION]: Task function must be specified at Gulp.set [as _setTask] (/Users/zhengk/Desktop/hexo/blog/node_modules/_undertaker@1.2.0@undertaker/lib/set-task.js:10:3) at Gulp.task (/Users/zhengk/Desktop/hexo/blog/node_modules/_undertaker@1.2.0@undertaker/lib/task.js:13:8) at Object.&lt;anonymous&gt; (/Users/zhengk/Desktop/hexo/blog/gulpfile.js:59:6) at Module._compile (internal/modules/cjs/loader.js:721:30) at Object.Module._extensions..js (internal/modules/cjs/loader.js:732:10) at Module.load (internal/modules/cjs/loader.js:620:32) at tryModuleLoad (internal/modules/cjs/loader.js:560:12) at Function.Module._load (internal/modules/cjs/loader.js:552:3) at Module.require (internal/modules/cjs/loader.js:657:17) at require (internal/modules/cjs/helpers.js:22:18) 这是由于gulp4中需要使用gulp.series 和 gulp.parallel来指定运行的任务。具体见上面gulpfile.js中的最后一行。 1gulp.task('default', gulp.series('uglify', 'cssmin', 'jsall', 'minify-html')); https://blog.csdn.net/qq_31975963/article/details/83034450 这一行是写明gulp需要执行的任务，然后需要注意的是，当其中某个任务失败或者没有东西需要压缩的时候，比如你没有用到fancybox却要执行fancybox:js任务，就会有如下的报错： 1234567891011121314151617181920[23:59:25] Using gulpfile ~/Desktop/hexo/blog/gulpfile.js[23:59:25] Starting 'default'...[23:59:25] Starting 'uglify'...[23:59:25] Finished 'uglify' after 24 ms[23:59:25] Starting 'cssmin'...[23:59:26] Finished 'cssmin' after 215 ms[23:59:26] Starting 'fancybox:js'...[23:59:26] 'fancybox:js' errored after 2.96 ms[23:59:26] Error: File not found with singular glob: /Users/zhengk/Desktop/hexo/blog/public/vendors/fancybox/source/jquery.fancybox.js (if this was purposeful, use `allowEmpty` option) at Glob.&lt;anonymous&gt; (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob-stream@6.1.0@glob-stream/readable.js:84:17) at Object.onceWrapper (events.js:277:13) at Glob.emit (events.js:189:13) at Glob.EventEmitter.emit (domain.js:441:20) at Glob._finish (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:197:8) at done (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:182:14) at Glob._processSimple2 (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:688:12) at /Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:676:10 at Glob._stat2 (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:772:12) at lstatcb_ (/Users/zhengk/Desktop/hexo/blog/node_modules/_glob@7.1.3@glob/glob.js:764:12)[23:59:26] 'default' errored after 246 ms 只要把对应的任务删掉就好了。 运行gult 先执行hexo g来生成静态网页，然后我们看下public文件夹下的静态文件大小： 1234$ls -lh...-rw-r--r-- 1 zhengk staff 57K 12 30 00:07 index.html... 然后我们再执行下gulp（执行时默认执行default任务，所以前面gulpfile.js中设置任务为default）对比下效果： 1234567891011121314151617$gulp[00:53:29] Working directory changed to ~/Desktop/hexo/blog[00:53:30] Using gulpfile ~/Desktop/hexo/blog/gulpfile.js[00:53:30] Starting 'default'...[00:53:30] Starting 'uglify'...[00:53:30] Finished 'uglify' after 23 ms[00:53:30] Starting 'cssmin'...[00:53:30] Finished 'cssmin' after 216 ms[00:53:30] Starting 'jsall'...[00:53:30] Finished 'jsall' after 56 ms[00:53:30] Starting 'minify-html'...[00:53:31] Finished 'minify-html' after 1.16 s[00:53:31] Finished 'default' after 1.46 s$ls -lh...-rw-r--r-- 1 zhengk staff 27K 12 30 00:07 hello-world.html... 可以发现足足小了30k，压缩了近一半大小。]]></content>
      <categories>
        <category>node</category>
      </categories>
      <tags>
        <tag>node</tag>
        <tag>hexo</tag>
        <tag>gulp</tag>
        <tag>seo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript中的Json、Map、Set]]></title>
    <url>%2FJavaScript%E4%B8%AD%E7%9A%84Json%E3%80%81Map%E3%80%81Set.html</url>
    <content type="text"><![CDATA[问题之前在一个项目中，需要根据申请的部门来获取对应的邮箱地址，想当然的使用了Map对象，结果在调试中完全没有问题，却在实际使用上失效了，查看了下后台log，提示获取到的邮箱地址是undefined。 排查及原因经过百度之后发现原来JS的Map对象的浏览器支持不好，虽然很多地方写IE11开始支持，但其实IE11是不支持new Map()这种方式新建的。 详见如下链接：https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Map#浏览器兼容 于是我把换成了用Json对象，果然就解决了。 所以尽量还是用Json吧。 总结Map对象JS的Map存放的是键值对，key值不允许重复。特别要注意的是在一些旧的浏览器中并不支持Map对象。 基本使用 12345678910111213141516171819202122// 新建Map对象&gt; m = new Map()Map &#123;&#125;// 新增Map键值对&gt; m.set('key1','value1')Map &#123; 'key1' =&gt; 'value1' &#125;// 获取Map中的key值&gt; m.get('key1')'value1'// 判断是否存在某key&gt; m.has('key1')true&gt; m.set('key2','value2')Map &#123; 'key1' =&gt; 'value1', 'key2' =&gt; 'value2' &#125;// 删除某键值对&gt; m.delete('key1')true&gt; mMap &#123; 'key2' =&gt; 'value2' &#125;// 清空Map对象&gt; m.clear()undefined 遍历 12345678910&gt; m.set('key1','value1')Map &#123; 'key1' =&gt; 'value1' &#125;&gt; m.set('key2','value2')Map &#123; 'key1' =&gt; 'value1', 'key2' =&gt; 'value2' &#125;&gt; m.forEach(function (value, key, map) &#123;... console.log(key + ":" + value)... &#125;)key1:value1key2:value2undefined Set对象Set对象可以理解为没有值的Map对象，一般用于存放一个不允许重复的列表 基本使用 123456789101112131415161718// 新建Set对象&gt; s = new Set()Set &#123;&#125;// 新增Set值&gt; s.add('key1')Set &#123; 'key1' &#125;&gt; s.add('key2')Set &#123; 'key1', 'key2' &#125;// 删除某值&gt; s.delete('key1')true&gt; sSet &#123; 'key2' &#125;// 清空Set对象&gt; s.clear()undefined&gt; sSet &#123;&#125; 遍历 Set对象的礼遍历和Map基本一样，但是由于Set对象没有value值，所以遍历的时候key和value是一样的。 12345678910&gt; s.add('key1')Set &#123; 'key1' &#125;&gt; s.add('key2')Set &#123; 'key1', 'key2' &#125;&gt; s.forEach(function (value, key, set) &#123;... console.log(key + ":" + value)... &#125;)key1:key1key2:key2undefined Json对象 基本使用 123456\\ 新建Json对象&gt; let currencyItems = &#123; '人民币': 1, '港币': 0.88, '澳门元': 0.86, '新台币': 0.2241, '美元': 6.905, '日元': 0.06, '英镑': 8.69, '欧元': 7.8, '韩元': 0.006, '泰铢': 0.21, '新西兰元': 4.69, '澳大利亚元': 4.96, '菲律宾比索': 0.13, '加拿大元': 5.16, '瑞士法郎': 6.92, '瑞典克朗': 0.76, '丹麦克朗': 1.05, '挪威克朗': 0.8 &#125;undefined\\ 获取Json对象某值&gt; currencyItems['人民币']1 获取Json对象的所有Key值 12345678910111213141516171819&gt; Object.keys(currencyItems)[ '人民币', '港币', '澳门元', '新台币', '美元', '日元', '英镑', '欧元', '韩元', '泰铢', '新西兰元', '澳大利亚元', '菲律宾比索', '加拿大元', '瑞士法郎', '瑞典克朗', '丹麦克朗', '挪威克朗' ] 获取Json对象的长度 12&gt; Object.keys(currencyItems).length18 遍历 12345678910111213141516171819202122&gt; for ( var i in currencyItems) &#123;... console.log(i + ":" + currencyItems[i])... &#125;人民币:1港币:0.88澳门元:0.86新台币:0.2241美元:6.905日元:0.06英镑:8.69欧元:7.8韩元:0.006泰铢:0.21新西兰元:4.69澳大利亚元:4.96菲律宾比索:0.13加拿大元:5.16瑞士法郎:6.92瑞典克朗:0.76丹麦克朗:1.05挪威克朗:0.8undefined]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>Json</tag>
        <tag>Map</tag>
        <tag>Set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows Server 2018 TCP 连接数限制问题]]></title>
    <url>%2FWindows-Server-2018-TCP-%E8%BF%9E%E6%8E%A5%E6%95%B0%E9%99%90%E5%88%B6%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[背景最近在查一个并发问题，在压测的时候，Nginx的error.log显示connect() failed(111: Connection refused)。而后端应用并未手工设置过拒绝连接。于是怀疑是在高并发的情况下，windows服务器可能存在自行拒绝连接的情况。 排查过程首先打开windows服务器上的 任务管理器 - 性能 - 资源监控器。TCP连接这儿显示总数为100。 然后开启压测，TCP连接开始飙升，然后问题出现了。 TCP连接满了，怎么就变成10了！不过瓶颈应该就是这儿了！ 结论经过各种百度，谷歌，发现我好像被误导了。 微软官方说从Windows Vista，Window server 2008 SP2 起，不在限制half-open TCP connections，也就是理论上不再有连接数的限制。 官方说明见这个地址：https://support.microsoft.com/zh-cn/help/969710/how-to-enable-the-half-open-tcp-connections-limit-in-windows-vista-wit 然后根据国外有个问答网站的结论，这个“10”，“100”这个显示应该是个Bug，并不是一共就10个或者100个。 可参考如下这个解释： https://serverfault.com/questions/448589/increasing-of-max-more-than-10-tcp-connections 那么怎么看确定的连接数呢？ 在 开始 - 运行 中输入 perfmon.exe打开性能监视器，然后添加TCPv4的计数器。 这里就可以看到当前的实际连接数了，图里当前最新连接数是“824“，远超前面显示的”10“或者”100“。 看来一不小心又碰到坑了。 那么最开始要查的Connection refused到底是什么原因呢，还得继续努力了。。。]]></content>
      <categories>
        <category>操作系统</category>
        <category>windows</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取当当网的书籍分类目录]]></title>
    <url>%2F%E8%8E%B7%E5%8F%96%E5%BD%93%E5%BD%93%E7%BD%91%E7%9A%84%E4%B9%A6%E7%B1%8D%E5%88%86%E7%B1%BB%E7%9B%AE%E5%BD%95.html</url>
    <content type="text"><![CDATA[背景之前单位新建了一个小图书馆，然后就有了这么一个需求，需要设置一下图书的分类与目录。要怎么定义呢，当然是百度咯。然后想到了卖书发家的当当网，打算把当当网上的所有图书分类全部抓下来提供给行政来作参考。 思路打开当当网的图书页面http://book.dangdang.com/，图书分类就在网页的左边，开启F12看源代码。 多看看就看出来规律了，关注红框部分。所有的分类其实都在&lt;a&gt;标签里，其中的href属性里的网址很有规律，去掉前面的域名之后，都以cp + 数字来命名，其中数字与数字之间用.来分割，代表一级目录和二级目录。 所以大体思路就是通过正则表达式先抓取href属性中含有cp开头的元素，然后找出所有第一节数字不同的元素，获取其text属性来当一级目录，然后把域名+cp+一级目录序号当做固定前缀来找对应的二级目录。 要注意就是去重还有一些删除一些网址不符合这个过滤的，以及所有的text记得用strip()来删除一下多余的空格和换行符号。 具体实现按上面的思路，主要用requests bs4就差不多了，详细代码就参考github吧，https://github.com/keejo125/ 有更好的方法的也欢迎分享。]]></content>
      <categories>
        <category>python</category>
        <category>网络爬虫与数据分析</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开启又关闭icloud云盘，我的桌面文件去哪里了！]]></title>
    <url>%2F%E5%BC%80%E5%90%AF%E5%8F%88%E5%85%B3%E9%97%ADicloud%E4%BA%91%E7%9B%98%EF%BC%8C%E6%88%91%E7%9A%84%E6%A1%8C%E9%9D%A2%E6%96%87%E4%BB%B6%E5%8E%BB%E5%93%AA%E9%87%8C%E4%BA%86%EF%BC%81.html</url>
    <content type="text"><![CDATA[最近更换笔记本，又不想直接通过时间胶囊设置新mac，于是乎在整理好旧资料之后，准备拷贝到新电脑时发现了iCloud云盘这个东西，可以自动备份桌面和文稿的内容到iCloud，那么在新电脑中再通过iCloud下载就好了，完美！ 结果，高估了iCloud的效果，开启之后，mac会上传桌面和文稿，速度超级慢，然后mac风扇呼呼的转，果断放弃，关闭了iCloud云盘。 关闭也很慢，卡了一会儿，提示 慢的不行，反正也没上传多少东西，于是就点了“停止更新并关闭” 然后就问题出现了！！！ 桌面空空如也，我的东西呢！！！ 在翻翻iCloud云盘，只有已经上传了的那一丢丢！！！ 急中生智，赶紧百度，翻了好结果贴，结论如下： 其实前面已经提示了，文件都能被放在了一个叫做 “iCloud云盘（归档）”中了。路径如下：/User/xxx/中。 真是虚惊一场，看到网上好多碰到一样问题的，好多人以为就没有了。 特地记录一下。]]></content>
      <categories>
        <category>操作系统</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>iCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于这个”博客“]]></title>
    <url>%2F%E5%85%B3%E4%BA%8E%E8%BF%99%E4%B8%AA%E2%80%9D%E5%8D%9A%E5%AE%A2%E2%80%9C.html</url>
    <content type="text"><![CDATA[最初是在腾讯的云开发者平台上知道的hexo，好像很方便的样子，又很Geek的样子，于是打算尝试一下。 后来发现hexo作为博客的话，缺少了很大一部分功能——评论。 虽然可以使用第三方插件，但总觉得有点怪怪的。 于是乎，我打算把这里作为记录自己日常知识积累，或是感悟的地方。所以在标题中用了有引号的”博客“。 把每次百度或是谷歌出来的答案都记下来，希望不会再搜第二次。 感觉上好像有点跌跌撞撞，但方向是往前不就好了么，是吧]]></content>
      <categories>
        <category>生活随笔</category>
        <category>感悟</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world.html</url>
    <content type="text"><![CDATA[这是一个新的开始。 马东说：我的底色的悲凉的。 蔡康永说：只有底色悲凉的乐观，才是真的乐观啊。 乐观起来，哪怕人间不值得。]]></content>
      <categories>
        <category>生活随笔</category>
        <category>感悟</category>
      </categories>
  </entry>
</search>
